// Code generated by schema2go. DO NOT EDIT.
// Generated: 2026-02-27T22:24:53+05:30

package aws

import (
	"encoding/json"
	"fmt"

	"github.com/plantonhq/mcp-server-planton/internal/parse"
	"google.golang.org/protobuf/types/known/structpb"
)

var (
	_ = json.Marshal
	_ = fmt.Errorf
	_ = parse.ValidateHeader
	_ = (*structpb.Struct)(nil)
)

// AwsKinesisFirehose is the Kubernetes-style resource envelope for an Amazon
//
//	Kinesis Data Firehose delivery stream.
type AwsKinesisFirehoseSpecInput struct {
	// The AWS region where the resource will be created.
	//  Example: "us-west-2", "eu-west-1"
	Region string `json:"region,omitempty" jsonschema:"The AWS region where the resource will be created. Example: 'us-west-2'; 'eu-west-1'"`
	// Kinesis Data Stream source configuration. When set, Firehose reads from
	//  the specified stream instead of accepting Direct PUT calls. The entire
	//  source configuration is ForceNew -- it cannot be changed after creation.
	//
	//  When a Kinesis source is configured, server-side encryption (sse_enabled)
	//  must ...
	KinesisStreamSource *AwsKinesisFirehoseKinesisStreamSourceInput `json:"kinesis_stream_source,omitempty" jsonschema:"Kinesis Data Stream source configuration. When set; Firehose reads from the specified stream instead of accepting Direct PUT calls. The entire source configuration is ForceNew -- it cannot be changed ..."`
	// Enable server-side encryption for data at rest in the delivery stream
	//  buffer. Only valid for Direct PUT sources -- when using a Kinesis stream
	//  source, encryption is handled by the source stream.
	//
	//  When true and sse_kms_key_arn is absent, uses the AWS-owned CMK.
	//  When true and sse_kms_key_arn is pr...
	SseEnabled bool `json:"sse_enabled,omitempty" jsonschema:"Enable server-side encryption for data at rest in the delivery stream buffer. Only valid for Direct PUT sources -- when using a Kinesis stream source; encryption is handled by the source stream. When ..."`
	// Customer-managed KMS key ARN for server-side encryption. When set,
	//  Firehose uses this key instead of the AWS-owned CMK. Requires
	//  sse_enabled to be true.
	SseKmsKeyArn string `json:"sse_kms_key_arn,omitempty" jsonschema:"Customer-managed KMS key ARN for server-side encryption. When set; Firehose uses this key instead of the AWS-owned CMK. Requires sse_enabled to be true."`
	// Extended S3 destination for data lake storage. Supports compression,
	//  Lambda transformation, dynamic partitioning, and Parquet/ORC format
	//  conversion via AWS Glue Data Catalog. The most feature-rich destination.
	ExtendedS3 *AwsKinesisFirehoseExtendedS3DestinationInput `json:"extended_s3,omitempty" jsonschema:"Extended S3 destination for data lake storage. Supports compression; Lambda transformation; dynamic partitioning; and Parquet/ORC format conversion via AWS Glue Data Catalog. The most feature-rich des..."`
	// OpenSearch destination for direct indexing into an Amazon OpenSearch
	//  Service domain. Supports index rotation, VPC delivery, and Lambda
	//  transformation. Failed documents are backed up to S3.
	Opensearch *AwsKinesisFirehoseOpenSearchDestinationInput `json:"opensearch,omitempty" jsonschema:"OpenSearch destination for direct indexing into an Amazon OpenSearch Service domain. Supports index rotation; VPC delivery; and Lambda transformation. Failed documents are backed up to S3."`
	// HTTP endpoint destination for delivery to any HTTPS endpoint. Supports
	//  custom headers, content encoding, and Lambda transformation. Commonly
	//  used for third-party integrations (Datadog, New Relic, Sumo Logic).
	//  Failed deliveries are backed up to S3.
	HttpEndpoint *AwsKinesisFirehoseHttpEndpointDestinationInput `json:"http_endpoint,omitempty" jsonschema:"HTTP endpoint destination for delivery to any HTTPS endpoint. Supports custom headers; content encoding; and Lambda transformation. Commonly used for third-party integrations (Datadog; New Relic; Sumo..."`
	// Redshift destination for data warehouse loading. Firehose stages data
	//  in S3, then issues a Redshift COPY command to load it. Supports Lambda
	//  transformation and optional S3 backup of source records.
	Redshift *AwsKinesisFirehoseRedshiftDestinationInput `json:"redshift,omitempty" jsonschema:"Redshift destination for data warehouse loading. Firehose stages data in S3; then issues a Redshift COPY command to load it. Supports Lambda transformation and optional S3 backup of source records."`
}

func (s *AwsKinesisFirehoseSpecInput) validate() error {
	if s.KinesisStreamSource != nil {
		if err := s.KinesisStreamSource.validate(); err != nil {
			return fmt.Errorf("kinesis_stream_source: %w", err)
		}
	}
	if s.ExtendedS3 != nil {
		if err := s.ExtendedS3.validate(); err != nil {
			return fmt.Errorf("extended_s3: %w", err)
		}
	}
	if s.Opensearch != nil {
		if err := s.Opensearch.validate(); err != nil {
			return fmt.Errorf("opensearch: %w", err)
		}
	}
	if s.HttpEndpoint != nil {
		if err := s.HttpEndpoint.validate(); err != nil {
			return fmt.Errorf("http_endpoint: %w", err)
		}
	}
	if s.Redshift != nil {
		if err := s.Redshift.validate(); err != nil {
			return fmt.Errorf("redshift: %w", err)
		}
	}
	return nil
}

func (s *AwsKinesisFirehoseSpecInput) applyDefaults() {
	if s.KinesisStreamSource != nil {
		s.KinesisStreamSource.applyDefaults()
	}
	if s.ExtendedS3 != nil {
		s.ExtendedS3.applyDefaults()
	}
	if s.Opensearch != nil {
		s.Opensearch.applyDefaults()
	}
	if s.HttpEndpoint != nil {
		s.HttpEndpoint.applyDefaults()
	}
	if s.Redshift != nil {
		s.Redshift.applyDefaults()
	}
}

func (s *AwsKinesisFirehoseSpecInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.Region != "" {
		m["region"] = s.Region
	}
	if s.KinesisStreamSource != nil {
		m["kinesis_stream_source"] = s.KinesisStreamSource.toMap()
	}
	if s.SseEnabled {
		m["sse_enabled"] = s.SseEnabled
	}
	if s.SseKmsKeyArn != "" {
		m["sse_kms_key_arn"] = s.SseKmsKeyArn
	}
	if s.ExtendedS3 != nil {
		m["extended_s3"] = s.ExtendedS3.toMap()
	}
	if s.Opensearch != nil {
		m["opensearch"] = s.Opensearch.toMap()
	}
	if s.HttpEndpoint != nil {
		m["http_endpoint"] = s.HttpEndpoint.toMap()
	}
	if s.Redshift != nil {
		m["redshift"] = s.Redshift.toMap()
	}
	return m
}

// AwsKinesisFirehoseBufferingHints controls when Firehose flushes buffered data
//
//	to the destination. Firehose delivers when EITHER the buffer size OR the
//	buffer interval is reached -- whichever comes first.
type AwsKinesisFirehoseBufferingHintsInput struct {
	// Buffer interval in seconds. Firehose flushes when this time elapses since
	//  the last flush, even if the buffer size threshold has not been reached.
	//
	//  Range: 0-900 seconds. Default varies by destination (typically 300).
	//  Lower values reduce delivery latency; higher values improve batching
	//  efficiency ...
	IntervalInSeconds int32 `json:"interval_in_seconds,omitempty" jsonschema:"Buffer interval in seconds. Firehose flushes when this time elapses since the last flush; even if the buffer size threshold has not been reached. Range: 0-900 seconds. Default varies by destination (t..."`
	// Buffer size in MiB. Firehose flushes when the accumulated data reaches
	//  this threshold.
	//
	//  Range: 1-128 MiB. Default varies by destination (typically 5 MiB).
	//  Larger buffers produce fewer, larger objects (better for query engines);
	//  smaller buffers provide faster delivery.
	//
	//  Note: Some destinations h...
	SizeInMbs int32 `json:"size_in_mbs,omitempty" jsonschema:"Buffer size in MiB. Firehose flushes when the accumulated data reaches this threshold. Range: 1-128 MiB. Default varies by destination (typically 5 MiB). Larger buffers produce fewer; larger objects (..."`
}

func (s *AwsKinesisFirehoseBufferingHintsInput) validate() error {
	return nil
}

func (s *AwsKinesisFirehoseBufferingHintsInput) applyDefaults() {
}

func (s *AwsKinesisFirehoseBufferingHintsInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.IntervalInSeconds != 0 {
		m["interval_in_seconds"] = s.IntervalInSeconds
	}
	if s.SizeInMbs != 0 {
		m["size_in_mbs"] = s.SizeInMbs
	}
	return m
}

// AwsKinesisFirehoseCloudwatchLogging configures delivery error logging to
//
//	Amazon CloudWatch Logs. When enabled, Firehose publishes error information
//	for troubleshooting delivery failures.
type AwsKinesisFirehoseCloudwatchLoggingInput struct {
	// Enable CloudWatch error logging for this delivery target.
	Enabled bool `json:"enabled,omitempty" jsonschema:"Enable CloudWatch error logging for this delivery target."`
	// CloudWatch Logs log group name where errors are published.
	//  Required when enabled is true.
	LogGroupName string `json:"log_group_name,omitempty" jsonschema:"CloudWatch Logs log group name where errors are published. Required when enabled is true."`
	// CloudWatch Logs log stream name within the log group.
	//  Required when enabled is true.
	LogStreamName string `json:"log_stream_name,omitempty" jsonschema:"CloudWatch Logs log stream name within the log group. Required when enabled is true."`
}

func (s *AwsKinesisFirehoseCloudwatchLoggingInput) validate() error {
	return nil
}

func (s *AwsKinesisFirehoseCloudwatchLoggingInput) applyDefaults() {
}

func (s *AwsKinesisFirehoseCloudwatchLoggingInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.Enabled {
		m["enabled"] = s.Enabled
	}
	if s.LogGroupName != "" {
		m["log_group_name"] = s.LogGroupName
	}
	if s.LogStreamName != "" {
		m["log_stream_name"] = s.LogStreamName
	}
	return m
}

// AwsKinesisFirehoseDataFormatConversion configures conversion from JSON input
//
//	to columnar formats (Apache Parquet or Apache ORC) using an AWS Glue Data
//	Catalog schema. Columnar formats dramatically improve query performance
//	(10-100x faster for analytical queries) and reduce storage costs (60-90%
//	compression ratio).
//
//	Prerequisites:
//	- An AWS Glue Data Catalog database and table with the schema definition
//	- The Firehose IAM role must have glue:GetTable and glue:GetTableVersions
//	  permissions on the Glue catalog
type AwsKinesisFirehoseDataFormatConversionInput struct {
	// Enable data format conversion. When true, output_format and schema are
	//  required.
	Enabled bool `json:"enabled,omitempty" jsonschema:"Enable data format conversion. When true; output_format and schema are required."`
	// Input data format for deserialization. Firehose reads incoming JSON
	//  records using this deserializer.
	//
	//  Valid values:
	//  - "OPENX_JSON" (default) -- OpenX JSON SerDe. Handles most JSON formats
	//    including nested objects. Recommended for general use.
	//  - "HIVE_JSON" -- Apache Hive JSON SerDe. Use for H...
	InputFormat string `json:"input_format,omitempty" jsonschema:"Input data format for deserialization. Firehose reads incoming JSON records using this deserializer. Valid values: - 'OPENX_JSON' (default) -- OpenX JSON SerDe. Handles most JSON formats including nes..."`
	// Output columnar format for serialization. Records are converted from
	//  JSON to this format before writing to S3.
	//
	//  Valid values:
	//  - "PARQUET" -- Apache Parquet format. Best for read-heavy analytical
	//    workloads (Athena, Spark, Presto). Excellent compression, predicate
	//    pushdown, and columnar pruni...
	OutputFormat string `json:"output_format,omitempty" jsonschema:"Output columnar format for serialization. Records are converted from JSON to this format before writing to S3. Valid values: - 'PARQUET' -- Apache Parquet format. Best for read-heavy analytical worklo..."`
	// Compression for Parquet output. Only used when output_format is "PARQUET".
	//  Valid values: "SNAPPY" (default), "GZIP", "UNCOMPRESSED".
	ParquetCompression string `json:"parquet_compression,omitempty" jsonschema:"Compression for Parquet output. Only used when output_format is 'PARQUET'. Valid values: 'SNAPPY' (default); 'GZIP'; 'UNCOMPRESSED'."`
	// Compression for ORC output. Only used when output_format is "ORC".
	//  Valid values: "SNAPPY" (default), "ZLIB", "NONE".
	OrcCompression string `json:"orc_compression,omitempty" jsonschema:"Compression for ORC output. Only used when output_format is 'ORC'. Valid values: 'SNAPPY' (default); 'ZLIB'; 'NONE'."`
	// AWS Glue Data Catalog schema reference. Defines the table schema used
	//  for converting JSON records to the columnar format. Required when
	//  data format conversion is enabled.
	Schema *AwsKinesisFirehoseGlueSchemaConfigInput `json:"schema,omitempty" jsonschema:"AWS Glue Data Catalog schema reference. Defines the table schema used for converting JSON records to the columnar format. Required when data format conversion is enabled."`
}

func (s *AwsKinesisFirehoseDataFormatConversionInput) validate() error {
	if s.Schema != nil {
		if err := s.Schema.validate(); err != nil {
			return fmt.Errorf("schema: %w", err)
		}
	}
	return nil
}

func (s *AwsKinesisFirehoseDataFormatConversionInput) applyDefaults() {
	if s.Schema != nil {
		s.Schema.applyDefaults()
	}
}

func (s *AwsKinesisFirehoseDataFormatConversionInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.Enabled {
		m["enabled"] = s.Enabled
	}
	if s.InputFormat != "" {
		m["input_format"] = s.InputFormat
	}
	if s.OutputFormat != "" {
		m["output_format"] = s.OutputFormat
	}
	if s.ParquetCompression != "" {
		m["parquet_compression"] = s.ParquetCompression
	}
	if s.OrcCompression != "" {
		m["orc_compression"] = s.OrcCompression
	}
	if s.Schema != nil {
		m["schema"] = s.Schema.toMap()
	}
	return m
}

// AwsKinesisFirehoseDynamicPartitioning configures dynamic partitioning for the
//
//	Extended S3 destination. When enabled, Firehose extracts partition keys from
//	record fields and uses them to construct S3 prefixes, creating a partitioned
//	data layout for efficient query engines (Athena, Spark, Presto).
//
//	The entire dynamic partitioning configuration is ForceNew -- it cannot be
//	enabled or disabled after the delivery stream is created.
type AwsKinesisFirehoseDynamicPartitioningInput struct {
	// Enable dynamic partitioning. ForceNew -- cannot be changed after creation.
	//  When enabled, configure partition key expressions in the S3 prefix using
	//  !{partitionKeyFromQuery:...} or !{partitionKeyFromLambda:...} syntax.
	Enabled bool `json:"enabled,omitempty" jsonschema:"Enable dynamic partitioning. ForceNew -- cannot be changed after creation. When enabled; configure partition key expressions in the S3 prefix using !{partitionKeyFromQuery:...} or !{partitionKeyFromLa..."`
	// Duration in seconds that Firehose retries delivery when a partition key
	//  expression fails or the S3 PutObject call is throttled.
	//  Range: 0-7200. Default: 300 seconds.
	RetryDurationInSeconds int32 `json:"retry_duration_in_seconds,omitempty" jsonschema:"Duration in seconds that Firehose retries delivery when a partition key expression fails or the S3 PutObject call is throttled. Range: 0-7200. Default: 300 seconds."`
}

func (s *AwsKinesisFirehoseDynamicPartitioningInput) validate() error {
	return nil
}

func (s *AwsKinesisFirehoseDynamicPartitioningInput) applyDefaults() {
}

func (s *AwsKinesisFirehoseDynamicPartitioningInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.Enabled {
		m["enabled"] = s.Enabled
	}
	if s.RetryDurationInSeconds != 0 {
		m["retry_duration_in_seconds"] = s.RetryDurationInSeconds
	}
	return m
}

// AwsKinesisFirehoseExtendedS3Destination configures delivery to Amazon S3 with
//
//	advanced features: Lambda transformation, dynamic partitioning, Parquet/ORC
//	format conversion, compression, and S3 backup of source records.
//
//	This is the most feature-rich destination and the most common (~60% of all
//	Firehose delivery streams). Use it for data lakes, log archives, analytics
//	pipelines, and any scenario where data needs to land in S3.
type AwsKinesisFirehoseExtendedS3DestinationInput struct {
	// S3 bucket ARN where records are delivered.
	BucketArn string `json:"bucket_arn" jsonschema:"required,S3 bucket ARN where records are delivered."`
	// IAM role ARN granting Firehose write access to the S3 bucket, KMS key
	//  (if encrypted), Lambda function (if processing), and Glue catalog (if
	//  format conversion is enabled).
	RoleArn string `json:"role_arn" jsonschema:"required,IAM role ARN granting Firehose write access to the S3 bucket; KMS key (if encrypted); Lambda function (if processing); and Glue catalog (if format conversion is enabled)."`
	// S3 key prefix prepended to every delivered object. Supports Firehose
	//  expression syntax for dynamic prefixes:
	//    "data/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/"
	//
	//  When dynamic partitioning is enabled, use partitioning keys:
	//    "data/customer=!{partitionKeyFromQuery:customer_...
	Prefix string `json:"prefix,omitempty" jsonschema:"S3 key prefix prepended to every delivered object. Supports Firehose expression syntax for dynamic prefixes: 'data/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/' When dynamic parti..."`
	// S3 key prefix for records that fail transformation or delivery.
	ErrorOutputPrefix string `json:"error_output_prefix,omitempty" jsonschema:"S3 key prefix for records that fail transformation or delivery."`
	// Compression format applied before writing to S3. When data format
	//  conversion is enabled, compression is applied to the converted
	//  (Parquet/ORC) output -- in that case, use the format-native compression
	//  (configured in data_format_conversion) and leave this as UNCOMPRESSED.
	//
	//  Valid values: "UNCOMPRE...
	CompressionFormat string `json:"compression_format,omitempty" jsonschema:"Compression format applied before writing to S3. When data format conversion is enabled; compression is applied to the converted (Parquet/ORC) output -- in that case; use the format-native compression..."`
	// Customer-managed KMS key ARN for S3 server-side encryption (SSE-KMS).
	KmsKeyArn string `json:"kms_key_arn,omitempty" jsonschema:"Customer-managed KMS key ARN for S3 server-side encryption (SSE-KMS)."`
	// Buffering hints for S3 delivery. Default: 300s interval, 5 MiB size.
	Buffering *AwsKinesisFirehoseBufferingHintsInput `json:"buffering,omitempty" jsonschema:"Buffering hints for S3 delivery. Default: 300s interval; 5 MiB size."`
	// IANA time zone for S3 prefix timestamp expressions.
	//  Default: "UTC". Example: "US/Eastern", "Europe/London".
	CustomTimeZone string `json:"custom_time_zone,omitempty" jsonschema:"IANA time zone for S3 prefix timestamp expressions. Default: 'UTC'. Example: 'US/Eastern'; 'Europe/London'."`
	// File extension appended to delivered S3 objects (e.g., ".json", ".parquet").
	//  Must start with a period. When data format conversion is enabled, the
	//  extension is typically set to match the output format.
	FileExtension string `json:"file_extension,omitempty" jsonschema:"File extension appended to delivered S3 objects (e.g.; '.json'; '.parquet'). Must start with a period. When data format conversion is enabled; the extension is typically set to match the output format..."`
	// S3 backup mode for source records. When "Enabled", a copy of the original
	//  (pre-transformation) records is written to s3_backup in addition to the
	//  primary destination. Useful for auditing and reprocessing.
	//
	//  Valid values: "Disabled" (default), "Enabled".
	S3BackupMode string `json:"s3_backup_mode,omitempty" jsonschema:"S3 backup mode for source records. When 'Enabled'; a copy of the original (pre-transformation) records is written to s3_backup in addition to the primary destination. Useful for auditing and reprocess..."`
	// S3 configuration for source record backup. Required when s3_backup_mode
	//  is "Enabled".
	S3Backup *AwsKinesisFirehoseS3ConfigInput `json:"s3_backup,omitempty" jsonschema:"S3 configuration for source record backup. Required when s3_backup_mode is 'Enabled'."`
	// Lambda-based record transformation. Applied before compression and
	//  format conversion.
	Processing *AwsKinesisFirehoseLambdaProcessingInput `json:"processing,omitempty" jsonschema:"Lambda-based record transformation. Applied before compression and format conversion."`
	// CloudWatch error logging for S3 delivery failures.
	Logging *AwsKinesisFirehoseCloudwatchLoggingInput `json:"logging,omitempty" jsonschema:"CloudWatch error logging for S3 delivery failures."`
	// Dynamic partitioning configuration. Enables partitioning delivered data
	//  by record fields (e.g., customer_id, event_type) for efficient querying
	//  with Athena, Spark, or Presto. ForceNew -- cannot be enabled/disabled
	//  after creation.
	DynamicPartitioning *AwsKinesisFirehoseDynamicPartitioningInput `json:"dynamic_partitioning,omitempty" jsonschema:"Dynamic partitioning configuration. Enables partitioning delivered data by record fields (e.g.; customer_id; event_type) for efficient querying with Athena; Spark; or Presto. ForceNew -- cannot be ena..."`
	// Data format conversion from JSON to columnar formats (Parquet or ORC)
	//  using an AWS Glue Data Catalog schema. Dramatically improves query
	//  performance and reduces storage cost for analytics workloads.
	DataFormatConversion *AwsKinesisFirehoseDataFormatConversionInput `json:"data_format_conversion,omitempty" jsonschema:"Data format conversion from JSON to columnar formats (Parquet or ORC) using an AWS Glue Data Catalog schema. Dramatically improves query performance and reduces storage cost for analytics workloads."`
}

func (s *AwsKinesisFirehoseExtendedS3DestinationInput) validate() error {
	if s.BucketArn == "" {
		return fmt.Errorf("bucket_arn is required")
	}
	if s.RoleArn == "" {
		return fmt.Errorf("role_arn is required")
	}
	if s.Buffering != nil {
		if err := s.Buffering.validate(); err != nil {
			return fmt.Errorf("buffering: %w", err)
		}
	}
	if s.S3Backup != nil {
		if err := s.S3Backup.validate(); err != nil {
			return fmt.Errorf("s3_backup: %w", err)
		}
	}
	if s.Processing != nil {
		if err := s.Processing.validate(); err != nil {
			return fmt.Errorf("processing: %w", err)
		}
	}
	if s.Logging != nil {
		if err := s.Logging.validate(); err != nil {
			return fmt.Errorf("logging: %w", err)
		}
	}
	if s.DynamicPartitioning != nil {
		if err := s.DynamicPartitioning.validate(); err != nil {
			return fmt.Errorf("dynamic_partitioning: %w", err)
		}
	}
	if s.DataFormatConversion != nil {
		if err := s.DataFormatConversion.validate(); err != nil {
			return fmt.Errorf("data_format_conversion: %w", err)
		}
	}
	return nil
}

func (s *AwsKinesisFirehoseExtendedS3DestinationInput) applyDefaults() {
	if s.Buffering != nil {
		s.Buffering.applyDefaults()
	}
	if s.S3Backup != nil {
		s.S3Backup.applyDefaults()
	}
	if s.Processing != nil {
		s.Processing.applyDefaults()
	}
	if s.Logging != nil {
		s.Logging.applyDefaults()
	}
	if s.DynamicPartitioning != nil {
		s.DynamicPartitioning.applyDefaults()
	}
	if s.DataFormatConversion != nil {
		s.DataFormatConversion.applyDefaults()
	}
}

func (s *AwsKinesisFirehoseExtendedS3DestinationInput) toMap() map[string]any {
	m := make(map[string]any)
	m["bucket_arn"] = s.BucketArn
	m["role_arn"] = s.RoleArn
	if s.Prefix != "" {
		m["prefix"] = s.Prefix
	}
	if s.ErrorOutputPrefix != "" {
		m["error_output_prefix"] = s.ErrorOutputPrefix
	}
	if s.CompressionFormat != "" {
		m["compression_format"] = s.CompressionFormat
	}
	if s.KmsKeyArn != "" {
		m["kms_key_arn"] = s.KmsKeyArn
	}
	if s.Buffering != nil {
		m["buffering"] = s.Buffering.toMap()
	}
	if s.CustomTimeZone != "" {
		m["custom_time_zone"] = s.CustomTimeZone
	}
	if s.FileExtension != "" {
		m["file_extension"] = s.FileExtension
	}
	if s.S3BackupMode != "" {
		m["s3_backup_mode"] = s.S3BackupMode
	}
	if s.S3Backup != nil {
		m["s3_backup"] = s.S3Backup.toMap()
	}
	if s.Processing != nil {
		m["processing"] = s.Processing.toMap()
	}
	if s.Logging != nil {
		m["logging"] = s.Logging.toMap()
	}
	if s.DynamicPartitioning != nil {
		m["dynamic_partitioning"] = s.DynamicPartitioning.toMap()
	}
	if s.DataFormatConversion != nil {
		m["data_format_conversion"] = s.DataFormatConversion.toMap()
	}
	return m
}

// AwsKinesisFirehoseGlueSchemaConfig references an AWS Glue Data Catalog table
//
//	that defines the schema for data format conversion. The table must exist in
//	the Glue catalog before the delivery stream is created.
type AwsKinesisFirehoseGlueSchemaConfigInput struct {
	// Glue Data Catalog database name containing the table.
	DatabaseName string `json:"database_name,omitempty" jsonschema:"Glue Data Catalog database name containing the table."`
	// Glue Data Catalog table name defining the record schema.
	TableName string `json:"table_name,omitempty" jsonschema:"Glue Data Catalog table name defining the record schema."`
	// IAM role ARN granting Firehose permission to access the Glue catalog.
	//  Must have glue:GetTable and glue:GetTableVersions permissions.
	RoleArn string `json:"role_arn" jsonschema:"required,IAM role ARN granting Firehose permission to access the Glue catalog. Must have glue:GetTable and glue:GetTableVersions permissions."`
	// Glue Data Catalog ID (AWS account ID). When omitted, defaults to the
	//  current AWS account.
	CatalogId string `json:"catalog_id,omitempty" jsonschema:"Glue Data Catalog ID (AWS account ID). When omitted; defaults to the current AWS account."`
	// AWS region of the Glue catalog. When omitted, defaults to the delivery
	//  stream's region.
	Region string `json:"region,omitempty" jsonschema:"AWS region of the Glue catalog. When omitted; defaults to the delivery stream's region."`
	// Table version to use. Default: "LATEST".
	VersionId string `json:"version_id,omitempty" jsonschema:"Table version to use. Default: 'LATEST'."`
}

func (s *AwsKinesisFirehoseGlueSchemaConfigInput) validate() error {
	if s.RoleArn == "" {
		return fmt.Errorf("role_arn is required")
	}
	return nil
}

func (s *AwsKinesisFirehoseGlueSchemaConfigInput) applyDefaults() {
}

func (s *AwsKinesisFirehoseGlueSchemaConfigInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.DatabaseName != "" {
		m["database_name"] = s.DatabaseName
	}
	if s.TableName != "" {
		m["table_name"] = s.TableName
	}
	m["role_arn"] = s.RoleArn
	if s.CatalogId != "" {
		m["catalog_id"] = s.CatalogId
	}
	if s.Region != "" {
		m["region"] = s.Region
	}
	if s.VersionId != "" {
		m["version_id"] = s.VersionId
	}
	return m
}

// AwsKinesisFirehoseHttpEndpointDestination configures delivery to any HTTPS
//
//	endpoint. This is the most flexible destination, commonly used for
//	third-party analytics platforms (Datadog, New Relic, Sumo Logic), custom
//	APIs, and webhook-based integrations.
//
//	The endpoint must accept HTTPS POST requests. Firehose sends records as
//	JSON arrays in the request body and expects an HTTP 200 response.
//	Failed deliveries are backed up to S3.
type AwsKinesisFirehoseHttpEndpointDestinationInput struct {
	// HTTPS URL of the destination endpoint. Must start with "https://".
	//  Maximum length: 1000 characters.
	//
	//  Examples:
	//  - "https://http-intake.logs.datadoghq.com/v1/input"
	//  - "https://api.honeycomb.io/1/kinesis_events/your-dataset"
	//  - "https://my-api.example.com/firehose"
	Url string `json:"url,omitempty" jsonschema:"HTTPS URL of the destination endpoint. Must start with 'https://'. Maximum length: 1000 characters. Examples: - 'https://http-intake.logs.datadoghq.com/v1/input' - 'https://api.honeycomb.io/1/kinesis_..."`
	// Human-readable name for the endpoint. Appears in the AWS Console and
	//  CloudWatch metrics. Maximum 256 characters.
	Name string `json:"name,omitempty" jsonschema:"Human-readable name for the endpoint. Appears in the AWS Console and CloudWatch metrics. Maximum 256 characters."`
	// Access key for endpoint authentication. The value is sent in the
	//  X-Amz-Firehose-Access-Key header. Sensitive -- treated as a secret.
	//  Maximum 4096 characters.
	AccessKey string `json:"access_key,omitempty" jsonschema:"Access key for endpoint authentication. The value is sent in the X-Amz-Firehose-Access-Key header. Sensitive -- treated as a secret. Maximum 4096 characters."`
	// IAM role ARN granting Firehose permission to deliver to the endpoint
	//  and write to the S3 backup bucket.
	RoleArn string `json:"role_arn,omitempty" jsonschema:"IAM role ARN granting Firehose permission to deliver to the endpoint and write to the S3 backup bucket."`
	// Buffering hints for HTTP delivery. Default: 300s interval, 5 MiB.
	Buffering *AwsKinesisFirehoseBufferingHintsInput `json:"buffering,omitempty" jsonschema:"Buffering hints for HTTP delivery. Default: 300s interval; 5 MiB."`
	// Retry duration in seconds for failed HTTP deliveries (non-2xx responses
	//  or timeouts). Range: 0-7200. Default: 300 seconds.
	RetryDurationInSeconds int32 `json:"retry_duration_in_seconds,omitempty" jsonschema:"Retry duration in seconds for failed HTTP deliveries (non-2xx responses or timeouts). Range: 0-7200. Default: 300 seconds."`
	// S3 backup mode. Controls when records are written to S3.
	//
	//  Valid values:
	//  - "FailedDataOnly" (default) -- only records that fail HTTP delivery
	//    are backed up to S3.
	//  - "AllData" -- all records are backed up to S3 in addition to being
	//    sent to the HTTP endpoint.
	S3BackupMode string `json:"s3_backup_mode,omitempty" jsonschema:"S3 backup mode. Controls when records are written to S3. Valid values: - 'FailedDataOnly' (default) -- only records that fail HTTP delivery are backed up to S3. - 'AllData' -- all records are backed u..."`
	// S3 configuration for backing up failed (or all) records. Required.
	S3Config *AwsKinesisFirehoseS3ConfigInput `json:"s3_config" jsonschema:"required,S3 configuration for backing up failed (or all) records. Required."`
	// Lambda-based record transformation before HTTP delivery.
	Processing *AwsKinesisFirehoseLambdaProcessingInput `json:"processing,omitempty" jsonschema:"Lambda-based record transformation before HTTP delivery."`
	// CloudWatch error logging for HTTP delivery failures.
	Logging *AwsKinesisFirehoseCloudwatchLoggingInput `json:"logging,omitempty" jsonschema:"CloudWatch error logging for HTTP delivery failures."`
	// Request configuration for customizing the HTTP request format.
	RequestConfig *AwsKinesisFirehoseRequestConfigInput `json:"request_config,omitempty" jsonschema:"Request configuration for customizing the HTTP request format."`
}

func (s *AwsKinesisFirehoseHttpEndpointDestinationInput) validate() error {
	if s.Buffering != nil {
		if err := s.Buffering.validate(); err != nil {
			return fmt.Errorf("buffering: %w", err)
		}
	}
	if s.S3Config == nil {
		return fmt.Errorf("s3_config is required")
	}
	if s.S3Config != nil {
		if err := s.S3Config.validate(); err != nil {
			return fmt.Errorf("s3_config: %w", err)
		}
	}
	if s.Processing != nil {
		if err := s.Processing.validate(); err != nil {
			return fmt.Errorf("processing: %w", err)
		}
	}
	if s.Logging != nil {
		if err := s.Logging.validate(); err != nil {
			return fmt.Errorf("logging: %w", err)
		}
	}
	if s.RequestConfig != nil {
		if err := s.RequestConfig.validate(); err != nil {
			return fmt.Errorf("request_config: %w", err)
		}
	}
	return nil
}

func (s *AwsKinesisFirehoseHttpEndpointDestinationInput) applyDefaults() {
	if s.Buffering != nil {
		s.Buffering.applyDefaults()
	}
	if s.S3Config != nil {
		s.S3Config.applyDefaults()
	}
	if s.Processing != nil {
		s.Processing.applyDefaults()
	}
	if s.Logging != nil {
		s.Logging.applyDefaults()
	}
	if s.RequestConfig != nil {
		s.RequestConfig.applyDefaults()
	}
}

func (s *AwsKinesisFirehoseHttpEndpointDestinationInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.Url != "" {
		m["url"] = s.Url
	}
	if s.Name != "" {
		m["name"] = s.Name
	}
	if s.AccessKey != "" {
		m["access_key"] = s.AccessKey
	}
	if s.RoleArn != "" {
		m["role_arn"] = s.RoleArn
	}
	if s.Buffering != nil {
		m["buffering"] = s.Buffering.toMap()
	}
	if s.RetryDurationInSeconds != 0 {
		m["retry_duration_in_seconds"] = s.RetryDurationInSeconds
	}
	if s.S3BackupMode != "" {
		m["s3_backup_mode"] = s.S3BackupMode
	}
	if s.S3Config != nil {
		m["s3_config"] = s.S3Config.toMap()
	}
	if s.Processing != nil {
		m["processing"] = s.Processing.toMap()
	}
	if s.Logging != nil {
		m["logging"] = s.Logging.toMap()
	}
	if s.RequestConfig != nil {
		m["request_config"] = s.RequestConfig.toMap()
	}
	return m
}

// AwsKinesisFirehoseKinesisStreamSource configures Firehose to read from an
//
//	existing Kinesis Data Stream. Firehose acts as a consumer with automatic
//	checkpointing, retry, and backpressure handling.
//
//	The entire source configuration is ForceNew -- all fields require replacing
//	the delivery stream if changed.
type AwsKinesisFirehoseKinesisStreamSourceInput struct {
	// ARN of the Kinesis Data Stream to read from. Firehose creates an internal
	//  consumer and reads all shards. The stream must exist before the delivery
	//  stream is created.
	StreamArn string `json:"stream_arn" jsonschema:"required,ARN of the Kinesis Data Stream to read from. Firehose creates an internal consumer and reads all shards. The stream must exist before the delivery stream is created."`
	// IAM role ARN that grants Firehose permission to read from the Kinesis
	//  stream. The role must have kinesis:GetRecords, kinesis:GetShardIterator,
	//  kinesis:DescribeStream, and kinesis:ListShards permissions.
	RoleArn string `json:"role_arn" jsonschema:"required,IAM role ARN that grants Firehose permission to read from the Kinesis stream. The role must have kinesis:GetRecords; kinesis:GetShardIterator; kinesis:DescribeStream; and kinesis:ListShards permission..."`
}

func (s *AwsKinesisFirehoseKinesisStreamSourceInput) validate() error {
	if s.StreamArn == "" {
		return fmt.Errorf("stream_arn is required")
	}
	if s.RoleArn == "" {
		return fmt.Errorf("role_arn is required")
	}
	return nil
}

func (s *AwsKinesisFirehoseKinesisStreamSourceInput) applyDefaults() {
}

func (s *AwsKinesisFirehoseKinesisStreamSourceInput) toMap() map[string]any {
	m := make(map[string]any)
	m["stream_arn"] = s.StreamArn
	m["role_arn"] = s.RoleArn
	return m
}

// AwsKinesisFirehoseLambdaProcessing configures an AWS Lambda function to
//
//	transform records before delivery. This is a simplified model covering the
//	dominant use case (~95% of processing configurations use Lambda).
//
//	In the IaC modules, this is mapped to the generic processor model with
//	type="Lambda" and typed parameter name/value pairs.
type AwsKinesisFirehoseLambdaProcessingInput struct {
	// Enable Lambda-based record transformation.
	Enabled bool `json:"enabled,omitempty" jsonschema:"Enable Lambda-based record transformation."`
	// ARN of the Lambda function that transforms records. The function receives
	//  batches of records and returns transformed records with a status
	//  (Ok, Dropped, ProcessingFailed) per record.
	//
	//  Required when enabled is true.
	LambdaArn string `json:"lambda_arn,omitempty" jsonschema:"ARN of the Lambda function that transforms records. The function receives batches of records and returns transformed records with a status (Ok; Dropped; ProcessingFailed) per record. Required when ena..."`
	// Buffer size in MiB that Firehose accumulates before invoking Lambda.
	//  Range: 1-3 MiB. Default: 3 MiB.
	//
	//  Smaller buffers invoke Lambda more frequently with smaller batches.
	//  Larger buffers (up to 3 MiB) are more efficient and reduce Lambda
	//  invocation costs.
	BufferSizeInMbs int32 `json:"buffer_size_in_mbs,omitempty" jsonschema:"Buffer size in MiB that Firehose accumulates before invoking Lambda. Range: 1-3 MiB. Default: 3 MiB. Smaller buffers invoke Lambda more frequently with smaller batches. Larger buffers (up to 3 MiB) ar..."`
	// Buffer interval in seconds. Firehose invokes Lambda when this interval
	//  elapses, even if the buffer size threshold has not been reached.
	//  Range: 60-900 seconds. Default: 60 seconds.
	BufferIntervalInSeconds int32 `json:"buffer_interval_in_seconds,omitempty" jsonschema:"Buffer interval in seconds. Firehose invokes Lambda when this interval elapses; even if the buffer size threshold has not been reached. Range: 60-900 seconds. Default: 60 seconds."`
	// Number of times Firehose retries a failed Lambda invocation before
	//  writing the record to the error output prefix.
	//  Range: 0-300. Default: 3.
	NumberOfRetries int32 `json:"number_of_retries,omitempty" jsonschema:"Number of times Firehose retries a failed Lambda invocation before writing the record to the error output prefix. Range: 0-300. Default: 3."`
}

func (s *AwsKinesisFirehoseLambdaProcessingInput) validate() error {
	return nil
}

func (s *AwsKinesisFirehoseLambdaProcessingInput) applyDefaults() {
}

func (s *AwsKinesisFirehoseLambdaProcessingInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.Enabled {
		m["enabled"] = s.Enabled
	}
	if s.LambdaArn != "" {
		m["lambda_arn"] = s.LambdaArn
	}
	if s.BufferSizeInMbs != 0 {
		m["buffer_size_in_mbs"] = s.BufferSizeInMbs
	}
	if s.BufferIntervalInSeconds != 0 {
		m["buffer_interval_in_seconds"] = s.BufferIntervalInSeconds
	}
	if s.NumberOfRetries != 0 {
		m["number_of_retries"] = s.NumberOfRetries
	}
	return m
}

// AwsKinesisFirehoseOpenSearchDestination configures delivery to an Amazon
//
//	OpenSearch Service domain. Firehose indexes records directly into the
//	specified index with configurable rotation, buffering, and retry behavior.
//	Failed documents are always backed up to S3.
//
//	Supports both public and VPC-deployed OpenSearch domains. For VPC domains,
//	configure vpc_config to enable Firehose to create ENIs in the VPC.
type AwsKinesisFirehoseOpenSearchDestinationInput struct {
	// ARN of the OpenSearch domain. Mutually exclusive with cluster_endpoint.
	//  Use this for domains managed within the same AWS account.
	DomainArn string `json:"domain_arn,omitempty" jsonschema:"ARN of the OpenSearch domain. Mutually exclusive with cluster_endpoint. Use this for domains managed within the same AWS account."`
	// OpenSearch cluster endpoint URL. Mutually exclusive with domain_arn.
	//  Use this for cross-account domains or non-standard endpoints.
	//  Format: "https://search-domain-xxxx.us-east-1.es.amazonaws.com"
	ClusterEndpoint string `json:"cluster_endpoint,omitempty" jsonschema:"OpenSearch cluster endpoint URL. Mutually exclusive with domain_arn. Use this for cross-account domains or non-standard endpoints. Format: 'https://search-domain-xxxx.us-east-1.es.amazonaws.com'"`
	// Name of the OpenSearch index to deliver records to. Required.
	//  When index_rotation_period is set, this becomes the index prefix and
	//  Firehose appends a timestamp suffix (e.g., "logs-2026-02-15").
	IndexName string `json:"index_name,omitempty" jsonschema:"Name of the OpenSearch index to deliver records to. Required. When index_rotation_period is set; this becomes the index prefix and Firehose appends a timestamp suffix (e.g.; 'logs-2026-02-15')."`
	// IAM role ARN granting Firehose permission to write to OpenSearch.
	//  Must have es:ESHttpPut and es:ESHttpGet permissions on the domain.
	RoleArn string `json:"role_arn" jsonschema:"required,IAM role ARN granting Firehose permission to write to OpenSearch. Must have es:ESHttpPut and es:ESHttpGet permissions on the domain."`
	// Index rotation period. Firehose appends a timestamp suffix to index_name
	//  and creates a new index at each rotation boundary.
	//
	//  Valid values: "NoRotation", "OneHour", "OneDay" (default), "OneWeek", "OneMonth".
	//
	//  "NoRotation" writes all records to the same index (use for small, static datasets).
	//  "One...
	IndexRotationPeriod string `json:"index_rotation_period,omitempty" jsonschema:"Index rotation period. Firehose appends a timestamp suffix to index_name and creates a new index at each rotation boundary. Valid values: 'NoRotation'; 'OneHour'; 'OneDay' (default); 'OneWeek'; 'OneMo..."`
	// OpenSearch document type name. Only relevant for Elasticsearch 6.x and
	//  earlier (OpenSearch does not use document types). Leave empty for
	//  OpenSearch domains.
	TypeName string `json:"type_name,omitempty" jsonschema:"OpenSearch document type name. Only relevant for Elasticsearch 6.x and earlier (OpenSearch does not use document types). Leave empty for OpenSearch domains."`
	// Buffering hints for OpenSearch delivery. Default: 300s interval, 5 MiB.
	//  Max size: 100 MiB for OpenSearch destinations.
	Buffering *AwsKinesisFirehoseBufferingHintsInput `json:"buffering,omitempty" jsonschema:"Buffering hints for OpenSearch delivery. Default: 300s interval; 5 MiB. Max size: 100 MiB for OpenSearch destinations."`
	// Retry duration in seconds for failed OpenSearch index requests.
	//  Range: 0-7200. Default: 300 seconds.
	//  Set to 0 to disable retries (failed documents go directly to S3 backup).
	RetryDurationInSeconds int32 `json:"retry_duration_in_seconds,omitempty" jsonschema:"Retry duration in seconds for failed OpenSearch index requests. Range: 0-7200. Default: 300 seconds. Set to 0 to disable retries (failed documents go directly to S3 backup)."`
	// S3 backup mode for documents. Controls when records are written to S3.
	//
	//  Valid values:
	//  - "FailedDocumentsOnly" (default) -- only documents that fail indexing
	//    are backed up to S3.
	//  - "AllDocuments" -- all documents are backed up to S3 in addition to
	//    being indexed in OpenSearch.
	S3BackupMode string `json:"s3_backup_mode,omitempty" jsonschema:"S3 backup mode for documents. Controls when records are written to S3. Valid values: - 'FailedDocumentsOnly' (default) -- only documents that fail indexing are backed up to S3. - 'AllDocuments' -- all..."`
	// S3 configuration for backing up failed (or all) documents. Required.
	S3Config *AwsKinesisFirehoseS3ConfigInput `json:"s3_config" jsonschema:"required,S3 configuration for backing up failed (or all) documents. Required."`
	// Lambda-based record transformation before indexing.
	Processing *AwsKinesisFirehoseLambdaProcessingInput `json:"processing,omitempty" jsonschema:"Lambda-based record transformation before indexing."`
	// CloudWatch error logging for OpenSearch delivery failures.
	Logging *AwsKinesisFirehoseCloudwatchLoggingInput `json:"logging,omitempty" jsonschema:"CloudWatch error logging for OpenSearch delivery failures."`
	// VPC configuration for delivering to VPC-deployed OpenSearch domains.
	//  ForceNew -- the VPC config cannot be changed after creation.
	//  When absent, Firehose delivers over the public internet.
	VpcConfig *AwsKinesisFirehoseVpcConfigInput `json:"vpc_config,omitempty" jsonschema:"VPC configuration for delivering to VPC-deployed OpenSearch domains. ForceNew -- the VPC config cannot be changed after creation. When absent; Firehose delivers over the public internet."`
}

func (s *AwsKinesisFirehoseOpenSearchDestinationInput) validate() error {
	if s.RoleArn == "" {
		return fmt.Errorf("role_arn is required")
	}
	if s.Buffering != nil {
		if err := s.Buffering.validate(); err != nil {
			return fmt.Errorf("buffering: %w", err)
		}
	}
	if s.S3Config == nil {
		return fmt.Errorf("s3_config is required")
	}
	if s.S3Config != nil {
		if err := s.S3Config.validate(); err != nil {
			return fmt.Errorf("s3_config: %w", err)
		}
	}
	if s.Processing != nil {
		if err := s.Processing.validate(); err != nil {
			return fmt.Errorf("processing: %w", err)
		}
	}
	if s.Logging != nil {
		if err := s.Logging.validate(); err != nil {
			return fmt.Errorf("logging: %w", err)
		}
	}
	if s.VpcConfig != nil {
		if err := s.VpcConfig.validate(); err != nil {
			return fmt.Errorf("vpc_config: %w", err)
		}
	}
	return nil
}

func (s *AwsKinesisFirehoseOpenSearchDestinationInput) applyDefaults() {
	if s.Buffering != nil {
		s.Buffering.applyDefaults()
	}
	if s.S3Config != nil {
		s.S3Config.applyDefaults()
	}
	if s.Processing != nil {
		s.Processing.applyDefaults()
	}
	if s.Logging != nil {
		s.Logging.applyDefaults()
	}
	if s.VpcConfig != nil {
		s.VpcConfig.applyDefaults()
	}
}

func (s *AwsKinesisFirehoseOpenSearchDestinationInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.DomainArn != "" {
		m["domain_arn"] = s.DomainArn
	}
	if s.ClusterEndpoint != "" {
		m["cluster_endpoint"] = s.ClusterEndpoint
	}
	if s.IndexName != "" {
		m["index_name"] = s.IndexName
	}
	m["role_arn"] = s.RoleArn
	if s.IndexRotationPeriod != "" {
		m["index_rotation_period"] = s.IndexRotationPeriod
	}
	if s.TypeName != "" {
		m["type_name"] = s.TypeName
	}
	if s.Buffering != nil {
		m["buffering"] = s.Buffering.toMap()
	}
	if s.RetryDurationInSeconds != 0 {
		m["retry_duration_in_seconds"] = s.RetryDurationInSeconds
	}
	if s.S3BackupMode != "" {
		m["s3_backup_mode"] = s.S3BackupMode
	}
	if s.S3Config != nil {
		m["s3_config"] = s.S3Config.toMap()
	}
	if s.Processing != nil {
		m["processing"] = s.Processing.toMap()
	}
	if s.Logging != nil {
		m["logging"] = s.Logging.toMap()
	}
	if s.VpcConfig != nil {
		m["vpc_config"] = s.VpcConfig.toMap()
	}
	return m
}

// AwsKinesisFirehoseRedshiftDestination configures delivery to an Amazon
//
//	Redshift data warehouse. Firehose stages records in an intermediate S3
//	bucket, then issues a Redshift COPY command to bulk-load the data into
//	the target table.
//
//	This two-stage process (S3 staging -> COPY) is the standard Redshift
//	ingestion pattern and provides excellent throughput for large data volumes.
//
//	Prerequisites:
//	- A Redshift cluster accessible from Firehose (public or via VPC)
//	- The target database and table must exist
//	- An S3 staging bucket with appropriate IAM permissions
type AwsKinesisFirehoseRedshiftDestinationInput struct {
	// JDBC URL of the Redshift cluster. Format:
	//    "jdbc:redshift://<endpoint>:<port>/<database>"
	//  Example: "jdbc:redshift://my-cluster.abcdef.us-east-1.redshift.amazonaws.com:5439/mydb"
	ClusterJdbcurl string `json:"cluster_jdbcurl,omitempty" jsonschema:"JDBC URL of the Redshift cluster. Format: 'jdbc:redshift://<endpoint>:<port>/<database>' Example: 'jdbc:redshift://my-cluster.abcdef.us-east-1.redshift.amazonaws.com:5439/mydb'"`
	// IAM role ARN granting Firehose permission to COPY from S3 to Redshift
	//  and write to the S3 staging bucket. Must have:
	//  - S3 read access to the staging bucket
	//  - Redshift COPY permission
	RoleArn string `json:"role_arn" jsonschema:"required,IAM role ARN granting Firehose permission to COPY from S3 to Redshift and write to the S3 staging bucket. Must have: - S3 read access to the staging bucket - Redshift COPY permission"`
	// Name of the target Redshift table for the COPY command.
	DataTableName string `json:"data_table_name,omitempty" jsonschema:"Name of the target Redshift table for the COPY command."`
	// Comma-separated list of column names for the COPY command. When set,
	//  only the specified columns are loaded. When absent, COPY loads into
	//  all columns in table order.
	DataTableColumns string `json:"data_table_columns,omitempty" jsonschema:"Comma-separated list of column names for the COPY command. When set; only the specified columns are loaded. When absent; COPY loads into all columns in table order."`
	// Additional COPY command options (e.g., "JSON 'auto'", "GZIP",
	//  "DELIMITER ','", "IGNOREHEADER 1"). Appended to the COPY command.
	CopyOptions string `json:"copy_options,omitempty" jsonschema:"Additional COPY command options (e.g.; 'JSON 'auto''; 'GZIP'; 'DELIMITER ';''; 'IGNOREHEADER 1'). Appended to the COPY command."`
	// Redshift database username for authentication.
	Username string `json:"username,omitempty" jsonschema:"Redshift database username for authentication."`
	// Redshift database password for authentication. Sensitive.
	//  Consider using AWS Secrets Manager for production workloads (not
	//  supported in v1 -- use direct credentials).
	Password string `json:"password,omitempty" jsonschema:"Redshift database password for authentication. Sensitive. Consider using AWS Secrets Manager for production workloads (not supported in v1 -- use direct credentials)."`
	// S3 configuration for the intermediate staging bucket. Firehose writes
	//  data to this S3 location, then issues a COPY command to load it into
	//  Redshift. This is NOT a backup -- it's the primary data path.
	S3Config *AwsKinesisFirehoseS3ConfigInput `json:"s3_config" jsonschema:"required,S3 configuration for the intermediate staging bucket. Firehose writes data to this S3 location; then issues a COPY command to load it into Redshift. This is NOT a backup -- it's the primary data path."`
	// Retry duration in seconds for failed Redshift COPY commands.
	//  Range: 0-7200. Default: 3600 seconds (1 hour).
	//  Redshift COPY can be slow, so a longer default is appropriate.
	RetryDurationInSeconds int32 `json:"retry_duration_in_seconds,omitempty" jsonschema:"Retry duration in seconds for failed Redshift COPY commands. Range: 0-7200. Default: 3600 seconds (1 hour). Redshift COPY can be slow; so a longer default is appropriate."`
	// S3 backup mode for source records (in addition to the staging S3).
	//  When "Enabled", a copy of the original records is written to
	//  s3_backup. Useful for auditing and reprocessing.
	//
	//  Valid values: "Disabled" (default), "Enabled".
	S3BackupMode string `json:"s3_backup_mode,omitempty" jsonschema:"S3 backup mode for source records (in addition to the staging S3). When 'Enabled'; a copy of the original records is written to s3_backup. Useful for auditing and reprocessing. Valid values: 'Disabled..."`
	// S3 configuration for source record backup. Required when
	//  s3_backup_mode is "Enabled".
	S3Backup *AwsKinesisFirehoseS3ConfigInput `json:"s3_backup,omitempty" jsonschema:"S3 configuration for source record backup. Required when s3_backup_mode is 'Enabled'."`
	// Lambda-based record transformation before staging to S3.
	Processing *AwsKinesisFirehoseLambdaProcessingInput `json:"processing,omitempty" jsonschema:"Lambda-based record transformation before staging to S3."`
	// CloudWatch error logging for Redshift COPY failures.
	Logging *AwsKinesisFirehoseCloudwatchLoggingInput `json:"logging,omitempty" jsonschema:"CloudWatch error logging for Redshift COPY failures."`
}

func (s *AwsKinesisFirehoseRedshiftDestinationInput) validate() error {
	if s.RoleArn == "" {
		return fmt.Errorf("role_arn is required")
	}
	if s.S3Config == nil {
		return fmt.Errorf("s3_config is required")
	}
	if s.S3Config != nil {
		if err := s.S3Config.validate(); err != nil {
			return fmt.Errorf("s3_config: %w", err)
		}
	}
	if s.S3Backup != nil {
		if err := s.S3Backup.validate(); err != nil {
			return fmt.Errorf("s3_backup: %w", err)
		}
	}
	if s.Processing != nil {
		if err := s.Processing.validate(); err != nil {
			return fmt.Errorf("processing: %w", err)
		}
	}
	if s.Logging != nil {
		if err := s.Logging.validate(); err != nil {
			return fmt.Errorf("logging: %w", err)
		}
	}
	return nil
}

func (s *AwsKinesisFirehoseRedshiftDestinationInput) applyDefaults() {
	if s.S3Config != nil {
		s.S3Config.applyDefaults()
	}
	if s.S3Backup != nil {
		s.S3Backup.applyDefaults()
	}
	if s.Processing != nil {
		s.Processing.applyDefaults()
	}
	if s.Logging != nil {
		s.Logging.applyDefaults()
	}
}

func (s *AwsKinesisFirehoseRedshiftDestinationInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.ClusterJdbcurl != "" {
		m["cluster_jdbcurl"] = s.ClusterJdbcurl
	}
	m["role_arn"] = s.RoleArn
	if s.DataTableName != "" {
		m["data_table_name"] = s.DataTableName
	}
	if s.DataTableColumns != "" {
		m["data_table_columns"] = s.DataTableColumns
	}
	if s.CopyOptions != "" {
		m["copy_options"] = s.CopyOptions
	}
	if s.Username != "" {
		m["username"] = s.Username
	}
	if s.Password != "" {
		m["password"] = s.Password
	}
	if s.S3Config != nil {
		m["s3_config"] = s.S3Config.toMap()
	}
	if s.RetryDurationInSeconds != 0 {
		m["retry_duration_in_seconds"] = s.RetryDurationInSeconds
	}
	if s.S3BackupMode != "" {
		m["s3_backup_mode"] = s.S3BackupMode
	}
	if s.S3Backup != nil {
		m["s3_backup"] = s.S3Backup.toMap()
	}
	if s.Processing != nil {
		m["processing"] = s.Processing.toMap()
	}
	if s.Logging != nil {
		m["logging"] = s.Logging.toMap()
	}
	return m
}

// AwsKinesisFirehoseRequestAttribute is a key-value pair sent as a custom
//
//	HTTP header with every request to the HTTP endpoint.
type AwsKinesisFirehoseRequestAttributeInput struct {
	// Header name.
	Name string `json:"name,omitempty" jsonschema:"Header name."`
	// Header value.
	Value string `json:"value,omitempty" jsonschema:"Header value."`
}

func (s *AwsKinesisFirehoseRequestAttributeInput) validate() error {
	return nil
}

func (s *AwsKinesisFirehoseRequestAttributeInput) applyDefaults() {
}

func (s *AwsKinesisFirehoseRequestAttributeInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.Name != "" {
		m["name"] = s.Name
	}
	if s.Value != "" {
		m["value"] = s.Value
	}
	return m
}

// AwsKinesisFirehoseRequestConfig customizes the HTTP request sent to the
//
//	endpoint. Allows setting content encoding and custom headers.
type AwsKinesisFirehoseRequestConfigInput struct {
	// Content encoding for the HTTP request body.
	//  Valid values: "NONE" (default), "GZIP".
	//  GZIP reduces payload size but adds CPU overhead.
	ContentEncoding string `json:"content_encoding,omitempty" jsonschema:"Content encoding for the HTTP request body. Valid values: 'NONE' (default); 'GZIP'. GZIP reduces payload size but adds CPU overhead."`
	// Custom key-value pairs sent as HTTP headers with every request.
	//  Use this for endpoint-specific metadata (e.g., dataset name,
	//  environment identifier, API version).
	CommonAttributes []*AwsKinesisFirehoseRequestAttributeInput `json:"common_attributes,omitempty" jsonschema:"Custom key-value pairs sent as HTTP headers with every request. Use this for endpoint-specific metadata (e.g.; dataset name; environment identifier; API version)."`
}

func (s *AwsKinesisFirehoseRequestConfigInput) validate() error {
	for i, v := range s.CommonAttributes {
		if v != nil {
			if err := v.validate(); err != nil {
				return fmt.Errorf("common_attributes[%d]: %w", i, err)
			}
		}
	}
	return nil
}

func (s *AwsKinesisFirehoseRequestConfigInput) applyDefaults() {
}

func (s *AwsKinesisFirehoseRequestConfigInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.ContentEncoding != "" {
		m["content_encoding"] = s.ContentEncoding
	}
	if len(s.CommonAttributes) > 0 {
		items := make([]any, len(s.CommonAttributes))
		for i, v := range s.CommonAttributes {
			if v != nil {
				items[i] = v.toMap()
			}
		}
		m["common_attributes"] = items
	}
	return m
}

// AwsKinesisFirehoseS3Config defines the S3 configuration used for backup or
//
//	error delivery. All non-S3 destinations (OpenSearch, HTTP endpoint, Redshift)
//	require an S3 configuration to store failed records. Redshift also uses S3
//	as an intermediate staging area for COPY operations.
type AwsKinesisFirehoseS3ConfigInput struct {
	// S3 bucket ARN where records are delivered.
	BucketArn string `json:"bucket_arn" jsonschema:"required,S3 bucket ARN where records are delivered."`
	// IAM role ARN that grants Firehose permission to write to the S3 bucket.
	//  The role must have s3:PutObject, s3:AbortMultipartUpload,
	//  s3:GetBucketLocation, and s3:ListBucket permissions.
	RoleArn string `json:"role_arn" jsonschema:"required,IAM role ARN that grants Firehose permission to write to the S3 bucket. The role must have s3:PutObject; s3:AbortMultipartUpload; s3:GetBucketLocation; and s3:ListBucket permissions."`
	// S3 key prefix prepended to delivered objects. Supports Firehose expression
	//  syntax for dynamic prefixes (e.g., "errors/year=!{timestamp:yyyy}/").
	Prefix string `json:"prefix,omitempty" jsonschema:"S3 key prefix prepended to delivered objects. Supports Firehose expression syntax for dynamic prefixes (e.g.; 'errors/year=!{timestamp:yyyy}/')."`
	// S3 key prefix for error output. When Firehose cannot deliver or transform
	//  a record, it writes to this prefix. Uses the same expression syntax as prefix.
	ErrorOutputPrefix string `json:"error_output_prefix,omitempty" jsonschema:"S3 key prefix for error output. When Firehose cannot deliver or transform a record; it writes to this prefix. Uses the same expression syntax as prefix."`
	// Compression format for delivered objects. Applied before writing to S3.
	//  Valid values: "UNCOMPRESSED", "GZIP", "ZIP", "Snappy", "HADOOP_SNAPPY".
	//  Default: "UNCOMPRESSED".
	CompressionFormat string `json:"compression_format,omitempty" jsonschema:"Compression format for delivered objects. Applied before writing to S3. Valid values: 'UNCOMPRESSED'; 'GZIP'; 'ZIP'; 'Snappy'; 'HADOOP_SNAPPY'. Default: 'UNCOMPRESSED'."`
	// Customer-managed KMS key ARN for S3 server-side encryption (SSE-KMS).
	//  When absent, S3 uses its default encryption settings (SSE-S3 or bucket
	//  default encryption).
	KmsKeyArn string `json:"kms_key_arn,omitempty" jsonschema:"Customer-managed KMS key ARN for S3 server-side encryption (SSE-KMS). When absent; S3 uses its default encryption settings (SSE-S3 or bucket default encryption)."`
	// Buffering hints for S3 delivery.
	Buffering *AwsKinesisFirehoseBufferingHintsInput `json:"buffering,omitempty" jsonschema:"Buffering hints for S3 delivery."`
	// CloudWatch logging configuration for S3 delivery errors.
	Logging *AwsKinesisFirehoseCloudwatchLoggingInput `json:"logging,omitempty" jsonschema:"CloudWatch logging configuration for S3 delivery errors."`
}

func (s *AwsKinesisFirehoseS3ConfigInput) validate() error {
	if s.BucketArn == "" {
		return fmt.Errorf("bucket_arn is required")
	}
	if s.RoleArn == "" {
		return fmt.Errorf("role_arn is required")
	}
	if s.Buffering != nil {
		if err := s.Buffering.validate(); err != nil {
			return fmt.Errorf("buffering: %w", err)
		}
	}
	if s.Logging != nil {
		if err := s.Logging.validate(); err != nil {
			return fmt.Errorf("logging: %w", err)
		}
	}
	return nil
}

func (s *AwsKinesisFirehoseS3ConfigInput) applyDefaults() {
	if s.Buffering != nil {
		s.Buffering.applyDefaults()
	}
	if s.Logging != nil {
		s.Logging.applyDefaults()
	}
}

func (s *AwsKinesisFirehoseS3ConfigInput) toMap() map[string]any {
	m := make(map[string]any)
	m["bucket_arn"] = s.BucketArn
	m["role_arn"] = s.RoleArn
	if s.Prefix != "" {
		m["prefix"] = s.Prefix
	}
	if s.ErrorOutputPrefix != "" {
		m["error_output_prefix"] = s.ErrorOutputPrefix
	}
	if s.CompressionFormat != "" {
		m["compression_format"] = s.CompressionFormat
	}
	if s.KmsKeyArn != "" {
		m["kms_key_arn"] = s.KmsKeyArn
	}
	if s.Buffering != nil {
		m["buffering"] = s.Buffering.toMap()
	}
	if s.Logging != nil {
		m["logging"] = s.Logging.toMap()
	}
	return m
}

// AwsKinesisFirehoseVpcConfig configures VPC delivery for OpenSearch
//
//	destinations. When set, Firehose creates ENIs in the specified subnets
//	to deliver data within the VPC. The entire VPC configuration is ForceNew.
type AwsKinesisFirehoseVpcConfigInput struct {
	// Subnet IDs where Firehose creates ENIs for VPC delivery. Provide at
	//  least one subnet. For high availability, use subnets in multiple AZs.
	SubnetIds []string `json:"subnet_ids,omitempty" jsonschema:"Subnet IDs where Firehose creates ENIs for VPC delivery. Provide at least one subnet. For high availability; use subnets in multiple AZs."`
	// Security group IDs applied to the ENIs. Must allow outbound HTTPS (443)
	//  traffic to the OpenSearch domain.
	SecurityGroupIds []string `json:"security_group_ids,omitempty" jsonschema:"Security group IDs applied to the ENIs. Must allow outbound HTTPS (443) traffic to the OpenSearch domain."`
	// IAM role ARN for Firehose to manage VPC ENIs. The role must have
	//  ec2:CreateNetworkInterface, ec2:DescribeNetworkInterfaces,
	//  ec2:DeleteNetworkInterface, and ec2:DescribeVpcs permissions.
	RoleArn string `json:"role_arn" jsonschema:"required,IAM role ARN for Firehose to manage VPC ENIs. The role must have ec2:CreateNetworkInterface; ec2:DescribeNetworkInterfaces; ec2:DeleteNetworkInterface; and ec2:DescribeVpcs permissions."`
}

func (s *AwsKinesisFirehoseVpcConfigInput) validate() error {
	if s.RoleArn == "" {
		return fmt.Errorf("role_arn is required")
	}
	return nil
}

func (s *AwsKinesisFirehoseVpcConfigInput) applyDefaults() {
}

func (s *AwsKinesisFirehoseVpcConfigInput) toMap() map[string]any {
	m := make(map[string]any)
	if len(s.SubnetIds) > 0 {
		m["subnet_ids"] = s.SubnetIds
	}
	if len(s.SecurityGroupIds) > 0 {
		m["security_group_ids"] = s.SecurityGroupIds
	}
	m["role_arn"] = s.RoleArn
	return m
}

// ParseAwsKinesisFirehose validates and normalizes a AwsKinesisFirehose cloud_object.
func ParseAwsKinesisFirehose(cloudObject map[string]any) (*structpb.Struct, error) {
	if err := parse.ValidateHeader(cloudObject, "aws.openmcf.org/v1", "AwsKinesisFirehose"); err != nil {
		return nil, err
	}

	specMap, err := parse.ExtractSpecMap(cloudObject)
	if err != nil {
		return nil, err
	}

	specBytes, err := json.Marshal(specMap)
	if err != nil {
		return nil, fmt.Errorf("marshal spec: %w", err)
	}

	var spec AwsKinesisFirehoseSpecInput
	if err := json.Unmarshal(specBytes, &spec); err != nil {
		return nil, fmt.Errorf("invalid spec: %w", err)
	}

	if err := spec.validate(); err != nil {
		return nil, err
	}

	spec.applyDefaults()

	return parse.RebuildCloudObject(cloudObject, spec.toMap())
}
