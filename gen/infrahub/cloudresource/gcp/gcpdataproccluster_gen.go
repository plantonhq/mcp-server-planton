// Code generated by schema2go. DO NOT EDIT.
// Generated: 2026-02-27T22:24:53+05:30

package gcp

import (
	"encoding/json"
	"fmt"

	"github.com/plantonhq/mcp-server-planton/internal/parse"
	"google.golang.org/protobuf/types/known/structpb"
)

var (
	_ = json.Marshal
	_ = fmt.Errorf
	_ = parse.ValidateHeader
	_ = (*structpb.Struct)(nil)
)

// gcp-dataproc-cluster
type GcpDataprocClusterSpecInput struct {
	// GCP project where the Dataproc cluster will be created.
	ProjectId string `json:"project_id" jsonschema:"required,GCP project where the Dataproc cluster will be created."`
	// GCP region for the cluster (e.g., "us-central1", "europe-west1").
	//  All cluster nodes will be placed in this region.
	//  Immutable after creation.
	Region string `json:"region" jsonschema:"required,GCP region for the cluster (e.g.; 'us-central1'; 'europe-west1'). All cluster nodes will be placed in this region. Immutable after creation."`
	// Name of the Dataproc cluster. Must start with a lowercase letter,
	//  can contain lowercase letters, numbers, and hyphens, and must end
	//  with a lowercase letter or number. Maximum 55 characters.
	//  Immutable after creation.
	ClusterName string `json:"cluster_name" jsonschema:"required,Name of the Dataproc cluster. Must start with a lowercase letter; can contain lowercase letters; numbers; and hyphens; and must end with a lowercase letter or number. Maximum 55 characters. Immutable ..."`
	// Cluster infrastructure configuration including nodes, software,
	//  networking, encryption, and lifecycle management.
	ClusterConfig *GcpDataprocClusterConfigInput `json:"cluster_config,omitempty" jsonschema:"Cluster infrastructure configuration including nodes; software; networking; encryption; and lifecycle management."`
	// Timeout for graceful YARN decommissioning when reducing the number
	//  of workers. During this period, YARN waits for running tasks to
	//  complete before shutting down nodes. Without this, scaling down
	//  can terminate running jobs.
	//  Format: duration in seconds with 's' suffix (e.g., "3600s").
	//  Default: "0...
	GracefulDecommissionTimeout string `json:"graceful_decommission_timeout,omitempty" jsonschema:"Timeout for graceful YARN decommissioning when reducing the number of workers. During this period; YARN waits for running tasks to complete before shutting down nodes. Without this; scaling down can t..."`
}

func (s *GcpDataprocClusterSpecInput) validate() error {
	if s.ProjectId == "" {
		return fmt.Errorf("project_id is required")
	}
	if s.Region == "" {
		return fmt.Errorf("region is required")
	}
	if s.ClusterName == "" {
		return fmt.Errorf("cluster_name is required")
	}
	if s.ClusterConfig != nil {
		if err := s.ClusterConfig.validate(); err != nil {
			return fmt.Errorf("cluster_config: %w", err)
		}
	}
	return nil
}

func (s *GcpDataprocClusterSpecInput) applyDefaults() {
	if s.ClusterConfig != nil {
		s.ClusterConfig.applyDefaults()
	}
}

func (s *GcpDataprocClusterSpecInput) toMap() map[string]any {
	m := make(map[string]any)
	m["project_id"] = s.ProjectId
	m["region"] = s.Region
	m["cluster_name"] = s.ClusterName
	if s.ClusterConfig != nil {
		m["cluster_config"] = s.ClusterConfig.toMap()
	}
	if s.GracefulDecommissionTimeout != "" {
		m["graceful_decommission_timeout"] = s.GracefulDecommissionTimeout
	}
	return m
}

// GcpDataprocClusterAccelerator configures GPU or TPU accelerators
//
//	attached to Dataproc cluster nodes for ML and compute workloads.
type GcpDataprocClusterAcceleratorInput struct {
	// Full accelerator type name (e.g., "nvidia-tesla-t4", "nvidia-tesla-v100").
	//  See https://cloud.google.com/compute/docs/gpus for available types.
	AcceleratorType string `json:"accelerator_type" jsonschema:"required,Full accelerator type name (e.g.; 'nvidia-tesla-t4'; 'nvidia-tesla-v100'). See https://cloud.google.com/compute/docs/gpus for available types."`
	// Number of accelerators to attach. Must be at least 1.
	AcceleratorCount int32 `json:"accelerator_count,omitempty" jsonschema:"Number of accelerators to attach. Must be at least 1."`
}

func (s *GcpDataprocClusterAcceleratorInput) validate() error {
	if s.AcceleratorType == "" {
		return fmt.Errorf("accelerator_type is required")
	}
	return nil
}

func (s *GcpDataprocClusterAcceleratorInput) applyDefaults() {
}

func (s *GcpDataprocClusterAcceleratorInput) toMap() map[string]any {
	m := make(map[string]any)
	m["accelerator_type"] = s.AcceleratorType
	if s.AcceleratorCount != 0 {
		m["accelerator_count"] = s.AcceleratorCount
	}
	return m
}

// GcpDataprocClusterConfig wraps all cluster infrastructure configuration.
//
//	This message mirrors the Terraform/Pulumi cluster_config structure,
//	keeping the API surface familiar to infrastructure engineers coming
//	from Terraform or the GCP console. All fields are optional -- GCP
//	provides sensible defaults for a basic cluster when fields are omitted.
type GcpDataprocClusterConfigInput struct {
	// Cloud Storage bucket for staging job dependencies, jar files, and
	//  other temporary data. If not specified, GCP auto-creates a staging
	//  bucket in the cluster's project and region.
	StagingBucket string `json:"staging_bucket,omitempty" jsonschema:"Cloud Storage bucket for staging job dependencies; jar files; and other temporary data. If not specified; GCP auto-creates a staging bucket in the cluster's project and region."`
	// Cloud Storage bucket for ephemeral cluster data (shuffle, spill).
	//  If not specified, GCP auto-creates a temp bucket.
	TempBucket string `json:"temp_bucket,omitempty" jsonschema:"Cloud Storage bucket for ephemeral cluster data (shuffle; spill). If not specified; GCP auto-creates a temp bucket."`
	// Compute Engine configuration for the cluster's nodes (networking,
	//  service account, zone, tags, metadata).
	GceConfig *GcpDataprocClusterGceConfigInput `json:"gce_config,omitempty" jsonschema:"Compute Engine configuration for the cluster's nodes (networking; service account; zone; tags; metadata)."`
	// Master node configuration. If not specified, GCP defaults to
	//  1 master with n2-standard-4 and 500 GB pd-standard disk.
	MasterConfig *GcpDataprocClusterMasterConfigInput `json:"master_config,omitempty" jsonschema:"Master node configuration. If not specified; GCP defaults to 1 master with n2-standard-4 and 500 GB pd-standard disk."`
	// Primary worker node configuration. If not specified, GCP defaults
	//  to 2 workers with n2-standard-4 and 500 GB pd-standard disk.
	WorkerConfig *GcpDataprocClusterWorkerConfigInput `json:"worker_config,omitempty" jsonschema:"Primary worker node configuration. If not specified; GCP defaults to 2 workers with n2-standard-4 and 500 GB pd-standard disk."`
	// Secondary (preemptible/spot) worker node configuration.
	//  If not specified, no secondary workers are created.
	SecondaryWorkerConfig *GcpDataprocClusterSecondaryWorkerConfigInput `json:"secondary_worker_config,omitempty" jsonschema:"Secondary (preemptible/spot) worker node configuration. If not specified; no secondary workers are created."`
	// Software configuration including Dataproc image version, optional
	//  components, and framework property overrides.
	SoftwareConfig *GcpDataprocClusterSoftwareConfigInput `json:"software_config,omitempty" jsonschema:"Software configuration including Dataproc image version; optional components; and framework property overrides."`
	// Initialization actions (startup scripts) that run on all nodes
	//  when the cluster is created.
	InitializationActions []*GcpDataprocClusterInitActionInput `json:"initialization_actions,omitempty" jsonschema:"Initialization actions (startup scripts) that run on all nodes when the cluster is created."`
	// URI of an existing Dataproc autoscaling policy to apply to the
	//  cluster. Format: projects/{project}/locations/{location}/autoscalingPolicies/{policy}
	//  The autoscaling policy must be created separately.
	AutoscalingPolicyUri string `json:"autoscaling_policy_uri,omitempty" jsonschema:"URI of an existing Dataproc autoscaling policy to apply to the cluster. Format: projects/{project}/locations/{location}/autoscalingPolicies/{policy} The autoscaling policy must be created separately."`
	// Cloud KMS key for encrypting persistent disks attached to cluster
	//  nodes (CMEK). Format: projects/{project}/locations/{location}/keyRings/{keyRing}/cryptoKeys/{key}
	//  If not specified, disks are encrypted with Google-managed keys.
	EncryptionKmsKeyName string `json:"encryption_kms_key_name,omitempty" jsonschema:"Cloud KMS key for encrypting persistent disks attached to cluster nodes (CMEK). Format: projects/{project}/locations/{location}/keyRings/{keyRing}/cryptoKeys/{key} If not specified; disks are encrypte..."`
	// Component Gateway configuration for authenticated web UI access.
	EndpointConfig *GcpDataprocClusterEndpointConfigInput `json:"endpoint_config,omitempty" jsonschema:"Component Gateway configuration for authenticated web UI access."`
	// Lifecycle configuration for automatic cluster shutdown and deletion.
	//  Critical for cost management of ephemeral batch processing clusters.
	LifecycleConfig *GcpDataprocClusterLifecycleConfigInput `json:"lifecycle_config,omitempty" jsonschema:"Lifecycle configuration for automatic cluster shutdown and deletion. Critical for cost management of ephemeral batch processing clusters."`
}

func (s *GcpDataprocClusterConfigInput) validate() error {
	if s.GceConfig != nil {
		if err := s.GceConfig.validate(); err != nil {
			return fmt.Errorf("gce_config: %w", err)
		}
	}
	if s.MasterConfig != nil {
		if err := s.MasterConfig.validate(); err != nil {
			return fmt.Errorf("master_config: %w", err)
		}
	}
	if s.WorkerConfig != nil {
		if err := s.WorkerConfig.validate(); err != nil {
			return fmt.Errorf("worker_config: %w", err)
		}
	}
	if s.SecondaryWorkerConfig != nil {
		if err := s.SecondaryWorkerConfig.validate(); err != nil {
			return fmt.Errorf("secondary_worker_config: %w", err)
		}
	}
	if s.SoftwareConfig != nil {
		if err := s.SoftwareConfig.validate(); err != nil {
			return fmt.Errorf("software_config: %w", err)
		}
	}
	for i, v := range s.InitializationActions {
		if v != nil {
			if err := v.validate(); err != nil {
				return fmt.Errorf("initialization_actions[%d]: %w", i, err)
			}
		}
	}
	if s.EndpointConfig != nil {
		if err := s.EndpointConfig.validate(); err != nil {
			return fmt.Errorf("endpoint_config: %w", err)
		}
	}
	if s.LifecycleConfig != nil {
		if err := s.LifecycleConfig.validate(); err != nil {
			return fmt.Errorf("lifecycle_config: %w", err)
		}
	}
	return nil
}

func (s *GcpDataprocClusterConfigInput) applyDefaults() {
	if s.GceConfig != nil {
		s.GceConfig.applyDefaults()
	}
	if s.MasterConfig != nil {
		s.MasterConfig.applyDefaults()
	}
	if s.WorkerConfig != nil {
		s.WorkerConfig.applyDefaults()
	}
	if s.SecondaryWorkerConfig != nil {
		s.SecondaryWorkerConfig.applyDefaults()
	}
	if s.SoftwareConfig != nil {
		s.SoftwareConfig.applyDefaults()
	}
	if s.EndpointConfig != nil {
		s.EndpointConfig.applyDefaults()
	}
	if s.LifecycleConfig != nil {
		s.LifecycleConfig.applyDefaults()
	}
}

func (s *GcpDataprocClusterConfigInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.StagingBucket != "" {
		m["staging_bucket"] = s.StagingBucket
	}
	if s.TempBucket != "" {
		m["temp_bucket"] = s.TempBucket
	}
	if s.GceConfig != nil {
		m["gce_config"] = s.GceConfig.toMap()
	}
	if s.MasterConfig != nil {
		m["master_config"] = s.MasterConfig.toMap()
	}
	if s.WorkerConfig != nil {
		m["worker_config"] = s.WorkerConfig.toMap()
	}
	if s.SecondaryWorkerConfig != nil {
		m["secondary_worker_config"] = s.SecondaryWorkerConfig.toMap()
	}
	if s.SoftwareConfig != nil {
		m["software_config"] = s.SoftwareConfig.toMap()
	}
	if len(s.InitializationActions) > 0 {
		items := make([]any, len(s.InitializationActions))
		for i, v := range s.InitializationActions {
			if v != nil {
				items[i] = v.toMap()
			}
		}
		m["initialization_actions"] = items
	}
	if s.AutoscalingPolicyUri != "" {
		m["autoscaling_policy_uri"] = s.AutoscalingPolicyUri
	}
	if s.EncryptionKmsKeyName != "" {
		m["encryption_kms_key_name"] = s.EncryptionKmsKeyName
	}
	if s.EndpointConfig != nil {
		m["endpoint_config"] = s.EndpointConfig.toMap()
	}
	if s.LifecycleConfig != nil {
		m["lifecycle_config"] = s.LifecycleConfig.toMap()
	}
	return m
}

// GcpDataprocClusterDiskConfig configures the boot disk and local SSDs
//
//	for Dataproc cluster node groups (master, worker, secondary worker).
//
//	The boot disk type defaults to "pd-standard" in GCP when not specified.
//	Local SSDs provide high-performance scratch storage for shuffle data.
type GcpDataprocClusterDiskConfigInput struct {
	// Size of the boot disk in GB. Minimum 10 GB.
	//  If not specified, GCP defaults to 500 GB for master and worker nodes.
	BootDiskSizeGb int32 `json:"boot_disk_size_gb,omitempty" jsonschema:"Size of the boot disk in GB. Minimum 10 GB. If not specified; GCP defaults to 500 GB for master and worker nodes."`
	// Boot disk type. Valid values: "pd-standard", "pd-ssd", "pd-balanced".
	//  If not specified, GCP defaults to "pd-standard".
	BootDiskType string `json:"boot_disk_type,omitempty" jsonschema:"Boot disk type. Valid values: 'pd-standard'; 'pd-ssd'; 'pd-balanced'. If not specified; GCP defaults to 'pd-standard'."`
	// Number of local SSDs to attach. Each local SSD is 375 GB.
	//  Default: 0 (no local SSDs).
	NumLocalSsds int32 `json:"num_local_ssds,omitempty" jsonschema:"Number of local SSDs to attach. Each local SSD is 375 GB. Default: 0 (no local SSDs)."`
}

func (s *GcpDataprocClusterDiskConfigInput) validate() error {
	return nil
}

func (s *GcpDataprocClusterDiskConfigInput) applyDefaults() {
}

func (s *GcpDataprocClusterDiskConfigInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.BootDiskSizeGb != 0 {
		m["boot_disk_size_gb"] = s.BootDiskSizeGb
	}
	if s.BootDiskType != "" {
		m["boot_disk_type"] = s.BootDiskType
	}
	if s.NumLocalSsds != 0 {
		m["num_local_ssds"] = s.NumLocalSsds
	}
	return m
}

// GcpDataprocClusterEndpointConfig controls access to the Dataproc
//
//	Component Gateway, which provides authenticated web UI access to
//	cluster services like Spark UI, YARN ResourceManager, HDFS NameNode,
//	Jupyter, and Zeppelin.
type GcpDataprocClusterEndpointConfigInput struct {
	// Whether to enable the Dataproc Component Gateway for web UI access.
	//  When enabled, GCP creates authenticated HTTPS endpoints for each
	//  cluster component. Requires the cluster to have external IP access
	//  or appropriate Private Google Access configuration.
	EnableHttpPortAccess bool `json:"enable_http_port_access,omitempty" jsonschema:"Whether to enable the Dataproc Component Gateway for web UI access. When enabled; GCP creates authenticated HTTPS endpoints for each cluster component. Requires the cluster to have external IP access ..."`
}

func (s *GcpDataprocClusterEndpointConfigInput) validate() error {
	return nil
}

func (s *GcpDataprocClusterEndpointConfigInput) applyDefaults() {
}

func (s *GcpDataprocClusterEndpointConfigInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.EnableHttpPortAccess {
		m["enable_http_port_access"] = s.EnableHttpPortAccess
	}
	return m
}

// GcpDataprocClusterGceConfig defines the Compute Engine environment
//
//	for the Dataproc cluster's nodes, including networking, identity, and
//	instance metadata.
//
//	Either network or subnetwork may be specified, but not both.
//	If neither is specified, the cluster uses the project's default VPC.
type GcpDataprocClusterGceConfigInput struct {
	// VPC network for the cluster's nodes. Mutually exclusive with subnetwork.
	//  Expects a network self-link or resource reference.
	Network string `json:"network,omitempty" jsonschema:"VPC network for the cluster's nodes. Mutually exclusive with subnetwork. Expects a network self-link or resource reference."`
	// VPC subnetwork for the cluster's nodes. Mutually exclusive with network.
	//  Using a subnetwork is recommended for production clusters with
	//  controlled IP ranges.
	Subnetwork string `json:"subnetwork,omitempty" jsonschema:"VPC subnetwork for the cluster's nodes. Mutually exclusive with network. Using a subnetwork is recommended for production clusters with controlled IP ranges."`
	// Service account for the cluster's VMs. If not specified, the default
	//  Compute Engine service account is used. A custom service account with
	//  minimal permissions is recommended for production.
	ServiceAccount string `json:"service_account,omitempty" jsonschema:"Service account for the cluster's VMs. If not specified; the default Compute Engine service account is used. A custom service account with minimal permissions is recommended for production."`
	// OAuth 2.0 scopes for the service account. If not specified, GCP uses
	//  a default set of scopes. Override only when you need to restrict or
	//  expand API access.
	ServiceAccountScopes []string `json:"service_account_scopes,omitempty" jsonschema:"OAuth 2.0 scopes for the service account. If not specified; GCP uses a default set of scopes. Override only when you need to restrict or expand API access."`
	// GCP zone within the region for node placement. If not specified,
	//  GCP auto-selects a zone within the cluster's region.
	Zone string `json:"zone,omitempty" jsonschema:"GCP zone within the region for node placement. If not specified; GCP auto-selects a zone within the cluster's region."`
	// Whether to use only internal IP addresses for cluster nodes.
	//  When true, nodes have no external IP and require Cloud NAT or
	//  Private Google Access for internet connectivity.
	//  Recommended for production to reduce attack surface.
	InternalIpOnly bool `json:"internal_ip_only,omitempty" jsonschema:"Whether to use only internal IP addresses for cluster nodes. When true; nodes have no external IP and require Cloud NAT or Private Google Access for internet connectivity. Recommended for production t..."`
	// GCE network tags applied to all cluster nodes. Useful for
	//  firewall rule targeting.
	Tags []string `json:"tags,omitempty" jsonschema:"GCE network tags applied to all cluster nodes. Useful for firewall rule targeting."`
	// Compute Engine metadata key-value pairs applied to all cluster nodes.
	//  Common use: startup scripts, environment variables for init actions.
	Metadata map[string]string `json:"metadata,omitempty" jsonschema:"Compute Engine metadata key-value pairs applied to all cluster nodes. Common use: startup scripts; environment variables for init actions."`
}

func (s *GcpDataprocClusterGceConfigInput) validate() error {
	return nil
}

func (s *GcpDataprocClusterGceConfigInput) applyDefaults() {
}

func (s *GcpDataprocClusterGceConfigInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.Network != "" {
		m["network"] = s.Network
	}
	if s.Subnetwork != "" {
		m["subnetwork"] = s.Subnetwork
	}
	if s.ServiceAccount != "" {
		m["service_account"] = s.ServiceAccount
	}
	if len(s.ServiceAccountScopes) > 0 {
		m["service_account_scopes"] = s.ServiceAccountScopes
	}
	if s.Zone != "" {
		m["zone"] = s.Zone
	}
	if s.InternalIpOnly {
		m["internal_ip_only"] = s.InternalIpOnly
	}
	if len(s.Tags) > 0 {
		m["tags"] = s.Tags
	}
	if len(s.Metadata) > 0 {
		m["metadata"] = s.Metadata
	}
	return m
}

// GcpDataprocClusterInitAction defines an initialization action (startup
//
//	script) that runs on all nodes when the cluster is created.
//
//	Initialization actions are commonly used to install additional software,
//	configure networking, or set up monitoring agents.
type GcpDataprocClusterInitActionInput struct {
	// GCS URI of the initialization script (must start with "gs://").
	Script string `json:"script" jsonschema:"required,GCS URI of the initialization script (must start with 'gs://')."`
	// Maximum time (in seconds) the script is allowed to run before
	//  being forcefully terminated. Default: 300 (5 minutes).
	TimeoutSec int32 `json:"timeout_sec,omitempty" jsonschema:"Maximum time (in seconds) the script is allowed to run before being forcefully terminated. Default: 300 (5 minutes)."`
}

func (s *GcpDataprocClusterInitActionInput) validate() error {
	if s.Script == "" {
		return fmt.Errorf("script is required")
	}
	return nil
}

func (s *GcpDataprocClusterInitActionInput) applyDefaults() {
}

func (s *GcpDataprocClusterInitActionInput) toMap() map[string]any {
	m := make(map[string]any)
	m["script"] = s.Script
	if s.TimeoutSec != 0 {
		m["timeout_sec"] = s.TimeoutSec
	}
	return m
}

// GcpDataprocClusterLifecycleConfig controls automatic cluster shutdown
//
//	and deletion, which is critical for cost management of ephemeral
//	Spark/Hadoop clusters.
//
//	Dataproc clusters are often ephemeral -- spun up for a batch job,
//	then torn down. Without lifecycle configuration, idle clusters
//	continue to incur costs indefinitely.
type GcpDataprocClusterLifecycleConfigInput struct {
	// Duration of inactivity after which the cluster is automatically
	//  deleted. Format: duration in seconds with 's' suffix (e.g., "1800s"
	//  for 30 minutes). Valid range: 10 minutes to 14 days.
	//
	//  A cluster is considered idle when no jobs are running and no
	//  interactive sessions are active.
	IdleDeleteTtl string `json:"idle_delete_ttl,omitempty" jsonschema:"Duration of inactivity after which the cluster is automatically deleted. Format: duration in seconds with 's' suffix (e.g.; '1800s' for 30 minutes). Valid range: 10 minutes to 14 days. A cluster is co..."`
	// RFC3339 timestamp at which the cluster is automatically deleted,
	//  regardless of activity (e.g., "2026-03-01T00:00:00Z").
	//  Useful for time-boxed clusters with a known end date.
	AutoDeleteTime string `json:"auto_delete_time,omitempty" jsonschema:"RFC3339 timestamp at which the cluster is automatically deleted; regardless of activity (e.g.; '2026-03-01T00:00:00Z'). Useful for time-boxed clusters with a known end date."`
}

func (s *GcpDataprocClusterLifecycleConfigInput) validate() error {
	return nil
}

func (s *GcpDataprocClusterLifecycleConfigInput) applyDefaults() {
}

func (s *GcpDataprocClusterLifecycleConfigInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.IdleDeleteTtl != "" {
		m["idle_delete_ttl"] = s.IdleDeleteTtl
	}
	if s.AutoDeleteTime != "" {
		m["auto_delete_time"] = s.AutoDeleteTime
	}
	return m
}

// GcpDataprocClusterMasterConfig defines the configuration for the
//
//	master node(s) of the Dataproc cluster.
//
//	For standard clusters, use 1 master. For high-availability clusters,
//	use 3 masters (GCP requirement for HA mode).
type GcpDataprocClusterMasterConfigInput struct {
	// Number of master instances. Valid values: 1 (standard) or 3 (HA).
	//  If not specified, GCP defaults to 1.
	NumInstances int32 `json:"num_instances,omitempty" jsonschema:"Number of master instances. Valid values: 1 (standard) or 3 (HA). If not specified; GCP defaults to 1."`
	// Compute Engine machine type (e.g., "n2-standard-4", "e2-standard-8").
	//  If not specified, GCP selects a default machine type.
	MachineType string `json:"machine_type,omitempty" jsonschema:"Compute Engine machine type (e.g.; 'n2-standard-4'; 'e2-standard-8'). If not specified; GCP selects a default machine type."`
	// Boot disk and local SSD configuration.
	DiskConfig *GcpDataprocClusterDiskConfigInput `json:"disk_config,omitempty" jsonschema:"Boot disk and local SSD configuration."`
	// GPU or TPU accelerators attached to master nodes.
	//  Typically not needed for masters unless running single-node ML workloads.
	Accelerators []*GcpDataprocClusterAcceleratorInput `json:"accelerators,omitempty" jsonschema:"GPU or TPU accelerators attached to master nodes. Typically not needed for masters unless running single-node ML workloads."`
	// Minimum CPU platform for the instances (e.g., "Intel Cascade Lake").
	//  Forces nodes onto a specific or newer CPU generation.
	MinCpuPlatform string `json:"min_cpu_platform,omitempty" jsonschema:"Minimum CPU platform for the instances (e.g.; 'Intel Cascade Lake'). Forces nodes onto a specific or newer CPU generation."`
	// Custom Dataproc image URI. If not specified, GCP uses the image
	//  determined by software_config.image_version.
	ImageUri string `json:"image_uri,omitempty" jsonschema:"Custom Dataproc image URI. If not specified; GCP uses the image determined by software_config.image_version."`
}

func (s *GcpDataprocClusterMasterConfigInput) validate() error {
	if s.DiskConfig != nil {
		if err := s.DiskConfig.validate(); err != nil {
			return fmt.Errorf("disk_config: %w", err)
		}
	}
	for i, v := range s.Accelerators {
		if v != nil {
			if err := v.validate(); err != nil {
				return fmt.Errorf("accelerators[%d]: %w", i, err)
			}
		}
	}
	return nil
}

func (s *GcpDataprocClusterMasterConfigInput) applyDefaults() {
	if s.DiskConfig != nil {
		s.DiskConfig.applyDefaults()
	}
}

func (s *GcpDataprocClusterMasterConfigInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.NumInstances != 0 {
		m["num_instances"] = s.NumInstances
	}
	if s.MachineType != "" {
		m["machine_type"] = s.MachineType
	}
	if s.DiskConfig != nil {
		m["disk_config"] = s.DiskConfig.toMap()
	}
	if len(s.Accelerators) > 0 {
		items := make([]any, len(s.Accelerators))
		for i, v := range s.Accelerators {
			if v != nil {
				items[i] = v.toMap()
			}
		}
		m["accelerators"] = items
	}
	if s.MinCpuPlatform != "" {
		m["min_cpu_platform"] = s.MinCpuPlatform
	}
	if s.ImageUri != "" {
		m["image_uri"] = s.ImageUri
	}
	return m
}

// GcpDataprocClusterSecondaryWorkerConfig defines the configuration
//
//	for secondary (preemptible or spot) worker nodes.
//
//	Secondary workers provide cost-optimized burst capacity for batch
//	workloads. They may be preempted by GCP at any time. Spot VMs are
//	the modern replacement for preemptible VMs with better pricing and
//	availability.
type GcpDataprocClusterSecondaryWorkerConfigInput struct {
	// Number of secondary worker instances. Default: 0.
	NumInstances int32 `json:"num_instances,omitempty" jsonschema:"Number of secondary worker instances. Default: 0."`
	// Preemptibility of the secondary workers.
	//  SPOT: modern spot VMs with dynamic pricing (recommended).
	//  PREEMPTIBLE: legacy preemptible VMs with fixed pricing.
	//  NON_PREEMPTIBLE: standard on-demand pricing (unusual for secondary workers).
	//  If not specified, GCP defaults to PREEMPTIBLE.
	Preemptibility string `json:"preemptibility,omitempty" jsonschema:"Preemptibility of the secondary workers. SPOT: modern spot VMs with dynamic pricing (recommended). PREEMPTIBLE: legacy preemptible VMs with fixed pricing. NON_PREEMPTIBLE: standard on-demand pricing (..."`
	// Boot disk and local SSD configuration for secondary workers.
	//  Secondary workers do not support custom machine types or accelerators;
	//  they inherit machine configuration from the primary worker config.
	DiskConfig *GcpDataprocClusterDiskConfigInput `json:"disk_config,omitempty" jsonschema:"Boot disk and local SSD configuration for secondary workers. Secondary workers do not support custom machine types or accelerators; they inherit machine configuration from the primary worker config."`
}

func (s *GcpDataprocClusterSecondaryWorkerConfigInput) validate() error {
	if s.DiskConfig != nil {
		if err := s.DiskConfig.validate(); err != nil {
			return fmt.Errorf("disk_config: %w", err)
		}
	}
	return nil
}

func (s *GcpDataprocClusterSecondaryWorkerConfigInput) applyDefaults() {
	if s.DiskConfig != nil {
		s.DiskConfig.applyDefaults()
	}
}

func (s *GcpDataprocClusterSecondaryWorkerConfigInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.NumInstances != 0 {
		m["num_instances"] = s.NumInstances
	}
	if s.Preemptibility != "" {
		m["preemptibility"] = s.Preemptibility
	}
	if s.DiskConfig != nil {
		m["disk_config"] = s.DiskConfig.toMap()
	}
	return m
}

// GcpDataprocClusterSoftwareConfig defines the Dataproc image version,
//
//	optional components, and Hadoop/Spark/YARN property overrides.
//
//	The image version determines the versions of Apache Spark, Hadoop,
//	Hive, Pig, and other components installed on the cluster.
type GcpDataprocClusterSoftwareConfigInput struct {
	// Dataproc image version (e.g., "2.2-debian12", "2.1-ubuntu20").
	//  See https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions
	//  If not specified, GCP uses the latest stable version.
	ImageVersion string `json:"image_version,omitempty" jsonschema:"Dataproc image version (e.g.; '2.2-debian12'; '2.1-ubuntu20'). See https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions If not specified; GCP uses the latest stable version."`
	// Optional components to install on the cluster. Common values:
	//  JUPYTER, DOCKER, PRESTO, ZEPPELIN, HIVE_WEBHCAT, FLINK, TRINO.
	//  See https://cloud.google.com/dataproc/docs/concepts/components/overview
	OptionalComponents []string `json:"optional_components,omitempty" jsonschema:"Optional components to install on the cluster. Common values: JUPYTER; DOCKER; PRESTO; ZEPPELIN; HIVE_WEBHCAT; FLINK; TRINO. See https://cloud.google.com/dataproc/docs/concepts/components/overview"`
	// Key-value pairs to override or set Hadoop, Spark, YARN, and other
	//  framework properties. Keys use the format "prefix:property", e.g.:
	//    "spark:spark.executor.memory" = "4g"
	//    "hdfs:dfs.replication" = "2"
	//    "yarn:yarn.nodemanager.resource.memory-mb" = "8192"
	//  See https://cloud.google.com/dataproc/...
	Properties map[string]string `json:"properties,omitempty" jsonschema:"Key-value pairs to override or set Hadoop; Spark; YARN; and other framework properties. Keys use the format 'prefix:property'; e.g.: 'spark:spark.executor.memory' = '4g' 'hdfs:dfs.replication' = '2' '..."`
}

func (s *GcpDataprocClusterSoftwareConfigInput) validate() error {
	return nil
}

func (s *GcpDataprocClusterSoftwareConfigInput) applyDefaults() {
}

func (s *GcpDataprocClusterSoftwareConfigInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.ImageVersion != "" {
		m["image_version"] = s.ImageVersion
	}
	if len(s.OptionalComponents) > 0 {
		m["optional_components"] = s.OptionalComponents
	}
	if len(s.Properties) > 0 {
		m["properties"] = s.Properties
	}
	return m
}

// GcpDataprocClusterWorkerConfig defines the configuration for the
//
//	primary worker nodes of the Dataproc cluster.
//
//	Primary workers are standard, on-demand VMs that persist for the
//	lifetime of the cluster. Use secondary workers for cost-optimized,
//	preemptible/spot capacity.
type GcpDataprocClusterWorkerConfigInput struct {
	// Number of primary worker instances.
	//  If not specified, GCP defaults to 2.
	NumInstances int32 `json:"num_instances,omitempty" jsonschema:"Number of primary worker instances. If not specified; GCP defaults to 2."`
	// Compute Engine machine type (e.g., "n2-standard-4", "e2-standard-8").
	//  If not specified, GCP selects a default machine type.
	MachineType string `json:"machine_type,omitempty" jsonschema:"Compute Engine machine type (e.g.; 'n2-standard-4'; 'e2-standard-8'). If not specified; GCP selects a default machine type."`
	// Boot disk and local SSD configuration.
	DiskConfig *GcpDataprocClusterDiskConfigInput `json:"disk_config,omitempty" jsonschema:"Boot disk and local SSD configuration."`
	// GPU or TPU accelerators attached to worker nodes.
	//  Common for Spark ML workloads using GPU-accelerated libraries.
	Accelerators []*GcpDataprocClusterAcceleratorInput `json:"accelerators,omitempty" jsonschema:"GPU or TPU accelerators attached to worker nodes. Common for Spark ML workloads using GPU-accelerated libraries."`
	// Minimum CPU platform for the instances.
	MinCpuPlatform string `json:"min_cpu_platform,omitempty" jsonschema:"Minimum CPU platform for the instances."`
	// Custom Dataproc image URI.
	ImageUri string `json:"image_uri,omitempty" jsonschema:"Custom Dataproc image URI."`
	// Minimum number of primary worker instances when autoscaling is active.
	//  The autoscaler will not scale below this threshold.
	MinNumInstances int32 `json:"min_num_instances,omitempty" jsonschema:"Minimum number of primary worker instances when autoscaling is active. The autoscaler will not scale below this threshold."`
}

func (s *GcpDataprocClusterWorkerConfigInput) validate() error {
	if s.DiskConfig != nil {
		if err := s.DiskConfig.validate(); err != nil {
			return fmt.Errorf("disk_config: %w", err)
		}
	}
	for i, v := range s.Accelerators {
		if v != nil {
			if err := v.validate(); err != nil {
				return fmt.Errorf("accelerators[%d]: %w", i, err)
			}
		}
	}
	return nil
}

func (s *GcpDataprocClusterWorkerConfigInput) applyDefaults() {
	if s.DiskConfig != nil {
		s.DiskConfig.applyDefaults()
	}
}

func (s *GcpDataprocClusterWorkerConfigInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.NumInstances != 0 {
		m["num_instances"] = s.NumInstances
	}
	if s.MachineType != "" {
		m["machine_type"] = s.MachineType
	}
	if s.DiskConfig != nil {
		m["disk_config"] = s.DiskConfig.toMap()
	}
	if len(s.Accelerators) > 0 {
		items := make([]any, len(s.Accelerators))
		for i, v := range s.Accelerators {
			if v != nil {
				items[i] = v.toMap()
			}
		}
		m["accelerators"] = items
	}
	if s.MinCpuPlatform != "" {
		m["min_cpu_platform"] = s.MinCpuPlatform
	}
	if s.ImageUri != "" {
		m["image_uri"] = s.ImageUri
	}
	if s.MinNumInstances != 0 {
		m["min_num_instances"] = s.MinNumInstances
	}
	return m
}

// ParseGcpDataprocCluster validates and normalizes a GcpDataprocCluster cloud_object.
func ParseGcpDataprocCluster(cloudObject map[string]any) (*structpb.Struct, error) {
	if err := parse.ValidateHeader(cloudObject, "gcp.openmcf.org/v1", "GcpDataprocCluster"); err != nil {
		return nil, err
	}

	specMap, err := parse.ExtractSpecMap(cloudObject)
	if err != nil {
		return nil, err
	}

	specBytes, err := json.Marshal(specMap)
	if err != nil {
		return nil, fmt.Errorf("marshal spec: %w", err)
	}

	var spec GcpDataprocClusterSpecInput
	if err := json.Unmarshal(specBytes, &spec); err != nil {
		return nil, fmt.Errorf("invalid spec: %w", err)
	}

	if err := spec.validate(); err != nil {
		return nil, err
	}

	spec.applyDefaults()

	return parse.RebuildCloudObject(cloudObject, spec.toMap())
}
