// Code generated by schema2go. DO NOT EDIT.
// Generated: 2026-02-27T22:24:53+05:30

package gcp

import (
	"encoding/json"
	"fmt"

	"github.com/plantonhq/mcp-server-planton/internal/parse"
	"google.golang.org/protobuf/types/known/structpb"
)

var (
	_ = json.Marshal
	_ = fmt.Errorf
	_ = parse.ValidateHeader
	_ = (*structpb.Struct)(nil)
)

// gcp-pubsub-topic
type GcpPubSubTopicSpecInput struct {
	// GCP project where the topic will be created.
	ProjectId string `json:"project_id" jsonschema:"required,GCP project where the topic will be created."`
	// Name of the Pub/Sub topic.
	//  Must be 3-255 characters, start with a letter, and contain only letters,
	//  numbers, hyphens, underscores, periods, tildes, plus signs, and percent signs.
	//  Immutable after creation.
	TopicName string `json:"topic_name" jsonschema:"required,Name of the Pub/Sub topic. Must be 3-255 characters; start with a letter; and contain only letters; numbers; hyphens; underscores; periods; tildes; plus signs; and percent signs. Immutable after creat..."`
	// Cloud KMS key for encrypting messages at rest (CMEK).
	//  Format: projects/{project}/locations/{location}/keyRings/{keyRing}/cryptoKeys/{key}
	//  The Pub/Sub service account must have roles/cloudkms.cryptoKeyEncrypterDecrypter
	//  on this key. If not set, messages are encrypted with Google-managed keys.
	KmsKeyName string `json:"kms_key_name,omitempty" jsonschema:"Cloud KMS key for encrypting messages at rest (CMEK). Format: projects/{project}/locations/{location}/keyRings/{keyRing}/cryptoKeys/{key} The Pub/Sub service account must have roles/cloudkms.cryptoKey..."`
	// Minimum duration to retain a message after it is published to the topic.
	//  When set, messages published in the last message_retention_duration are always
	//  available to subscribers, and any attached subscription can seek to a timestamp
	//  within the retention window.
	//  Format: duration string (e.g., "604...
	MessageRetentionDuration string `json:"message_retention_duration,omitempty" jsonschema:"Minimum duration to retain a message after it is published to the topic. When set; messages published in the last message_retention_duration are always available to subscribers; and any attached subsc..."`
	// Policy constraining the set of GCP regions where messages may be stored.
	//  When not set, no regional constraints are applied.
	MessageStoragePolicy *GcpPubSubTopicMessageStoragePolicyInput `json:"message_storage_policy,omitempty" jsonschema:"Policy constraining the set of GCP regions where messages may be stored. When not set; no regional constraints are applied."`
	// Schema validation settings for messages published to the topic.
	//  When set, all published messages are validated against the specified schema.
	SchemaSettings *GcpPubSubTopicSchemaSettingsInput `json:"schema_settings,omitempty" jsonschema:"Schema validation settings for messages published to the topic. When set; all published messages are validated against the specified schema."`
	// Settings for ingesting data from external sources into this topic.
	//  Supports AWS Kinesis, AWS MSK, Azure Event Hubs, Cloud Storage, and
	//  Confluent Cloud. Typically one data source is configured per topic.
	IngestionDataSourceSettings *GcpPubSubTopicIngestionDataSourceSettingsInput `json:"ingestion_data_source_settings,omitempty" jsonschema:"Settings for ingesting data from external sources into this topic. Supports AWS Kinesis; AWS MSK; Azure Event Hubs; Cloud Storage; and Confluent Cloud. Typically one data source is configured per topi..."`
}

func (s *GcpPubSubTopicSpecInput) validate() error {
	if s.ProjectId == "" {
		return fmt.Errorf("project_id is required")
	}
	if s.TopicName == "" {
		return fmt.Errorf("topic_name is required")
	}
	if s.MessageStoragePolicy != nil {
		if err := s.MessageStoragePolicy.validate(); err != nil {
			return fmt.Errorf("message_storage_policy: %w", err)
		}
	}
	if s.SchemaSettings != nil {
		if err := s.SchemaSettings.validate(); err != nil {
			return fmt.Errorf("schema_settings: %w", err)
		}
	}
	if s.IngestionDataSourceSettings != nil {
		if err := s.IngestionDataSourceSettings.validate(); err != nil {
			return fmt.Errorf("ingestion_data_source_settings: %w", err)
		}
	}
	return nil
}

func (s *GcpPubSubTopicSpecInput) applyDefaults() {
	if s.MessageStoragePolicy != nil {
		s.MessageStoragePolicy.applyDefaults()
	}
	if s.SchemaSettings != nil {
		s.SchemaSettings.applyDefaults()
	}
	if s.IngestionDataSourceSettings != nil {
		s.IngestionDataSourceSettings.applyDefaults()
	}
}

func (s *GcpPubSubTopicSpecInput) toMap() map[string]any {
	m := make(map[string]any)
	m["project_id"] = s.ProjectId
	m["topic_name"] = s.TopicName
	if s.KmsKeyName != "" {
		m["kms_key_name"] = s.KmsKeyName
	}
	if s.MessageRetentionDuration != "" {
		m["message_retention_duration"] = s.MessageRetentionDuration
	}
	if s.MessageStoragePolicy != nil {
		m["message_storage_policy"] = s.MessageStoragePolicy.toMap()
	}
	if s.SchemaSettings != nil {
		m["schema_settings"] = s.SchemaSettings.toMap()
	}
	if s.IngestionDataSourceSettings != nil {
		m["ingestion_data_source_settings"] = s.IngestionDataSourceSettings.toMap()
	}
	return m
}

// GcpPubSubTopicIngestionAwsKinesis defines settings for ingesting data from
//
//	Amazon Kinesis Data Streams into this Pub/Sub topic.
type GcpPubSubTopicIngestionAwsKinesisInput struct {
	// The ARN of the Kinesis data stream to ingest from.
	StreamArn string `json:"stream_arn" jsonschema:"required,The ARN of the Kinesis data stream to ingest from."`
	// The ARN of the Kinesis consumer to use for Enhanced Fan-Out delivery.
	ConsumerArn string `json:"consumer_arn" jsonschema:"required,The ARN of the Kinesis consumer to use for Enhanced Fan-Out delivery."`
	// The ARN of the AWS IAM role used for cross-account access to the Kinesis stream.
	AwsRoleArn string `json:"aws_role_arn" jsonschema:"required,The ARN of the AWS IAM role used for cross-account access to the Kinesis stream."`
	// The GCP service account used for Federated Identity authentication with AWS.
	GcpServiceAccount string `json:"gcp_service_account" jsonschema:"required,The GCP service account used for Federated Identity authentication with AWS."`
}

func (s *GcpPubSubTopicIngestionAwsKinesisInput) validate() error {
	if s.StreamArn == "" {
		return fmt.Errorf("stream_arn is required")
	}
	if s.ConsumerArn == "" {
		return fmt.Errorf("consumer_arn is required")
	}
	if s.AwsRoleArn == "" {
		return fmt.Errorf("aws_role_arn is required")
	}
	if s.GcpServiceAccount == "" {
		return fmt.Errorf("gcp_service_account is required")
	}
	return nil
}

func (s *GcpPubSubTopicIngestionAwsKinesisInput) applyDefaults() {
}

func (s *GcpPubSubTopicIngestionAwsKinesisInput) toMap() map[string]any {
	m := make(map[string]any)
	m["stream_arn"] = s.StreamArn
	m["consumer_arn"] = s.ConsumerArn
	m["aws_role_arn"] = s.AwsRoleArn
	m["gcp_service_account"] = s.GcpServiceAccount
	return m
}

// GcpPubSubTopicIngestionAwsMsk defines settings for ingesting data from
//
//	Amazon Managed Streaming for Apache Kafka (MSK) into this Pub/Sub topic.
type GcpPubSubTopicIngestionAwsMskInput struct {
	// The ARN of the MSK cluster to ingest from.
	ClusterArn string `json:"cluster_arn" jsonschema:"required,The ARN of the MSK cluster to ingest from."`
	// The name of the Kafka topic in MSK to ingest from.
	Topic string `json:"topic" jsonschema:"required,The name of the Kafka topic in MSK to ingest from."`
	// The ARN of the AWS IAM role used for cross-account access to the MSK cluster.
	AwsRoleArn string `json:"aws_role_arn" jsonschema:"required,The ARN of the AWS IAM role used for cross-account access to the MSK cluster."`
	// The GCP service account used for Federated Identity authentication with AWS.
	GcpServiceAccount string `json:"gcp_service_account" jsonschema:"required,The GCP service account used for Federated Identity authentication with AWS."`
}

func (s *GcpPubSubTopicIngestionAwsMskInput) validate() error {
	if s.ClusterArn == "" {
		return fmt.Errorf("cluster_arn is required")
	}
	if s.Topic == "" {
		return fmt.Errorf("topic is required")
	}
	if s.AwsRoleArn == "" {
		return fmt.Errorf("aws_role_arn is required")
	}
	if s.GcpServiceAccount == "" {
		return fmt.Errorf("gcp_service_account is required")
	}
	return nil
}

func (s *GcpPubSubTopicIngestionAwsMskInput) applyDefaults() {
}

func (s *GcpPubSubTopicIngestionAwsMskInput) toMap() map[string]any {
	m := make(map[string]any)
	m["cluster_arn"] = s.ClusterArn
	m["topic"] = s.Topic
	m["aws_role_arn"] = s.AwsRoleArn
	m["gcp_service_account"] = s.GcpServiceAccount
	return m
}

// GcpPubSubTopicIngestionAzureEventHubs defines settings for ingesting data from
//
//	Azure Event Hubs into this Pub/Sub topic.
type GcpPubSubTopicIngestionAzureEventHubsInput struct {
	// The Azure resource group containing the Event Hubs namespace.
	ResourceGroup string `json:"resource_group,omitempty" jsonschema:"The Azure resource group containing the Event Hubs namespace."`
	// The Azure Event Hubs namespace.
	Namespace string `json:"namespace,omitempty" jsonschema:"The Azure Event Hubs namespace."`
	// The name of the Event Hub to ingest from.
	EventHub string `json:"event_hub,omitempty" jsonschema:"The name of the Event Hub to ingest from."`
	// The Azure AD application client ID for authentication.
	ClientId string `json:"client_id,omitempty" jsonschema:"The Azure AD application client ID for authentication."`
	// The Azure AD tenant ID.
	TenantId string `json:"tenant_id,omitempty" jsonschema:"The Azure AD tenant ID."`
	// The Azure subscription ID.
	SubscriptionId string `json:"subscription_id,omitempty" jsonschema:"The Azure subscription ID."`
	// The GCP service account used for Federated Identity authentication with Azure.
	GcpServiceAccount string `json:"gcp_service_account,omitempty" jsonschema:"The GCP service account used for Federated Identity authentication with Azure."`
}

func (s *GcpPubSubTopicIngestionAzureEventHubsInput) validate() error {
	return nil
}

func (s *GcpPubSubTopicIngestionAzureEventHubsInput) applyDefaults() {
}

func (s *GcpPubSubTopicIngestionAzureEventHubsInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.ResourceGroup != "" {
		m["resource_group"] = s.ResourceGroup
	}
	if s.Namespace != "" {
		m["namespace"] = s.Namespace
	}
	if s.EventHub != "" {
		m["event_hub"] = s.EventHub
	}
	if s.ClientId != "" {
		m["client_id"] = s.ClientId
	}
	if s.TenantId != "" {
		m["tenant_id"] = s.TenantId
	}
	if s.SubscriptionId != "" {
		m["subscription_id"] = s.SubscriptionId
	}
	if s.GcpServiceAccount != "" {
		m["gcp_service_account"] = s.GcpServiceAccount
	}
	return m
}

// GcpPubSubTopicIngestionCloudStorage defines settings for ingesting data from
//
//	a Google Cloud Storage bucket into this Pub/Sub topic.
type GcpPubSubTopicIngestionCloudStorageInput struct {
	// The name of the Cloud Storage bucket to ingest from (without "gs://" prefix).
	//  See: https://cloud.google.com/storage/docs/buckets#naming
	Bucket string `json:"bucket" jsonschema:"required,The name of the Cloud Storage bucket to ingest from (without 'gs://' prefix). See: https://cloud.google.com/storage/docs/buckets#naming"`
	// Glob pattern used to match objects that will be ingested.
	//  If unset, all objects in the bucket will be ingested.
	MatchGlob string `json:"match_glob,omitempty" jsonschema:"Glob pattern used to match objects that will be ingested. If unset; all objects in the bucket will be ingested."`
	// Only ingest objects with a creation timestamp equal to or later than this value.
	//  Format: RFC 3339 (e.g., "2024-01-01T00:00:00Z").
	//  If unset, all objects are eligible for ingestion regardless of creation time.
	MinimumObjectCreateTime string `json:"minimum_object_create_time,omitempty" jsonschema:"Only ingest objects with a creation timestamp equal to or later than this value. Format: RFC 3339 (e.g.; '2024-01-01T00:00:00Z'). If unset; all objects are eligible for ingestion regardless of creatio..."`
	// Read Cloud Storage data in Avro binary format. The bytes of each object
	//  are set to the data field of a Pub/Sub message.
	//  Set this field (as an empty message) to select Avro format.
	AvroFormat *GcpPubSubTopicIngestionCloudStorageAvroFormatInput `json:"avro_format,omitempty" jsonschema:"Read Cloud Storage data in Avro binary format. The bytes of each object are set to the data field of a Pub/Sub message. Set this field (as an empty message) to select Avro format."`
	// Read Cloud Storage data written via Cloud Storage subscriptions.
	//  Restores the data and attributes of the originally exported Pub/Sub messages.
	//  Set this field (as an empty message) to select Pub/Sub Avro format.
	PubsubAvroFormat *GcpPubSubTopicIngestionCloudStoragePubsubAvroFormatInput `json:"pubsub_avro_format,omitempty" jsonschema:"Read Cloud Storage data written via Cloud Storage subscriptions. Restores the data and attributes of the originally exported Pub/Sub messages. Set this field (as an empty message) to select Pub/Sub Av..."`
	// Read Cloud Storage data in text format. Each line of text (as defined by
	//  the delimiter) becomes the data field of a Pub/Sub message.
	TextFormat *GcpPubSubTopicIngestionCloudStorageTextFormatInput `json:"text_format,omitempty" jsonschema:"Read Cloud Storage data in text format. Each line of text (as defined by the delimiter) becomes the data field of a Pub/Sub message."`
}

func (s *GcpPubSubTopicIngestionCloudStorageInput) validate() error {
	if s.Bucket == "" {
		return fmt.Errorf("bucket is required")
	}
	if s.AvroFormat != nil {
		if err := s.AvroFormat.validate(); err != nil {
			return fmt.Errorf("avro_format: %w", err)
		}
	}
	if s.PubsubAvroFormat != nil {
		if err := s.PubsubAvroFormat.validate(); err != nil {
			return fmt.Errorf("pubsub_avro_format: %w", err)
		}
	}
	if s.TextFormat != nil {
		if err := s.TextFormat.validate(); err != nil {
			return fmt.Errorf("text_format: %w", err)
		}
	}
	return nil
}

func (s *GcpPubSubTopicIngestionCloudStorageInput) applyDefaults() {
	if s.AvroFormat != nil {
		s.AvroFormat.applyDefaults()
	}
	if s.PubsubAvroFormat != nil {
		s.PubsubAvroFormat.applyDefaults()
	}
	if s.TextFormat != nil {
		s.TextFormat.applyDefaults()
	}
}

func (s *GcpPubSubTopicIngestionCloudStorageInput) toMap() map[string]any {
	m := make(map[string]any)
	m["bucket"] = s.Bucket
	if s.MatchGlob != "" {
		m["match_glob"] = s.MatchGlob
	}
	if s.MinimumObjectCreateTime != "" {
		m["minimum_object_create_time"] = s.MinimumObjectCreateTime
	}
	if s.AvroFormat != nil {
		m["avro_format"] = s.AvroFormat.toMap()
	}
	if s.PubsubAvroFormat != nil {
		m["pubsub_avro_format"] = s.PubsubAvroFormat.toMap()
	}
	if s.TextFormat != nil {
		m["text_format"] = s.TextFormat.toMap()
	}
	return m
}

// GcpPubSubTopicIngestionCloudStorageAvroFormat selects Avro binary format
//
//	for Cloud Storage ingestion. The bytes of each object are set to the
//	data field of a Pub/Sub message. No additional configuration is needed.
type GcpPubSubTopicIngestionCloudStorageAvroFormatInput struct {
}

func (s *GcpPubSubTopicIngestionCloudStorageAvroFormatInput) validate() error {
	return nil
}

func (s *GcpPubSubTopicIngestionCloudStorageAvroFormatInput) applyDefaults() {
}

func (s *GcpPubSubTopicIngestionCloudStorageAvroFormatInput) toMap() map[string]any {
	m := make(map[string]any)
	return m
}

// GcpPubSubTopicIngestionCloudStoragePubsubAvroFormat selects the Pub/Sub Avro
//
//	format for Cloud Storage ingestion. This format restores the data and attributes
//	fields of originally exported Pub/Sub messages.
type GcpPubSubTopicIngestionCloudStoragePubsubAvroFormatInput struct {
}

func (s *GcpPubSubTopicIngestionCloudStoragePubsubAvroFormatInput) validate() error {
	return nil
}

func (s *GcpPubSubTopicIngestionCloudStoragePubsubAvroFormatInput) applyDefaults() {
}

func (s *GcpPubSubTopicIngestionCloudStoragePubsubAvroFormatInput) toMap() map[string]any {
	m := make(map[string]any)
	return m
}

// GcpPubSubTopicIngestionCloudStorageTextFormat defines the text format
//
//	configuration for Cloud Storage ingestion. Each line of text (delimited
//	by the specified delimiter) becomes the data field of a Pub/Sub message.
type GcpPubSubTopicIngestionCloudStorageTextFormatInput struct {
	// The line delimiter. Defaults to newline ("\n") when not set.
	Delimiter string `json:"delimiter,omitempty" jsonschema:"The line delimiter. Defaults to newline ('\\n') when not set."`
}

func (s *GcpPubSubTopicIngestionCloudStorageTextFormatInput) validate() error {
	return nil
}

func (s *GcpPubSubTopicIngestionCloudStorageTextFormatInput) applyDefaults() {
}

func (s *GcpPubSubTopicIngestionCloudStorageTextFormatInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.Delimiter != "" {
		m["delimiter"] = s.Delimiter
	}
	return m
}

// GcpPubSubTopicIngestionConfluentCloud defines settings for ingesting data from
//
//	Confluent Cloud into this Pub/Sub topic.
type GcpPubSubTopicIngestionConfluentCloudInput struct {
	// The Confluent Cloud bootstrap server address. Format: host:port.
	BootstrapServer string `json:"bootstrap_server" jsonschema:"required,The Confluent Cloud bootstrap server address. Format: host:port."`
	// The name of the Confluent Cloud topic to ingest from.
	Topic string `json:"topic" jsonschema:"required,The name of the Confluent Cloud topic to ingest from."`
	// The Workload Identity Pool ID used for Federated Identity authentication
	//  with Confluent Cloud.
	IdentityPoolId string `json:"identity_pool_id" jsonschema:"required,The Workload Identity Pool ID used for Federated Identity authentication with Confluent Cloud."`
	// The GCP service account used for Federated Identity authentication
	//  with Confluent Cloud.
	GcpServiceAccount string `json:"gcp_service_account" jsonschema:"required,The GCP service account used for Federated Identity authentication with Confluent Cloud."`
	// The Confluent Cloud cluster ID. Optional.
	ClusterId string `json:"cluster_id,omitempty" jsonschema:"The Confluent Cloud cluster ID. Optional."`
}

func (s *GcpPubSubTopicIngestionConfluentCloudInput) validate() error {
	if s.BootstrapServer == "" {
		return fmt.Errorf("bootstrap_server is required")
	}
	if s.Topic == "" {
		return fmt.Errorf("topic is required")
	}
	if s.IdentityPoolId == "" {
		return fmt.Errorf("identity_pool_id is required")
	}
	if s.GcpServiceAccount == "" {
		return fmt.Errorf("gcp_service_account is required")
	}
	return nil
}

func (s *GcpPubSubTopicIngestionConfluentCloudInput) applyDefaults() {
}

func (s *GcpPubSubTopicIngestionConfluentCloudInput) toMap() map[string]any {
	m := make(map[string]any)
	m["bootstrap_server"] = s.BootstrapServer
	m["topic"] = s.Topic
	m["identity_pool_id"] = s.IdentityPoolId
	m["gcp_service_account"] = s.GcpServiceAccount
	if s.ClusterId != "" {
		m["cluster_id"] = s.ClusterId
	}
	return m
}

// GcpPubSubTopicIngestionDataSourceSettings configures data ingestion from
//
//	external sources into this Pub/Sub topic. Typically one data source is
//	configured per topic.
//
//	Supported sources:
//	  - AWS Kinesis Data Streams
//	  - AWS Managed Streaming for Apache Kafka (MSK)
//	  - Azure Event Hubs
//	  - Google Cloud Storage
//	  - Confluent Cloud
type GcpPubSubTopicIngestionDataSourceSettingsInput struct {
	// Ingest from Amazon Kinesis Data Streams.
	AwsKinesis *GcpPubSubTopicIngestionAwsKinesisInput `json:"aws_kinesis,omitempty" jsonschema:"Ingest from Amazon Kinesis Data Streams."`
	// Ingest from Amazon Managed Streaming for Apache Kafka (MSK).
	AwsMsk *GcpPubSubTopicIngestionAwsMskInput `json:"aws_msk,omitempty" jsonschema:"Ingest from Amazon Managed Streaming for Apache Kafka (MSK)."`
	// Ingest from Azure Event Hubs.
	AzureEventHubs *GcpPubSubTopicIngestionAzureEventHubsInput `json:"azure_event_hubs,omitempty" jsonschema:"Ingest from Azure Event Hubs."`
	// Ingest from Google Cloud Storage.
	CloudStorage *GcpPubSubTopicIngestionCloudStorageInput `json:"cloud_storage,omitempty" jsonschema:"Ingest from Google Cloud Storage."`
	// Ingest from Confluent Cloud.
	ConfluentCloud *GcpPubSubTopicIngestionConfluentCloudInput `json:"confluent_cloud,omitempty" jsonschema:"Ingest from Confluent Cloud."`
	// Platform logging settings for the ingestion pipeline.
	PlatformLogsSettings *GcpPubSubTopicIngestionPlatformLogsSettingsInput `json:"platform_logs_settings,omitempty" jsonschema:"Platform logging settings for the ingestion pipeline."`
}

func (s *GcpPubSubTopicIngestionDataSourceSettingsInput) validate() error {
	if s.AwsKinesis != nil {
		if err := s.AwsKinesis.validate(); err != nil {
			return fmt.Errorf("aws_kinesis: %w", err)
		}
	}
	if s.AwsMsk != nil {
		if err := s.AwsMsk.validate(); err != nil {
			return fmt.Errorf("aws_msk: %w", err)
		}
	}
	if s.AzureEventHubs != nil {
		if err := s.AzureEventHubs.validate(); err != nil {
			return fmt.Errorf("azure_event_hubs: %w", err)
		}
	}
	if s.CloudStorage != nil {
		if err := s.CloudStorage.validate(); err != nil {
			return fmt.Errorf("cloud_storage: %w", err)
		}
	}
	if s.ConfluentCloud != nil {
		if err := s.ConfluentCloud.validate(); err != nil {
			return fmt.Errorf("confluent_cloud: %w", err)
		}
	}
	if s.PlatformLogsSettings != nil {
		if err := s.PlatformLogsSettings.validate(); err != nil {
			return fmt.Errorf("platform_logs_settings: %w", err)
		}
	}
	return nil
}

func (s *GcpPubSubTopicIngestionDataSourceSettingsInput) applyDefaults() {
	if s.AwsKinesis != nil {
		s.AwsKinesis.applyDefaults()
	}
	if s.AwsMsk != nil {
		s.AwsMsk.applyDefaults()
	}
	if s.AzureEventHubs != nil {
		s.AzureEventHubs.applyDefaults()
	}
	if s.CloudStorage != nil {
		s.CloudStorage.applyDefaults()
	}
	if s.ConfluentCloud != nil {
		s.ConfluentCloud.applyDefaults()
	}
	if s.PlatformLogsSettings != nil {
		s.PlatformLogsSettings.applyDefaults()
	}
}

func (s *GcpPubSubTopicIngestionDataSourceSettingsInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.AwsKinesis != nil {
		m["aws_kinesis"] = s.AwsKinesis.toMap()
	}
	if s.AwsMsk != nil {
		m["aws_msk"] = s.AwsMsk.toMap()
	}
	if s.AzureEventHubs != nil {
		m["azure_event_hubs"] = s.AzureEventHubs.toMap()
	}
	if s.CloudStorage != nil {
		m["cloud_storage"] = s.CloudStorage.toMap()
	}
	if s.ConfluentCloud != nil {
		m["confluent_cloud"] = s.ConfluentCloud.toMap()
	}
	if s.PlatformLogsSettings != nil {
		m["platform_logs_settings"] = s.PlatformLogsSettings.toMap()
	}
	return m
}

// GcpPubSubTopicIngestionPlatformLogsSettings configures platform logging for
//
//	the ingestion pipeline. Controls the minimum severity level of logs generated
//	during data ingestion.
type GcpPubSubTopicIngestionPlatformLogsSettingsInput struct {
	// The minimum severity level of platform logs that will be written.
	//  Valid values: "DISABLED", "DEBUG", "INFO", "WARNING", "ERROR".
	//  If unset, no platform logs will be generated.
	Severity string `json:"severity,omitempty" jsonschema:"The minimum severity level of platform logs that will be written. Valid values: 'DISABLED'; 'DEBUG'; 'INFO'; 'WARNING'; 'ERROR'. If unset; no platform logs will be generated."`
}

func (s *GcpPubSubTopicIngestionPlatformLogsSettingsInput) validate() error {
	return nil
}

func (s *GcpPubSubTopicIngestionPlatformLogsSettingsInput) applyDefaults() {
}

func (s *GcpPubSubTopicIngestionPlatformLogsSettingsInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.Severity != "" {
		m["severity"] = s.Severity
	}
	return m
}

// GcpPubSubTopicMessageStoragePolicy constrains the set of Google Cloud Platform
//
//	regions where messages published to the topic may be persisted in storage.
type GcpPubSubTopicMessageStoragePolicyInput struct {
	// A list of GCP region IDs where messages may be persisted in storage.
	//  Messages published by publishers running in non-allowed regions will be
	//  routed for storage in one of the allowed regions.
	//  Must contain at least one region when the policy is specified.
	AllowedPersistenceRegions []string `json:"allowed_persistence_regions,omitempty" jsonschema:"A list of GCP region IDs where messages may be persisted in storage. Messages published by publishers running in non-allowed regions will be routed for storage in one of the allowed regions. Must cont..."`
	// When true, allowed_persistence_regions is also used to enforce in-transit
	//  guarantees for messages. Pub/Sub will fail publish operations on this topic
	//  and subscribe operations on any subscription attached to this topic in any
	//  region not listed in allowed_persistence_regions.
	EnforceInTransit bool `json:"enforce_in_transit,omitempty" jsonschema:"When true; allowed_persistence_regions is also used to enforce in-transit guarantees for messages. Pub/Sub will fail publish operations on this topic and subscribe operations on any subscription attac..."`
}

func (s *GcpPubSubTopicMessageStoragePolicyInput) validate() error {
	if len(s.AllowedPersistenceRegions) < 1 {
		return fmt.Errorf("allowed_persistence_regions requires at least 1 items, got %d", len(s.AllowedPersistenceRegions))
	}
	return nil
}

func (s *GcpPubSubTopicMessageStoragePolicyInput) applyDefaults() {
}

func (s *GcpPubSubTopicMessageStoragePolicyInput) toMap() map[string]any {
	m := make(map[string]any)
	if len(s.AllowedPersistenceRegions) > 0 {
		m["allowed_persistence_regions"] = s.AllowedPersistenceRegions
	}
	if s.EnforceInTransit {
		m["enforce_in_transit"] = s.EnforceInTransit
	}
	return m
}

// GcpPubSubTopicSchemaSettings defines schema validation settings for messages
//
//	published to the topic.
type GcpPubSubTopicSchemaSettingsInput struct {
	// The fully qualified name of the Pub/Sub schema that messages must conform to.
	//  Format: projects/{project}/schemas/{schema}
	Schema string `json:"schema" jsonschema:"required,The fully qualified name of the Pub/Sub schema that messages must conform to. Format: projects/{project}/schemas/{schema}"`
	// The encoding of messages validated against the schema.
	//  Valid values: "JSON" or "BINARY".
	Encoding string `json:"encoding,omitempty" jsonschema:"The encoding of messages validated against the schema. Valid values: 'JSON' or 'BINARY'."`
}

func (s *GcpPubSubTopicSchemaSettingsInput) validate() error {
	if s.Schema == "" {
		return fmt.Errorf("schema is required")
	}
	return nil
}

func (s *GcpPubSubTopicSchemaSettingsInput) applyDefaults() {
}

func (s *GcpPubSubTopicSchemaSettingsInput) toMap() map[string]any {
	m := make(map[string]any)
	m["schema"] = s.Schema
	if s.Encoding != "" {
		m["encoding"] = s.Encoding
	}
	return m
}

// ParseGcpPubSubTopic validates and normalizes a GcpPubSubTopic cloud_object.
func ParseGcpPubSubTopic(cloudObject map[string]any) (*structpb.Struct, error) {
	if err := parse.ValidateHeader(cloudObject, "gcp.openmcf.org/v1", "GcpPubSubTopic"); err != nil {
		return nil, err
	}

	specMap, err := parse.ExtractSpecMap(cloudObject)
	if err != nil {
		return nil, err
	}

	specBytes, err := json.Marshal(specMap)
	if err != nil {
		return nil, fmt.Errorf("marshal spec: %w", err)
	}

	var spec GcpPubSubTopicSpecInput
	if err := json.Unmarshal(specBytes, &spec); err != nil {
		return nil, fmt.Errorf("invalid spec: %w", err)
	}

	if err := spec.validate(); err != nil {
		return nil, err
	}

	spec.applyDefaults()

	return parse.RebuildCloudObject(cloudObject, spec.toMap())
}
