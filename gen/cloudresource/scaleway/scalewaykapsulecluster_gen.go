// Code generated by schema2go. DO NOT EDIT.
// Generated: 2026-02-26T22:50:56+05:30

package scaleway

import (
	"encoding/json"
	"fmt"

	"github.com/plantoncloud/mcp-server-planton/internal/parse"
	"google.golang.org/protobuf/types/known/structpb"
)

var (
	_ = json.Marshal
	_ = fmt.Errorf
	_ = parse.ValidateHeader
	_ = (*structpb.Struct)(nil)
)

// scaleway-kapsule-cluster
type ScalewayKapsuleClusterSpecInput struct {
	// The Scaleway region where the cluster will be created.
	//  Examples: "fr-par", "nl-ams", "pl-waw"
	//
	//  The region determines which data centers are available for node pools.
	//  All node pools in this cluster must be in zones within this region.
	//
	//  This field is required and cannot be changed after creation.
	Region string `json:"region" jsonschema:"required,The Scaleway region where the cluster will be created. Examples: 'fr-par'; 'nl-ams'; 'pl-waw' The region determines which data centers are available for node pools. All node pools in this cluster must..."`
	// Kubernetes version for the cluster.
	//
	//  Can be a minor version (e.g., "1.32") or a full patch version (e.g.,
	//  "1.32.3"). When auto-upgrade is enabled, use a minor version so
	//  Scaleway can automatically apply patch upgrades.
	//
	//  Available versions depend on the region and can be checked via the
	//  Scaleway...
	KubernetesVersion string `json:"kubernetes_version" jsonschema:"required,Kubernetes version for the cluster. Can be a minor version (e.g.; '1.32') or a full patch version (e.g.; '1.32.3'). When auto-upgrade is enabled; use a minor version so Scaleway can automatically appl..."`
	// Container Network Interface (CNI) plugin for pod networking.
	//
	//  Options:
	//    - "cilium" (recommended) -- eBPF-based. High performance, advanced
	//      network policies, service mesh integration, Hubble observability.
	//    - "calico" -- Mature, widely adopted. Standard Kubernetes network
	//      policies. Goo...
	Cni string `json:"cni" jsonschema:"required,Container Network Interface (CNI) plugin for pod networking. Options: - 'cilium' (recommended) -- eBPF-based. High performance; advanced network policies; service mesh integration; Hubble observabilit..."`
	// The Private Network to attach the cluster to.
	//
	//  All Kapsule clusters require a Private Network. Nodes communicate
	//  with the control plane and with each other over this network.
	//
	//  Can be a literal Private Network UUID or a reference to a
	//  ScalewayPrivateNetwork resource's output.
	//
	//  In infra charts, t...
	PrivateNetworkId string `json:"private_network_id" jsonschema:"required,The Private Network to attach the cluster to. All Kapsule clusters require a Private Network. Nodes communicate with the control plane and with each other over this network. Can be a literal Private N..."`
	// Kapsule cluster type.
	//
	//  Options:
	//    - "kapsule" (default) -- Mutualized (shared) control plane. Suitable
	//      for most workloads. No additional cost for the control plane.
	//    - "kapsule-dedicated-4"  -- Dedicated control plane, 4 nodes max.
	//    - "kapsule-dedicated-8"  -- Dedicated control plane, 8 n...
	Type string `json:"type,omitempty" jsonschema:"Kapsule cluster type. Options: - 'kapsule' (default) -- Mutualized (shared) control plane. Suitable for most workloads. No additional cost for the control plane. - 'kapsule-dedicated-4' -- Dedicated c..."`
	// Human-readable description for the cluster.
	//
	//  Optional. Shown in the Scaleway console for identification.
	Description string `json:"description,omitempty" jsonschema:"Human-readable description for the cluster. Optional. Shown in the Scaleway console for identification."`
	// Whether to delete additional resources (LBs, volumes, routes) created
	//  by Kubernetes when the cluster is destroyed.
	//
	//  When true, Scaleway automatically cleans up load balancers (from
	//  Services of type LoadBalancer), persistent volumes (from PVCs), and
	//  other resources that Kubernetes provisioned dur...
	DeleteAdditionalResources bool `json:"delete_additional_resources,omitempty" jsonschema:"Whether to delete additional resources (LBs; volumes; routes) created by Kubernetes when the cluster is destroyed. When true; Scaleway automatically cleans up load balancers (from Services of type Loa..."`
	// Automatic patch version upgrade configuration.
	//
	//  When enabled, Scaleway automatically upgrades the cluster to the
	//  latest patch version within the current minor version during the
	//  specified maintenance window. For example, if the cluster is on
	//  1.32.1, it may be upgraded to 1.32.3 during the window...
	AutoUpgrade *ScalewayKapsuleAutoUpgradeInput `json:"auto_upgrade,omitempty" jsonschema:"Automatic patch version upgrade configuration. When enabled; Scaleway automatically upgrades the cluster to the latest patch version within the current minor version during the specified maintenance w..."`
	// Cluster-wide autoscaler configuration.
	//
	//  Controls HOW the Kubernetes cluster autoscaler behaves when scaling
	//  node pools. Autoscaling itself is toggled per-pool (on the default
	//  node pool below, or on separate ScalewayKapsulePool resources), but
	//  the behavior parameters (delays, thresholds, algorith...
	AutoscalerConfig *ScalewayKapsuleAutoscalerConfigInput `json:"autoscaler_config,omitempty" jsonschema:"Cluster-wide autoscaler configuration. Controls HOW the Kubernetes cluster autoscaler behaves when scaling node pools. Autoscaling itself is toggled per-pool (on the default node pool below; or on sep..."`
	// Kubernetes feature gates to enable on the cluster.
	//
	//  Feature gates are alpha/beta Kubernetes features that can be toggled.
	//  Example: ["GracefulNodeShutdown", "HPAContainerMetrics"]
	//
	//  Consult the Kubernetes documentation for available feature gates
	//  at the cluster's version. Enabling unstable feature...
	FeatureGates []string `json:"feature_gates,omitempty" jsonschema:"Kubernetes feature gates to enable on the cluster. Feature gates are alpha/beta Kubernetes features that can be toggled. Example: ['GracefulNodeShutdown'; 'HPAContainerMetrics'] Consult the Kubernetes..."`
	// Kubernetes admission plugins to enable on the cluster.
	//
	//  Admission plugins intercept API server requests after authentication
	//  and authorization but before persistence. Scaleway enables a standard
	//  set by default; this field adds additional plugins.
	//
	//  Example: ["AlwaysPullImages", "NodeRestriction"]...
	AdmissionPlugins []string `json:"admission_plugins,omitempty" jsonschema:"Kubernetes admission plugins to enable on the cluster. Admission plugins intercept API server requests after authentication and authorization but before persistence. Scaleway enables a standard set by..."`
	// Pod CIDR for the cluster's pod network.
	//
	//  The IP range allocated to pods. Each node gets a /24 subnet from
	//  this range. Must be large enough to accommodate all pods across
	//  all nodes.
	//
	//  Default: "100.64.0.0/15" (131,072 addresses). Only change this
	//  if you have specific IP planning requirements or c...
	PodCidr string `json:"pod_cidr,omitempty" jsonschema:"Pod CIDR for the cluster's pod network. The IP range allocated to pods. Each node gets a /24 subnet from this range. Must be large enough to accommodate all pods across all nodes. Default: '100.64.0.0..."`
	// Service CIDR for Kubernetes services.
	//
	//  The IP range allocated to ClusterIP services. Must not overlap
	//  with pod_cidr or the Private Network's subnet.
	//
	//  Default: "10.32.0.0/20" (4,096 addresses). Only change this if
	//  you have specific IP planning requirements or conflicts.
	//
	//  IMPORTANT: Cannot be cha...
	ServiceCidr string `json:"service_cidr,omitempty" jsonschema:"Service CIDR for Kubernetes services. The IP range allocated to ClusterIP services. Must not overlap with pod_cidr or the Private Network's subnet. Default: '10.32.0.0/20' (4;096 addresses). Only chan..."`
	// The default node pool configuration.
	//
	//  Every Kapsule cluster needs at least one node pool to run workloads.
	//  This embedded pool is created alongside the cluster, giving you a
	//  working cluster from a single resource.
	//
	//  For additional node pools with different instance types, labels, or
	//  taints, creat...
	DefaultNodePool *ScalewayKapsuleDefaultNodePoolInput `json:"default_node_pool" jsonschema:"required,The default node pool configuration. Every Kapsule cluster needs at least one node pool to run workloads. This embedded pool is created alongside the cluster; giving you a working cluster from a singl..."`
}

func (s *ScalewayKapsuleClusterSpecInput) validate() error {
	if s.Region == "" {
		return fmt.Errorf("region is required")
	}
	if s.KubernetesVersion == "" {
		return fmt.Errorf("kubernetes_version is required")
	}
	if s.Cni == "" {
		return fmt.Errorf("cni is required")
	}
	if s.PrivateNetworkId == "" {
		return fmt.Errorf("private_network_id is required")
	}
	if s.AutoUpgrade != nil {
		if err := s.AutoUpgrade.validate(); err != nil {
			return fmt.Errorf("auto_upgrade: %w", err)
		}
	}
	if s.AutoscalerConfig != nil {
		if err := s.AutoscalerConfig.validate(); err != nil {
			return fmt.Errorf("autoscaler_config: %w", err)
		}
	}
	if s.DefaultNodePool == nil {
		return fmt.Errorf("default_node_pool is required")
	}
	if s.DefaultNodePool != nil {
		if err := s.DefaultNodePool.validate(); err != nil {
			return fmt.Errorf("default_node_pool: %w", err)
		}
	}
	return nil
}

func (s *ScalewayKapsuleClusterSpecInput) applyDefaults() {
	if s.AutoUpgrade != nil {
		s.AutoUpgrade.applyDefaults()
	}
	if s.AutoscalerConfig != nil {
		s.AutoscalerConfig.applyDefaults()
	}
	if s.DefaultNodePool != nil {
		s.DefaultNodePool.applyDefaults()
	}
}

func (s *ScalewayKapsuleClusterSpecInput) toMap() map[string]any {
	m := make(map[string]any)
	m["region"] = s.Region
	m["kubernetes_version"] = s.KubernetesVersion
	m["cni"] = s.Cni
	m["private_network_id"] = s.PrivateNetworkId
	if s.Type != "" {
		m["type"] = s.Type
	}
	if s.Description != "" {
		m["description"] = s.Description
	}
	if s.DeleteAdditionalResources {
		m["delete_additional_resources"] = s.DeleteAdditionalResources
	}
	if s.AutoUpgrade != nil {
		m["auto_upgrade"] = s.AutoUpgrade.toMap()
	}
	if s.AutoscalerConfig != nil {
		m["autoscaler_config"] = s.AutoscalerConfig.toMap()
	}
	if len(s.FeatureGates) > 0 {
		m["feature_gates"] = s.FeatureGates
	}
	if len(s.AdmissionPlugins) > 0 {
		m["admission_plugins"] = s.AdmissionPlugins
	}
	if s.PodCidr != "" {
		m["pod_cidr"] = s.PodCidr
	}
	if s.ServiceCidr != "" {
		m["service_cidr"] = s.ServiceCidr
	}
	if s.DefaultNodePool != nil {
		m["default_node_pool"] = s.DefaultNodePool.toMap()
	}
	return m
}

// ScalewayKapsuleAutoUpgrade configures automatic Kubernetes patch
//
//	version upgrades for the cluster.
//
//	When enabled, Scaleway upgrades the cluster to the latest available
//	patch version during the specified maintenance window. For example,
//	a cluster on 1.32.1 may be upgraded to 1.32.3.
//
//	Auto-upgrade only applies to patch versions within the same minor
//	version. Minor version upgrades (e.g., 1.32 -> 1.33) must be
//	triggered manually.
type ScalewayKapsuleAutoUpgradeInput struct {
	// Whether auto-upgrade is enabled.
	Enable bool `json:"enable" jsonschema:"required,Whether auto-upgrade is enabled."`
	// UTC hour (0-23) when the maintenance window starts.
	//
	//  The upgrade process begins at this hour. Choose a low-traffic
	//  period for your workloads. Example: 2 (2:00 AM UTC).
	MaintenanceWindowStartHour int32 `json:"maintenance_window_start_hour" jsonschema:"required,UTC hour (0-23) when the maintenance window starts. The upgrade process begins at this hour. Choose a low-traffic period for your workloads. Example: 2 (2:00 AM UTC)."`
	// Day of the week for the maintenance window.
	//
	//  Options: "monday", "tuesday", "wednesday", "thursday", "friday",
	//  "saturday", "sunday", or "any" (Scaleway picks the best day).
	//
	//  Example: "sunday" for weekend maintenance.
	MaintenanceWindowDay string `json:"maintenance_window_day" jsonschema:"required,Day of the week for the maintenance window. Options: 'monday'; 'tuesday'; 'wednesday'; 'thursday'; 'friday'; 'saturday'; 'sunday'; or 'any' (Scaleway picks the best day). Example: 'sunday' for weekend..."`
}

func (s *ScalewayKapsuleAutoUpgradeInput) validate() error {
	if s.MaintenanceWindowDay == "" {
		return fmt.Errorf("maintenance_window_day is required")
	}
	return nil
}

func (s *ScalewayKapsuleAutoUpgradeInput) applyDefaults() {
}

func (s *ScalewayKapsuleAutoUpgradeInput) toMap() map[string]any {
	m := make(map[string]any)
	m["enable"] = s.Enable
	m["maintenance_window_start_hour"] = s.MaintenanceWindowStartHour
	m["maintenance_window_day"] = s.MaintenanceWindowDay
	return m
}

// ScalewayKapsuleAutoscalerConfig controls the behavior of the Kubernetes
//
//	cluster autoscaler at the cluster level.
//
//	These settings apply to ALL autoscaling-enabled pools in the cluster.
//	Individual pools toggle autoscaling on/off, but the behavior parameters
//	(how aggressively to scale, how long to wait, etc.) are configured here.
//
//	All fields are optional. Omitted fields use Scaleway's defaults.
type ScalewayKapsuleAutoscalerConfigInput struct {
	// Disable the scale-down behavior entirely.
	//
	//  When true, the autoscaler only scales UP (adds nodes) but never
	//  removes underutilized nodes. Useful during rollouts or when
	//  stability is more important than cost optimization.
	//
	//  Default: false.
	DisableScaleDown bool `json:"disable_scale_down,omitempty" jsonschema:"Disable the scale-down behavior entirely. When true; the autoscaler only scales UP (adds nodes) but never removes underutilized nodes. Useful during rollouts or when stability is more important than c..."`
	// How long to wait after a scale-up before considering scale-down.
	//
	//  Duration string (e.g., "10m", "15m"). Prevents thrashing by
	//  allowing newly added nodes time to receive workloads.
	//
	//  Default: "10m".
	ScaleDownDelayAfterAdd string `json:"scale_down_delay_after_add,omitempty" jsonschema:"How long to wait after a scale-up before considering scale-down. Duration string (e.g.; '10m'; '15m'). Prevents thrashing by allowing newly added nodes time to receive workloads. Default: '10m'."`
	// How long a node must be underutilized before it's eligible for
	//  scale-down.
	//
	//  Duration string (e.g., "10m", "20m"). Higher values tolerate
	//  temporary dips in utilization.
	//
	//  Default: "10m".
	ScaleDownUnneededTime string `json:"scale_down_unneeded_time,omitempty" jsonschema:"How long a node must be underutilized before it's eligible for scale-down. Duration string (e.g.; '10m'; '20m'). Higher values tolerate temporary dips in utilization. Default: '10m'."`
	// Resource estimation algorithm for scheduling decisions.
	//
	//  Options:
	//    - "binpacking" (default) -- Estimates how many nodes are needed
	//      by bin-packing pending pods. Most accurate for heterogeneous
	//      workloads.
	Estimator string `json:"estimator,omitempty" jsonschema:"Resource estimation algorithm for scheduling decisions. Options: - 'binpacking' (default) -- Estimates how many nodes are needed by bin-packing pending pods. Most accurate for heterogeneous workloads."`
	// Node group expansion strategy when multiple groups can accommodate
	//  pending pods.
	//
	//  Options:
	//    - "random" (default) -- Pick a random eligible group.
	//    - "most-pods"  -- Pick the group that schedules the most pods.
	//    - "least-waste" -- Pick the group with least resource waste.
	//    - "priority" -- P...
	Expander string `json:"expander,omitempty" jsonschema:"Node group expansion strategy when multiple groups can accommodate pending pods. Options: - 'random' (default) -- Pick a random eligible group. - 'most-pods' -- Pick the group that schedules the most ..."`
	// CPU/memory utilization threshold below which a node is considered
	//  underutilized and eligible for scale-down.
	//
	//  Range: 0.0 to 1.0. For example, 0.5 means a node using less than
	//  50% of its allocatable resources is a scale-down candidate.
	//
	//  Default: 0.5. Lower values are more aggressive (remove more ...
	ScaleDownUtilizationThreshold float64 `json:"scale_down_utilization_threshold,omitempty" jsonschema:"CPU/memory utilization threshold below which a node is considered underutilized and eligible for scale-down. Range: 0.0 to 1.0. For example; 0.5 means a node using less than 50% of its allocatable res..."`
	// Maximum time (in seconds) the autoscaler waits for pod termination
	//  during scale-down.
	//
	//  Pods with long graceful termination periods may need a higher value.
	//
	//  Default: 600 (10 minutes).
	MaxGracefulTerminationSec int32 `json:"max_graceful_termination_sec,omitempty" jsonschema:"Maximum time (in seconds) the autoscaler waits for pod termination during scale-down. Pods with long graceful termination periods may need a higher value. Default: 600 (10 minutes)."`
	// Whether to consider DaemonSet resource usage when calculating node
	//  utilization.
	//
	//  When false (default), DaemonSet resource requests are excluded from
	//  utilization calculations, making scale-down more conservative.
	//
	//  Default: false.
	IgnoreDaemonsetsUtilization bool `json:"ignore_daemonsets_utilization,omitempty" jsonschema:"Whether to consider DaemonSet resource usage when calculating node utilization. When false (default); DaemonSet resource requests are excluded from utilization calculations; making scale-down more con..."`
	// Whether to balance the number of nodes across similar node groups.
	//
	//  When true, the autoscaler tries to keep similarly-sized node groups
	//  at the same size. Useful for multi-AZ setups.
	//
	//  Default: false.
	BalanceSimilarNodeGroups bool `json:"balance_similar_node_groups,omitempty" jsonschema:"Whether to balance the number of nodes across similar node groups. When true; the autoscaler tries to keep similarly-sized node groups at the same size. Useful for multi-AZ setups. Default: false."`
	// Priority cutoff for expendable pods during scale-down.
	//
	//  Pods with priority below this value are considered expendable and
	//  won't prevent scale-down. Default: -10.
	ExpendablePodsPriorityCutoff int32 `json:"expendable_pods_priority_cutoff,omitempty" jsonschema:"Priority cutoff for expendable pods during scale-down. Pods with priority below this value are considered expendable and won't prevent scale-down. Default: -10."`
}

func (s *ScalewayKapsuleAutoscalerConfigInput) validate() error {
	return nil
}

func (s *ScalewayKapsuleAutoscalerConfigInput) applyDefaults() {
}

func (s *ScalewayKapsuleAutoscalerConfigInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.DisableScaleDown {
		m["disable_scale_down"] = s.DisableScaleDown
	}
	if s.ScaleDownDelayAfterAdd != "" {
		m["scale_down_delay_after_add"] = s.ScaleDownDelayAfterAdd
	}
	if s.ScaleDownUnneededTime != "" {
		m["scale_down_unneeded_time"] = s.ScaleDownUnneededTime
	}
	if s.Estimator != "" {
		m["estimator"] = s.Estimator
	}
	if s.Expander != "" {
		m["expander"] = s.Expander
	}
	if s.ScaleDownUtilizationThreshold != 0 {
		m["scale_down_utilization_threshold"] = s.ScaleDownUtilizationThreshold
	}
	if s.MaxGracefulTerminationSec != 0 {
		m["max_graceful_termination_sec"] = s.MaxGracefulTerminationSec
	}
	if s.IgnoreDaemonsetsUtilization {
		m["ignore_daemonsets_utilization"] = s.IgnoreDaemonsetsUtilization
	}
	if s.BalanceSimilarNodeGroups {
		m["balance_similar_node_groups"] = s.BalanceSimilarNodeGroups
	}
	if s.ExpendablePodsPriorityCutoff != 0 {
		m["expendable_pods_priority_cutoff"] = s.ExpendablePodsPriorityCutoff
	}
	return m
}

// ScalewayKapsuleDefaultNodePool defines the embedded default node pool
//
//	that ships with the cluster.
//
//	This pool is created as part of the cluster composite and provides
//	immediate compute capacity. Its lifecycle is tied to the cluster --
//	deleting the cluster deletes this pool and all its nodes.
//
//	For workload isolation (different instance types, GPU pools, spot
//	nodes), create additional `ScalewayKapsulePool` resources instead of
//	expanding this default pool.
type ScalewayKapsuleDefaultNodePoolInput struct {
	// Pool name. If omitted, defaults to "{cluster-name}-default".
	//
	//  Must be unique within the cluster. Use a descriptive name like
	//  "system", "default", or "general".
	//
	//  IMPORTANT: Cannot be changed after creation.
	Name string `json:"name,omitempty" jsonschema:"Pool name. If omitted; defaults to '{cluster-name}-default'. Must be unique within the cluster. Use a descriptive name like 'system'; 'default'; or 'general'. IMPORTANT: Cannot be changed after creati..."`
	// Instance type for worker nodes (required).
	//
	//  Determines CPU, RAM, and local storage for each node. Common types:
	//    - Development:  "DEV1-M" (3 vCPU, 4 GB RAM)
	//    - General:      "GP1-XS" (4 vCPU, 16 GB RAM), "GP1-S" (8 vCPU, 32 GB)
	//    - Production:   "PRO2-S" (2 vCPU, 8 GB), "PRO2-M" (4 vCPU, 16 GB...
	NodeType string `json:"node_type" jsonschema:"required,Instance type for worker nodes (required). Determines CPU; RAM; and local storage for each node. Common types: - Development: 'DEV1-M' (3 vCPU; 4 GB RAM) - General: 'GP1-XS' (4 vCPU; 16 GB RAM); 'GP1-..."`
	// Initial number of nodes in the pool (required).
	//
	//  When autoscaling is disabled, this is the fixed pool size. When
	//  autoscaling is enabled, this is the initial size -- the autoscaler
	//  will adjust between min_size and max_size based on workload demands.
	//
	//  Minimum: 1 (a pool must have at least one node...
	Size int32 `json:"size" jsonschema:"required,Initial number of nodes in the pool (required). When autoscaling is disabled; this is the fixed pool size. When autoscaling is enabled; this is the initial size -- the autoscaler will adjust between m..."`
	// Enable the cluster autoscaler for this pool.
	//
	//  When true, Kubernetes automatically adds or removes nodes based on
	//  pending pod resource requests. Requires min_size and max_size to
	//  be configured. The autoscaler's behavior (delays, thresholds) is
	//  controlled by the cluster-level `autoscaler_config`.
	// ...
	AutoScale bool `json:"auto_scale,omitempty" jsonschema:"Enable the cluster autoscaler for this pool. When true; Kubernetes automatically adds or removes nodes based on pending pod resource requests. Requires min_size and max_size to be configured. The auto..."`
	// Minimum number of nodes when autoscaling is enabled.
	//
	//  The autoscaler will not scale below this number, even if all nodes
	//  are underutilized. Set to at least 1 for availability.
	//
	//  Only meaningful when auto_scale is true.
	MinSize int32 `json:"min_size,omitempty" jsonschema:"Minimum number of nodes when autoscaling is enabled. The autoscaler will not scale below this number; even if all nodes are underutilized. Set to at least 1 for availability. Only meaningful when auto..."`
	// Maximum number of nodes when autoscaling is enabled.
	//
	//  The autoscaler will not scale above this number, even if pods are
	//  pending. Controls cost ceiling.
	//
	//  Only meaningful when auto_scale is true.
	MaxSize int32 `json:"max_size,omitempty" jsonschema:"Maximum number of nodes when autoscaling is enabled. The autoscaler will not scale above this number; even if pods are pending. Controls cost ceiling. Only meaningful when auto_scale is true."`
	// Enable autohealing for this pool.
	//
	//  When true, Scaleway automatically detects and replaces unhealthy
	//  nodes. A node is considered unhealthy if its kubelet stops
	//  reporting status for a configurable period.
	//
	//  Recommended for production clusters.
	Autohealing bool `json:"autohealing,omitempty" jsonschema:"Enable autohealing for this pool. When true; Scaleway automatically detects and replaces unhealthy nodes. A node is considered unhealthy if its kubelet stops reporting status for a configurable period..."`
	// Container runtime for pool nodes.
	//
	//  Options:
	//    - "containerd" (default, recommended) -- Industry-standard container
	//      runtime. Required for Kubernetes 1.24+.
	//
	//  IMPORTANT: Cannot be changed after creation.
	ContainerRuntime string `json:"container_runtime,omitempty" jsonschema:"Container runtime for pool nodes. Options: - 'containerd' (default; recommended) -- Industry-standard container runtime. Required for Kubernetes 1.24+. IMPORTANT: Cannot be changed after creation."`
	// Root volume type for pool nodes.
	//
	//  Controls the storage backing each node's root filesystem. Options
	//  depend on the instance type and availability zone.
	//
	//  IMPORTANT: Cannot be changed after creation.
	RootVolumeType string `json:"root_volume_type,omitempty" jsonschema:"Root volume type for pool nodes. Controls the storage backing each node's root filesystem. Options depend on the instance type and availability zone. IMPORTANT: Cannot be changed after creation."`
	// Root volume size in GB for pool nodes.
	//
	//  If omitted, uses the default size for the instance type. Increase
	//  for workloads that pull many large container images or need
	//  significant local ephemeral storage.
	//
	//  IMPORTANT: Cannot be changed after creation.
	RootVolumeSizeInGb int32 `json:"root_volume_size_in_gb,omitempty" jsonschema:"Root volume size in GB for pool nodes. If omitted; uses the default size for the instance type. Increase for workloads that pull many large container images or need significant local ephemeral storage..."`
	// Disable public IP addresses on pool nodes.
	//
	//  When true, nodes have only private IPs (from the cluster's Private
	//  Network). This is the recommended security posture for production:
	//  nodes are not reachable from the internet.
	//
	//  Requires a Public Gateway or NAT on the Private Network so nodes
	//  can reac...
	PublicIpDisabled bool `json:"public_ip_disabled,omitempty" jsonschema:"Disable public IP addresses on pool nodes. When true; nodes have only private IPs (from the cluster's Private Network). This is the recommended security posture for production: nodes are not reachable..."`
	// Node pool upgrade policy for rolling updates.
	//
	//  Controls how nodes are replaced during Kubernetes version upgrades
	//  or pool configuration changes.
	//
	//  Optional. If omitted, Scaleway uses defaults (max_surge=0,
	//  max_unavailable=1 -- one node at a time).
	UpgradePolicy *ScalewayKapsuleNodePoolUpgradePolicyInput `json:"upgrade_policy,omitempty" jsonschema:"Node pool upgrade policy for rolling updates. Controls how nodes are replaced during Kubernetes version upgrades or pool configuration changes. Optional. If omitted; Scaleway uses defaults (max_surge=..."`
}

func (s *ScalewayKapsuleDefaultNodePoolInput) validate() error {
	if s.NodeType == "" {
		return fmt.Errorf("node_type is required")
	}
	if s.UpgradePolicy != nil {
		if err := s.UpgradePolicy.validate(); err != nil {
			return fmt.Errorf("upgrade_policy: %w", err)
		}
	}
	return nil
}

func (s *ScalewayKapsuleDefaultNodePoolInput) applyDefaults() {
	if s.UpgradePolicy != nil {
		s.UpgradePolicy.applyDefaults()
	}
}

func (s *ScalewayKapsuleDefaultNodePoolInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.Name != "" {
		m["name"] = s.Name
	}
	m["node_type"] = s.NodeType
	m["size"] = s.Size
	if s.AutoScale {
		m["auto_scale"] = s.AutoScale
	}
	if s.MinSize != 0 {
		m["min_size"] = s.MinSize
	}
	if s.MaxSize != 0 {
		m["max_size"] = s.MaxSize
	}
	if s.Autohealing {
		m["autohealing"] = s.Autohealing
	}
	if s.ContainerRuntime != "" {
		m["container_runtime"] = s.ContainerRuntime
	}
	if s.RootVolumeType != "" {
		m["root_volume_type"] = s.RootVolumeType
	}
	if s.RootVolumeSizeInGb != 0 {
		m["root_volume_size_in_gb"] = s.RootVolumeSizeInGb
	}
	if s.PublicIpDisabled {
		m["public_ip_disabled"] = s.PublicIpDisabled
	}
	if s.UpgradePolicy != nil {
		m["upgrade_policy"] = s.UpgradePolicy.toMap()
	}
	return m
}

// ScalewayKapsuleNodePoolUpgradePolicy controls how nodes are replaced
//
//	during pool upgrades.
//
//	During a Kubernetes version upgrade or pool reconfiguration, nodes
//	are replaced one or more at a time according to this policy.
type ScalewayKapsuleNodePoolUpgradePolicyInput struct {
	// Maximum number of extra nodes created during an upgrade.
	//
	//  Surge nodes are temporary workers that accept workloads while
	//  existing nodes are drained and replaced. Higher values speed up
	//  upgrades but temporarily increase cost.
	//
	//  Default: 0 (no surge nodes).
	MaxSurge int32 `json:"max_surge,omitempty" jsonschema:"Maximum number of extra nodes created during an upgrade. Surge nodes are temporary workers that accept workloads while existing nodes are drained and replaced. Higher values speed up upgrades but temp..."`
	// Maximum number of nodes that can be unavailable simultaneously
	//  during an upgrade.
	//
	//  Controls the disruption budget. Setting this to 1 means nodes
	//  are replaced one at a time (safest but slowest).
	//
	//  Default: 1.
	MaxUnavailable int32 `json:"max_unavailable,omitempty" jsonschema:"Maximum number of nodes that can be unavailable simultaneously during an upgrade. Controls the disruption budget. Setting this to 1 means nodes are replaced one at a time (safest but slowest). Default..."`
}

func (s *ScalewayKapsuleNodePoolUpgradePolicyInput) validate() error {
	return nil
}

func (s *ScalewayKapsuleNodePoolUpgradePolicyInput) applyDefaults() {
}

func (s *ScalewayKapsuleNodePoolUpgradePolicyInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.MaxSurge != 0 {
		m["max_surge"] = s.MaxSurge
	}
	if s.MaxUnavailable != 0 {
		m["max_unavailable"] = s.MaxUnavailable
	}
	return m
}

// ParseScalewayKapsuleCluster validates and normalizes a ScalewayKapsuleCluster cloud_object.
func ParseScalewayKapsuleCluster(cloudObject map[string]any) (*structpb.Struct, error) {
	if err := parse.ValidateHeader(cloudObject, "scaleway.openmcf.org/v1", "ScalewayKapsuleCluster"); err != nil {
		return nil, err
	}

	specMap, err := parse.ExtractSpecMap(cloudObject)
	if err != nil {
		return nil, err
	}

	specBytes, err := json.Marshal(specMap)
	if err != nil {
		return nil, fmt.Errorf("marshal spec: %w", err)
	}

	var spec ScalewayKapsuleClusterSpecInput
	if err := json.Unmarshal(specBytes, &spec); err != nil {
		return nil, fmt.Errorf("invalid spec: %w", err)
	}

	if err := spec.validate(); err != nil {
		return nil, err
	}

	spec.applyDefaults()

	return parse.RebuildCloudObject(cloudObject, spec.toMap())
}
