// Code generated by schema2go. DO NOT EDIT.
// Generated: 2026-02-26T21:50:49+05:30

package scaleway

import (
	"encoding/json"
	"fmt"

	"github.com/plantoncloud/mcp-server-planton/gen/parse"
	"google.golang.org/protobuf/types/known/structpb"
)

var (
	_ = json.Marshal
	_ = fmt.Errorf
	_ = parse.ValidateHeader
	_ = (*structpb.Struct)(nil)
)

// scaleway-kapsule-pool
type ScalewayKapsulePoolSpecInput struct {
	// The Scaleway region where the pool will be created.
	//
	//  Must match the parent cluster's region. All nodes in this pool will
	//  be placed in zones within this region.
	//
	//  Examples: "fr-par", "nl-ams", "pl-waw"
	//
	//  IMPORTANT: Cannot be changed after creation.
	Region string `json:"region" jsonschema:"required,The Scaleway region where the pool will be created. Must match the parent cluster's region. All nodes in this pool will be placed in zones within this region. Examples: 'fr-par'; 'nl-ams'; 'pl-waw' IM..."`
	// Reference to the Kapsule cluster in which to create this node pool.
	//
	//  Can be a literal cluster ID or a reference to a ScalewayKapsuleCluster
	//  resource's output. In infra charts, this is wired via `valueFrom`.
	//
	//  IMPORTANT: Cannot be changed after creation.
	ClusterId string `json:"cluster_id" jsonschema:"required,Reference to the Kapsule cluster in which to create this node pool. Can be a literal cluster ID or a reference to a ScalewayKapsuleCluster resource's output. In infra charts; this is wired via 'valueF..."`
	// Instance type for worker nodes (required).
	//
	//  Determines CPU, RAM, and local storage for each node. Common types:
	//    - Development:  "DEV1-M" (3 vCPU, 4 GB RAM)
	//    - General:      "GP1-XS" (4 vCPU, 16 GB RAM), "GP1-S" (8 vCPU, 32 GB)
	//    - Production:   "PRO2-S" (2 vCPU, 8 GB), "PRO2-M" (4 vCPU, 16 GB...
	NodeType string `json:"node_type" jsonschema:"required,Instance type for worker nodes (required). Determines CPU; RAM; and local storage for each node. Common types: - Development: 'DEV1-M' (3 vCPU; 4 GB RAM) - General: 'GP1-XS' (4 vCPU; 16 GB RAM); 'GP1-..."`
	// Number of nodes in the pool (required).
	//
	//  When autoscaling is disabled, this is the fixed pool size. When
	//  autoscaling is enabled, this is the initial size -- the autoscaler
	//  will adjust between min_size and max_size based on workload demands.
	//
	//  Minimum: 1 (a pool must have at least one node).
	//
	//  Not...
	Size int32 `json:"size" jsonschema:"required,Number of nodes in the pool (required). When autoscaling is disabled; this is the fixed pool size. When autoscaling is enabled; this is the initial size -- the autoscaler will adjust between min_size ..."`
	// Enable the cluster autoscaler for this pool.
	//
	//  When true, Kubernetes automatically adds or removes nodes based on
	//  pending pod resource requests. Requires min_size and max_size to
	//  be configured. The autoscaler's behavior (delays, thresholds) is
	//  controlled by the cluster-level `autoscaler_config` o...
	AutoScale bool `json:"auto_scale,omitempty" jsonschema:"Enable the cluster autoscaler for this pool. When true; Kubernetes automatically adds or removes nodes based on pending pod resource requests. Requires min_size and max_size to be configured. The auto..."`
	// Minimum number of nodes when autoscaling is enabled.
	//
	//  The autoscaler will not scale below this number, even if all nodes
	//  are underutilized. Set to at least 1 for availability.
	//
	//  Only meaningful when auto_scale is true.
	MinSize int32 `json:"min_size,omitempty" jsonschema:"Minimum number of nodes when autoscaling is enabled. The autoscaler will not scale below this number; even if all nodes are underutilized. Set to at least 1 for availability. Only meaningful when auto..."`
	// Maximum number of nodes when autoscaling is enabled.
	//
	//  The autoscaler will not scale above this number, even if pods are
	//  pending. Controls cost ceiling.
	//
	//  Only meaningful when auto_scale is true.
	MaxSize int32 `json:"max_size,omitempty" jsonschema:"Maximum number of nodes when autoscaling is enabled. The autoscaler will not scale above this number; even if pods are pending. Controls cost ceiling. Only meaningful when auto_scale is true."`
	// Enable autohealing for this pool.
	//
	//  When true, Scaleway automatically detects and replaces unhealthy
	//  nodes. A node is considered unhealthy if its kubelet stops
	//  reporting status for a configurable period.
	//
	//  Recommended for production pools.
	Autohealing bool `json:"autohealing,omitempty" jsonschema:"Enable autohealing for this pool. When true; Scaleway automatically detects and replaces unhealthy nodes. A node is considered unhealthy if its kubelet stops reporting status for a configurable period..."`
	// Container runtime for pool nodes.
	//
	//  Options:
	//    - "containerd" (default, recommended) -- Industry-standard container
	//      runtime. Required for Kubernetes 1.24+.
	//
	//  IMPORTANT: Cannot be changed after creation.
	ContainerRuntime string `json:"container_runtime,omitempty" jsonschema:"Container runtime for pool nodes. Options: - 'containerd' (default; recommended) -- Industry-standard container runtime. Required for Kubernetes 1.24+. IMPORTANT: Cannot be changed after creation."`
	// Root volume type for pool nodes.
	//
	//  Controls the storage backing each node's root filesystem. Options
	//  depend on the instance type and availability zone.
	//
	//  IMPORTANT: Cannot be changed after creation.
	RootVolumeType string `json:"root_volume_type,omitempty" jsonschema:"Root volume type for pool nodes. Controls the storage backing each node's root filesystem. Options depend on the instance type and availability zone. IMPORTANT: Cannot be changed after creation."`
	// Root volume size in GB for pool nodes.
	//
	//  If omitted, uses the default size for the instance type. Increase
	//  for workloads that pull many large container images or need
	//  significant local ephemeral storage.
	//
	//  IMPORTANT: Cannot be changed after creation.
	RootVolumeSizeInGb int32 `json:"root_volume_size_in_gb,omitempty" jsonschema:"Root volume size in GB for pool nodes. If omitted; uses the default size for the instance type. Increase for workloads that pull many large container images or need significant local ephemeral storage..."`
	// Disable public IP addresses on pool nodes.
	//
	//  When true, nodes have only private IPs (from the cluster's Private
	//  Network). This is the recommended security posture for production:
	//  nodes are not reachable from the internet.
	//
	//  Requires a Public Gateway or NAT on the Private Network so nodes
	//  can reac...
	PublicIpDisabled bool `json:"public_ip_disabled,omitempty" jsonschema:"Disable public IP addresses on pool nodes. When true; nodes have only private IPs (from the cluster's Private Network). This is the recommended security posture for production: nodes are not reachable..."`
	// Zone within the region to place pool nodes.
	//
	//  Optional. If omitted, Scaleway chooses the zone automatically.
	//  Use this for zone-specific placement (e.g., "fr-par-1", "fr-par-2").
	//
	//  All nodes in this pool will be in the specified zone. For multi-AZ
	//  deployments, create separate pools in different zon...
	Zone string `json:"zone,omitempty" jsonschema:"Zone within the region to place pool nodes. Optional. If omitted; Scaleway chooses the zone automatically. Use this for zone-specific placement (e.g.; 'fr-par-1'; 'fr-par-2'). All nodes in this pool w..."`
	// Placement group ID for anti-affinity scheduling.
	//
	//  Associates pool nodes with a Scaleway Instance placement group. Use
	//  placement groups to spread nodes across different hypervisors for
	//  higher availability.
	//
	//  Must be a valid placement group UUID. Placement groups are created
	//  separately via the Sca...
	PlacementGroupId string `json:"placement_group_id,omitempty" jsonschema:"Placement group ID for anti-affinity scheduling. Associates pool nodes with a Scaleway Instance placement group. Use placement groups to spread nodes across different hypervisors for higher availabili..."`
	// Kubernetes labels to apply to all nodes in this pool.
	//
	//  Labels are key-value pairs used for node selection and workload
	//  scheduling. Pods use `nodeSelector` or node affinity rules to
	//  target nodes with specific labels.
	//
	//  Example: {"workload": "gpu", "team": "ml", "tier": "compute"}
	//
	//  Implementation:...
	KubernetesLabels map[string]string `json:"kubernetes_labels,omitempty" jsonschema:"Kubernetes labels to apply to all nodes in this pool. Labels are key-value pairs used for node selection and workload scheduling. Pods use 'nodeSelector' or node affinity rules to target nodes with sp..."`
	// Kubernetes taints to apply to all nodes in this pool.
	//
	//  Taints prevent pods from being scheduled on these nodes unless they
	//  have matching tolerations. Commonly used for workload isolation
	//  (e.g., GPU nodes, dedicated system pools, batch processing).
	//
	//  Implementation: Taints are applied via Scaleway...
	Taints []*ScalewayKapsulePoolTaintInput `json:"taints,omitempty" jsonschema:"Kubernetes taints to apply to all nodes in this pool. Taints prevent pods from being scheduled on these nodes unless they have matching tolerations. Commonly used for workload isolation (e.g.; GPU nod..."`
	// Node pool upgrade policy for rolling updates.
	//
	//  Controls how nodes are replaced during Kubernetes version upgrades
	//  or pool configuration changes.
	//
	//  Optional. If omitted, Scaleway uses defaults (max_surge=0,
	//  max_unavailable=1 -- one node at a time).
	UpgradePolicy *ScalewayKapsulePoolUpgradePolicyInput `json:"upgrade_policy,omitempty" jsonschema:"Node pool upgrade policy for rolling updates. Controls how nodes are replaced during Kubernetes version upgrades or pool configuration changes. Optional. If omitted; Scaleway uses defaults (max_surge=..."`
	// Custom kubelet arguments for pool nodes.
	//
	//  Power-user escape hatch for setting kubelet flags not exposed as
	//  first-class fields. Example: {"maxPods": "150"}.
	//
	//  WARNING: Use with caution. Incorrect kubelet arguments can prevent
	//  nodes from joining the cluster or cause instability.
	KubeletArgs map[string]string `json:"kubelet_args,omitempty" jsonschema:"Custom kubelet arguments for pool nodes. Power-user escape hatch for setting kubelet flags not exposed as first-class fields. Example: {'maxPods': '150'}. WARNING: Use with caution. Incorrect kubelet ..."`
}

func (s *ScalewayKapsulePoolSpecInput) validate() error {
	if s.Region == "" {
		return fmt.Errorf("region is required")
	}
	if s.ClusterId == "" {
		return fmt.Errorf("cluster_id is required")
	}
	if s.NodeType == "" {
		return fmt.Errorf("node_type is required")
	}
	for i, v := range s.Taints {
		if v != nil {
			if err := v.validate(); err != nil {
				return fmt.Errorf("taints[%d]: %w", i, err)
			}
		}
	}
	if s.UpgradePolicy != nil {
		if err := s.UpgradePolicy.validate(); err != nil {
			return fmt.Errorf("upgrade_policy: %w", err)
		}
	}
	return nil
}

func (s *ScalewayKapsulePoolSpecInput) applyDefaults() {
	if s.UpgradePolicy != nil {
		s.UpgradePolicy.applyDefaults()
	}
}

func (s *ScalewayKapsulePoolSpecInput) toMap() map[string]any {
	m := make(map[string]any)
	m["region"] = s.Region
	m["cluster_id"] = s.ClusterId
	m["node_type"] = s.NodeType
	m["size"] = s.Size
	if s.AutoScale {
		m["auto_scale"] = s.AutoScale
	}
	if s.MinSize != 0 {
		m["min_size"] = s.MinSize
	}
	if s.MaxSize != 0 {
		m["max_size"] = s.MaxSize
	}
	if s.Autohealing {
		m["autohealing"] = s.Autohealing
	}
	if s.ContainerRuntime != "" {
		m["container_runtime"] = s.ContainerRuntime
	}
	if s.RootVolumeType != "" {
		m["root_volume_type"] = s.RootVolumeType
	}
	if s.RootVolumeSizeInGb != 0 {
		m["root_volume_size_in_gb"] = s.RootVolumeSizeInGb
	}
	if s.PublicIpDisabled {
		m["public_ip_disabled"] = s.PublicIpDisabled
	}
	if s.Zone != "" {
		m["zone"] = s.Zone
	}
	if s.PlacementGroupId != "" {
		m["placement_group_id"] = s.PlacementGroupId
	}
	if len(s.KubernetesLabels) > 0 {
		m["kubernetes_labels"] = s.KubernetesLabels
	}
	if len(s.Taints) > 0 {
		items := make([]any, len(s.Taints))
		for i, v := range s.Taints {
			if v != nil {
				items[i] = v.toMap()
			}
		}
		m["taints"] = items
	}
	if s.UpgradePolicy != nil {
		m["upgrade_policy"] = s.UpgradePolicy.toMap()
	}
	if len(s.KubeletArgs) > 0 {
		m["kubelet_args"] = s.KubeletArgs
	}
	return m
}

// ScalewayKapsulePoolTaint represents a Kubernetes taint to apply to all
//
//	nodes in the pool.
//
//	Taints work together with tolerations to ensure that pods are not
//	scheduled onto inappropriate nodes. A taint on a node instructs the
//	scheduler to avoid placing pods that don't tolerate the taint.
type ScalewayKapsulePoolTaintInput struct {
	// The taint key.
	//
	//  Examples: "nvidia.com/gpu", "dedicated", "workload", "node-role"
	Key string `json:"key" jsonschema:"required,The taint key. Examples: 'nvidia.com/gpu'; 'dedicated'; 'workload'; 'node-role'"`
	// The taint value.
	//
	//  Examples: "true", "gpu", "batch", "system"
	Value string `json:"value,omitempty" jsonschema:"The taint value. Examples: 'true'; 'gpu'; 'batch'; 'system'"`
	// The taint effect.
	//
	//  Must be one of:
	//    - "NoSchedule" -- Pods that don't tolerate this taint will not be
	//      scheduled on the node. Existing pods are not evicted.
	//    - "PreferNoSchedule" -- Kubernetes will try to avoid scheduling
	//      pods that don't tolerate this taint, but it's not guaranteed.
	//   ...
	Effect string `json:"effect" jsonschema:"required,The taint effect. Must be one of: - 'NoSchedule' -- Pods that don't tolerate this taint will not be scheduled on the node. Existing pods are not evicted. - 'PreferNoSchedule' -- Kubernetes will try to..."`
}

func (s *ScalewayKapsulePoolTaintInput) validate() error {
	if s.Key == "" {
		return fmt.Errorf("key is required")
	}
	if s.Effect == "" {
		return fmt.Errorf("effect is required")
	}
	return nil
}

func (s *ScalewayKapsulePoolTaintInput) applyDefaults() {
}

func (s *ScalewayKapsulePoolTaintInput) toMap() map[string]any {
	m := make(map[string]any)
	m["key"] = s.Key
	if s.Value != "" {
		m["value"] = s.Value
	}
	m["effect"] = s.Effect
	return m
}

// ScalewayKapsulePoolUpgradePolicy controls how nodes are replaced
//
//	during pool upgrades.
//
//	During a Kubernetes version upgrade or pool reconfiguration, nodes
//	are replaced one or more at a time according to this policy.
type ScalewayKapsulePoolUpgradePolicyInput struct {
	// Maximum number of extra nodes created during an upgrade.
	//
	//  Surge nodes are temporary workers that accept workloads while
	//  existing nodes are drained and replaced. Higher values speed up
	//  upgrades but temporarily increase cost.
	//
	//  Default: 0 (no surge nodes).
	MaxSurge int32 `json:"max_surge,omitempty" jsonschema:"Maximum number of extra nodes created during an upgrade. Surge nodes are temporary workers that accept workloads while existing nodes are drained and replaced. Higher values speed up upgrades but temp..."`
	// Maximum number of nodes that can be unavailable simultaneously
	//  during an upgrade.
	//
	//  Controls the disruption budget. Setting this to 1 means nodes
	//  are replaced one at a time (safest but slowest).
	//
	//  Default: 1.
	MaxUnavailable int32 `json:"max_unavailable,omitempty" jsonschema:"Maximum number of nodes that can be unavailable simultaneously during an upgrade. Controls the disruption budget. Setting this to 1 means nodes are replaced one at a time (safest but slowest). Default..."`
}

func (s *ScalewayKapsulePoolUpgradePolicyInput) validate() error {
	return nil
}

func (s *ScalewayKapsulePoolUpgradePolicyInput) applyDefaults() {
}

func (s *ScalewayKapsulePoolUpgradePolicyInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.MaxSurge != 0 {
		m["max_surge"] = s.MaxSurge
	}
	if s.MaxUnavailable != 0 {
		m["max_unavailable"] = s.MaxUnavailable
	}
	return m
}

// ParseScalewayKapsulePool validates and normalizes a ScalewayKapsulePool cloud_object.
func ParseScalewayKapsulePool(cloudObject map[string]any) (*structpb.Struct, error) {
	if err := parse.ValidateHeader(cloudObject, "scaleway.openmcf.org/v1", "ScalewayKapsulePool"); err != nil {
		return nil, err
	}

	specMap, err := parse.ExtractSpecMap(cloudObject)
	if err != nil {
		return nil, err
	}

	specBytes, err := json.Marshal(specMap)
	if err != nil {
		return nil, fmt.Errorf("marshal spec: %w", err)
	}

	var spec ScalewayKapsulePoolSpecInput
	if err := json.Unmarshal(specBytes, &spec); err != nil {
		return nil, fmt.Errorf("invalid spec: %w", err)
	}

	if err := spec.validate(); err != nil {
		return nil, err
	}

	spec.applyDefaults()

	return parse.RebuildCloudObject(cloudObject, spec.toMap())
}
