// Code generated by schema2go. DO NOT EDIT.
// Generated: 2026-02-26T22:01:03+05:30

package gcp

import (
	"encoding/json"
	"fmt"

	"github.com/plantoncloud/mcp-server-planton/internal/parse"
	"google.golang.org/protobuf/types/known/structpb"
)

var (
	_ = json.Marshal
	_ = fmt.Errorf
	_ = parse.ValidateHeader
	_ = (*structpb.Struct)(nil)
)

// gcp-dataproc-virtual-cluster
type GcpDataprocVirtualClusterSpecInput struct {
	// GCP project in which to create the Dataproc virtual cluster.
	ProjectId string `json:"project_id" jsonschema:"required,GCP project in which to create the Dataproc virtual cluster."`
	// GCP region for the Dataproc virtual cluster (e.g., "us-central1").
	//  Must match the region of the target GKE cluster.
	Region string `json:"region" jsonschema:"required,GCP region for the Dataproc virtual cluster (e.g.; 'us-central1'). Must match the region of the target GKE cluster."`
	// Name of the Dataproc cluster resource in GCP.
	//  Must be lowercase letters, numbers, and hyphens; start with a letter; end with a letter or number.
	//  Optional: when empty, defaults to metadata.name.
	ClusterName string `json:"cluster_name,omitempty" jsonschema:"Name of the Dataproc cluster resource in GCP. Must be lowercase letters; numbers; and hyphens; start with a letter; end with a letter or number. Optional: when empty; defaults to metadata.name."`
	// Fully qualified resource ID of the target GKE cluster.
	//  Format: projects/{project}/locations/{location}/clusters/{name}
	//  The GKE cluster must be in the same project and region as this virtual cluster.
	GkeClusterTarget string `json:"gke_cluster_target" jsonschema:"required,Fully qualified resource ID of the target GKE cluster. Format: projects/{project}/locations/{location}/clusters/{name} The GKE cluster must be in the same project and region as this virtual cluster."`
	// Kubernetes namespace in which the Dataproc virtual cluster will be deployed.
	//  If not specified, Dataproc creates a namespace using the cluster name.
	//  The namespace is created automatically if it does not exist.
	KubernetesNamespace string `json:"kubernetes_namespace,omitempty" jsonschema:"Kubernetes namespace in which the Dataproc virtual cluster will be deployed. If not specified; Dataproc creates a namespace using the cluster name. The namespace is created automatically if it does no..."`
	// Cloud Storage bucket used to stage job dependencies, config files, and
	//  driver console output. If not specified, a default staging bucket is used.
	StagingBucket string `json:"staging_bucket,omitempty" jsonschema:"Cloud Storage bucket used to stage job dependencies; config files; and driver console output. If not specified; a default staging bucket is used."`
	// Kubernetes software configuration for the Dataproc virtual cluster.
	//  Defines component versions (SPARK is mandatory) and optional daemon properties.
	SoftwareConfig *GcpDataprocVirtualClusterSoftwareConfigInput `json:"software_config" jsonschema:"required,Kubernetes software configuration for the Dataproc virtual cluster. Defines component versions (SPARK is mandatory) and optional daemon properties."`
	// GKE node pool targets that define where Dataproc workloads are scheduled.
	//  At least one node pool target must be assigned the DEFAULT role.
	NodePoolTargets []*GcpDataprocVirtualClusterNodePoolTargetInput `json:"node_pool_targets,omitempty" jsonschema:"GKE node pool targets that define where Dataproc workloads are scheduled. At least one node pool target must be assigned the DEFAULT role."`
	// Optional auxiliary services for the virtual cluster.
	AuxiliaryServicesConfig *GcpDataprocVirtualClusterAuxiliaryServicesConfigInput `json:"auxiliary_services_config,omitempty" jsonschema:"Optional auxiliary services for the virtual cluster."`
}

func (s *GcpDataprocVirtualClusterSpecInput) validate() error {
	if s.ProjectId == "" {
		return fmt.Errorf("project_id is required")
	}
	if s.Region == "" {
		return fmt.Errorf("region is required")
	}
	if s.GkeClusterTarget == "" {
		return fmt.Errorf("gke_cluster_target is required")
	}
	if s.SoftwareConfig == nil {
		return fmt.Errorf("software_config is required")
	}
	if s.SoftwareConfig != nil {
		if err := s.SoftwareConfig.validate(); err != nil {
			return fmt.Errorf("software_config: %w", err)
		}
	}
	if len(s.NodePoolTargets) < 1 {
		return fmt.Errorf("node_pool_targets requires at least 1 items, got %d", len(s.NodePoolTargets))
	}
	for i, v := range s.NodePoolTargets {
		if v != nil {
			if err := v.validate(); err != nil {
				return fmt.Errorf("node_pool_targets[%d]: %w", i, err)
			}
		}
	}
	if s.AuxiliaryServicesConfig != nil {
		if err := s.AuxiliaryServicesConfig.validate(); err != nil {
			return fmt.Errorf("auxiliary_services_config: %w", err)
		}
	}
	return nil
}

func (s *GcpDataprocVirtualClusterSpecInput) applyDefaults() {
	if s.SoftwareConfig != nil {
		s.SoftwareConfig.applyDefaults()
	}
	if s.AuxiliaryServicesConfig != nil {
		s.AuxiliaryServicesConfig.applyDefaults()
	}
}

func (s *GcpDataprocVirtualClusterSpecInput) toMap() map[string]any {
	m := make(map[string]any)
	m["project_id"] = s.ProjectId
	m["region"] = s.Region
	if s.ClusterName != "" {
		m["cluster_name"] = s.ClusterName
	}
	m["gke_cluster_target"] = s.GkeClusterTarget
	if s.KubernetesNamespace != "" {
		m["kubernetes_namespace"] = s.KubernetesNamespace
	}
	if s.StagingBucket != "" {
		m["staging_bucket"] = s.StagingBucket
	}
	if s.SoftwareConfig != nil {
		m["software_config"] = s.SoftwareConfig.toMap()
	}
	if len(s.NodePoolTargets) > 0 {
		items := make([]any, len(s.NodePoolTargets))
		for i, v := range s.NodePoolTargets {
			if v != nil {
				items[i] = v.toMap()
			}
		}
		m["node_pool_targets"] = items
	}
	if s.AuxiliaryServicesConfig != nil {
		m["auxiliary_services_config"] = s.AuxiliaryServicesConfig.toMap()
	}
	return m
}

// GcpDataprocVirtualClusterAuxiliaryServicesConfig configures optional services
//
//	that enhance the virtual cluster.
type GcpDataprocVirtualClusterAuxiliaryServicesConfigInput struct {
	// Fully qualified resource name of an existing Dataproc Metastore service
	//  to use as the Hive metastore for the virtual cluster.
	//  Format: projects/{project}/locations/{location}/services/{service}
	MetastoreService string `json:"metastore_service,omitempty" jsonschema:"Fully qualified resource name of an existing Dataproc Metastore service to use as the Hive metastore for the virtual cluster. Format: projects/{project}/locations/{location}/services/{service}"`
	// Fully qualified resource name of an existing Dataproc Cluster that serves
	//  as the Spark History Server for this virtual cluster.
	//  Format: projects/{project}/regions/{region}/clusters/{cluster}
	SparkHistoryServerCluster string `json:"spark_history_server_cluster,omitempty" jsonschema:"Fully qualified resource name of an existing Dataproc Cluster that serves as the Spark History Server for this virtual cluster. Format: projects/{project}/regions/{region}/clusters/{cluster}"`
}

func (s *GcpDataprocVirtualClusterAuxiliaryServicesConfigInput) validate() error {
	return nil
}

func (s *GcpDataprocVirtualClusterAuxiliaryServicesConfigInput) applyDefaults() {
}

func (s *GcpDataprocVirtualClusterAuxiliaryServicesConfigInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.MetastoreService != "" {
		m["metastore_service"] = s.MetastoreService
	}
	if s.SparkHistoryServerCluster != "" {
		m["spark_history_server_cluster"] = s.SparkHistoryServerCluster
	}
	return m
}

// GcpDataprocVirtualClusterNodePoolAutoscaling defines autoscaling bounds
//
//	for a GKE node pool used by Dataproc.
type GcpDataprocVirtualClusterNodePoolAutoscalingInput struct {
	// Minimum number of nodes in the pool. Must be >= 0.
	MinNodeCount int32 `json:"min_node_count,omitempty" jsonschema:"Minimum number of nodes in the pool. Must be >= 0."`
	// Maximum number of nodes in the pool. Must be >= min_node_count and > 0.
	MaxNodeCount int32 `json:"max_node_count,omitempty" jsonschema:"Maximum number of nodes in the pool. Must be >= min_node_count and > 0."`
}

func (s *GcpDataprocVirtualClusterNodePoolAutoscalingInput) validate() error {
	return nil
}

func (s *GcpDataprocVirtualClusterNodePoolAutoscalingInput) applyDefaults() {
}

func (s *GcpDataprocVirtualClusterNodePoolAutoscalingInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.MinNodeCount != 0 {
		m["min_node_count"] = s.MinNodeCount
	}
	if s.MaxNodeCount != 0 {
		m["max_node_count"] = s.MaxNodeCount
	}
	return m
}

// GcpDataprocVirtualClusterNodePoolConfig defines the desired shape of a GKE
//
//	node pool used by the Dataproc virtual cluster.
type GcpDataprocVirtualClusterNodePoolConfigInput struct {
	// Compute Engine zones where node pool nodes will be located.
	//  All node pools must use the same set of locations.
	Locations []string `json:"locations,omitempty" jsonschema:"Compute Engine zones where node pool nodes will be located. All node pools must use the same set of locations."`
	// Autoscaling configuration for this node pool.
	Autoscaling *GcpDataprocVirtualClusterNodePoolAutoscalingInput `json:"autoscaling,omitempty" jsonschema:"Autoscaling configuration for this node pool."`
	// Compute Engine machine type for nodes (e.g., "n1-standard-4").
	MachineType string `json:"machine_type,omitempty" jsonschema:"Compute Engine machine type for nodes (e.g.; 'n1-standard-4')."`
	// Number of local SSD disks to attach to each node.
	LocalSsdCount int32 `json:"local_ssd_count,omitempty" jsonschema:"Number of local SSD disks to attach to each node."`
	// Minimum CPU platform (e.g., "Intel Haswell", "Intel Sandy Bridge").
	MinCpuPlatform string `json:"min_cpu_platform,omitempty" jsonschema:"Minimum CPU platform (e.g.; 'Intel Haswell'; 'Intel Sandy Bridge')."`
	// Whether nodes are created as preemptible VM instances.
	//  Preemptible nodes cannot be used in a node pool with the CONTROLLER role
	//  or the DEFAULT role when CONTROLLER is not separately assigned.
	Preemptible bool `json:"preemptible,omitempty" jsonschema:"Whether nodes are created as preemptible VM instances. Preemptible nodes cannot be used in a node pool with the CONTROLLER role or the DEFAULT role when CONTROLLER is not separately assigned."`
	// Whether nodes are created as Spot VM instances (rebrand of preemptible).
	//  Same role restrictions as preemptible.
	Spot bool `json:"spot,omitempty" jsonschema:"Whether nodes are created as Spot VM instances (rebrand of preemptible). Same role restrictions as preemptible."`
}

func (s *GcpDataprocVirtualClusterNodePoolConfigInput) validate() error {
	if s.Autoscaling != nil {
		if err := s.Autoscaling.validate(); err != nil {
			return fmt.Errorf("autoscaling: %w", err)
		}
	}
	return nil
}

func (s *GcpDataprocVirtualClusterNodePoolConfigInput) applyDefaults() {
	if s.Autoscaling != nil {
		s.Autoscaling.applyDefaults()
	}
}

func (s *GcpDataprocVirtualClusterNodePoolConfigInput) toMap() map[string]any {
	m := make(map[string]any)
	if len(s.Locations) > 0 {
		m["locations"] = s.Locations
	}
	if s.Autoscaling != nil {
		m["autoscaling"] = s.Autoscaling.toMap()
	}
	if s.MachineType != "" {
		m["machine_type"] = s.MachineType
	}
	if s.LocalSsdCount != 0 {
		m["local_ssd_count"] = s.LocalSsdCount
	}
	if s.MinCpuPlatform != "" {
		m["min_cpu_platform"] = s.MinCpuPlatform
	}
	if s.Preemptible {
		m["preemptible"] = s.Preemptible
	}
	if s.Spot {
		m["spot"] = s.Spot
	}
	return m
}

// GcpDataprocVirtualClusterNodePoolTarget assigns a GKE node pool to a Dataproc role.
//
//	Roles determine which workload types are scheduled on each node pool.
type GcpDataprocVirtualClusterNodePoolTargetInput struct {
	// Reference to a GKE node pool. Can be a short name (resolved within the
	//  target GKE cluster) or a fully qualified resource path.
	NodePool string `json:"node_pool" jsonschema:"required,Reference to a GKE node pool. Can be a short name (resolved within the target GKE cluster) or a fully qualified resource path."`
	// Dataproc roles assigned to this node pool.
	//  Valid values: "DEFAULT", "CONTROLLER", "SPARK_DRIVER", "SPARK_EXECUTOR".
	//  At least one node pool target across the cluster must have the "DEFAULT" role.
	//  Each role may only be assigned to one node pool target.
	Roles []string `json:"roles,omitempty" jsonschema:"Dataproc roles assigned to this node pool. Valid values: 'DEFAULT'; 'CONTROLLER'; 'SPARK_DRIVER'; 'SPARK_EXECUTOR'. At least one node pool target across the cluster must have the 'DEFAULT' role. Each ..."`
	// Optional node pool configuration. When specified, Dataproc attempts to
	//  create or verify a node pool matching this shape. If a node pool with the
	//  same name already exists, it is verified against all specified fields.
	NodePoolConfig *GcpDataprocVirtualClusterNodePoolConfigInput `json:"node_pool_config,omitempty" jsonschema:"Optional node pool configuration. When specified; Dataproc attempts to create or verify a node pool matching this shape. If a node pool with the same name already exists; it is verified against all sp..."`
}

func (s *GcpDataprocVirtualClusterNodePoolTargetInput) validate() error {
	if s.NodePool == "" {
		return fmt.Errorf("node_pool is required")
	}
	if len(s.Roles) < 1 {
		return fmt.Errorf("roles requires at least 1 items, got %d", len(s.Roles))
	}
	if s.NodePoolConfig != nil {
		if err := s.NodePoolConfig.validate(); err != nil {
			return fmt.Errorf("node_pool_config: %w", err)
		}
	}
	return nil
}

func (s *GcpDataprocVirtualClusterNodePoolTargetInput) applyDefaults() {
	if s.NodePoolConfig != nil {
		s.NodePoolConfig.applyDefaults()
	}
}

func (s *GcpDataprocVirtualClusterNodePoolTargetInput) toMap() map[string]any {
	m := make(map[string]any)
	m["node_pool"] = s.NodePool
	if len(s.Roles) > 0 {
		m["roles"] = s.Roles
	}
	if s.NodePoolConfig != nil {
		m["node_pool_config"] = s.NodePoolConfig.toMap()
	}
	return m
}

// GcpDataprocVirtualClusterSoftwareConfig defines the Kubernetes-level software
//
//	configuration for the Dataproc virtual cluster.
type GcpDataprocVirtualClusterSoftwareConfigInput struct {
	// Component versions to install. Keys are component names from the
	//  KubernetesComponent enumeration (e.g., "SPARK"). Values are version strings
	//  (e.g., "3.5-dataproc-17").
	//
	//  The "SPARK" component is mandatory -- the cluster will fail to create without it.
	ComponentVersion map[string]string `json:"component_version" jsonschema:"required,Component versions to install. Keys are component names from the KubernetesComponent enumeration (e.g.; 'SPARK'). Values are version strings (e.g.; '3.5-dataproc-17'). The 'SPARK' component is mandato..."`
	// Optional daemon config properties in prefix:property format.
	//  Example: {"spark:spark.kubernetes.container.image": "custom-image:latest"}
	Properties map[string]string `json:"properties,omitempty" jsonschema:"Optional daemon config properties in prefix:property format. Example: {'spark:spark.kubernetes.container.image': 'custom-image:latest'}"`
}

func (s *GcpDataprocVirtualClusterSoftwareConfigInput) validate() error {
	return nil
}

func (s *GcpDataprocVirtualClusterSoftwareConfigInput) applyDefaults() {
}

func (s *GcpDataprocVirtualClusterSoftwareConfigInput) toMap() map[string]any {
	m := make(map[string]any)
	if len(s.ComponentVersion) > 0 {
		m["component_version"] = s.ComponentVersion
	}
	if len(s.Properties) > 0 {
		m["properties"] = s.Properties
	}
	return m
}

// ParseGcpDataprocVirtualCluster validates and normalizes a GcpDataprocVirtualCluster cloud_object.
func ParseGcpDataprocVirtualCluster(cloudObject map[string]any) (*structpb.Struct, error) {
	if err := parse.ValidateHeader(cloudObject, "gcp.openmcf.org/v1", "GcpDataprocVirtualCluster"); err != nil {
		return nil, err
	}

	specMap, err := parse.ExtractSpecMap(cloudObject)
	if err != nil {
		return nil, err
	}

	specBytes, err := json.Marshal(specMap)
	if err != nil {
		return nil, fmt.Errorf("marshal spec: %w", err)
	}

	var spec GcpDataprocVirtualClusterSpecInput
	if err := json.Unmarshal(specBytes, &spec); err != nil {
		return nil, fmt.Errorf("invalid spec: %w", err)
	}

	if err := spec.validate(); err != nil {
		return nil, err
	}

	spec.applyDefaults()

	return parse.RebuildCloudObject(cloudObject, spec.toMap())
}
