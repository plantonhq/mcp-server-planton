// Code generated by schema2go. DO NOT EDIT.
// Generated: 2026-02-26T21:50:49+05:30

package kubernetes

import (
	"encoding/json"
	"fmt"

	"github.com/plantoncloud/mcp-server-planton/gen/parse"
	"google.golang.org/protobuf/types/known/structpb"
)

var (
	_ = json.Marshal
	_ = fmt.Errorf
	_ = parse.ValidateHeader
	_ = (*structpb.Struct)(nil)
)

// kafka-kubernetes
type KubernetesKafkaSpecInput struct {
	// Target Kubernetes Cluster
	TargetCluster *KubernetesClusterSelectorInput `json:"target_cluster,omitempty" jsonschema:"Target Kubernetes Cluster"`
	// Kubernetes Namespace
	Namespace string `json:"namespace" jsonschema:"required,Kubernetes Namespace"`
	// flag to indicate if the namespace should be created
	CreateNamespace bool `json:"create_namespace,omitempty" jsonschema:"flag to indicate if the namespace should be created"`
	// A list of Kafka topics to be created in the Kafka cluster.
	KafkaTopics []*KafkaTopicInput `json:"kafka_topics,omitempty" jsonschema:"A list of Kafka topics to be created in the Kafka cluster."`
	// The specifications for the Kafka broker containers.
	BrokerContainer *KubernetesKafkaBrokerContainerInput `json:"broker_container,omitempty" jsonschema:"The specifications for the Kafka broker containers."`
	// The specifications for the Zookeeper containers.
	ZookeeperContainer *KubernetesKafkaZookeeperContainerInput `json:"zookeeper_container,omitempty" jsonschema:"The specifications for the Zookeeper containers."`
	// The specifications for the Schema Registry containers.
	SchemaRegistryContainer *KubernetesKafkaSchemaRegistryContainerInput `json:"schema_registry_container,omitempty" jsonschema:"The specifications for the Schema Registry containers."`
	// The ingress configuration for the Kafka deployment.
	Ingress *KubernetesKafkaIngressInput `json:"ingress,omitempty" jsonschema:"The ingress configuration for the Kafka deployment."`
	// A flag to toggle the deployment of the Kafka UI component.
	IsDeployKafkaUi bool `json:"is_deploy_kafka_ui,omitempty" jsonschema:"A flag to toggle the deployment of the Kafka UI component."`
}

func (s *KubernetesKafkaSpecInput) validate() error {
	if s.TargetCluster != nil {
		if err := s.TargetCluster.validate(); err != nil {
			return fmt.Errorf("target_cluster: %w", err)
		}
	}
	if s.Namespace == "" {
		return fmt.Errorf("namespace is required")
	}
	for i, v := range s.KafkaTopics {
		if v != nil {
			if err := v.validate(); err != nil {
				return fmt.Errorf("kafka_topics[%d]: %w", i, err)
			}
		}
	}
	if s.BrokerContainer != nil {
		if err := s.BrokerContainer.validate(); err != nil {
			return fmt.Errorf("broker_container: %w", err)
		}
	}
	if s.ZookeeperContainer != nil {
		if err := s.ZookeeperContainer.validate(); err != nil {
			return fmt.Errorf("zookeeper_container: %w", err)
		}
	}
	if s.SchemaRegistryContainer != nil {
		if err := s.SchemaRegistryContainer.validate(); err != nil {
			return fmt.Errorf("schema_registry_container: %w", err)
		}
	}
	if s.Ingress != nil {
		if err := s.Ingress.validate(); err != nil {
			return fmt.Errorf("ingress: %w", err)
		}
	}
	return nil
}

func (s *KubernetesKafkaSpecInput) applyDefaults() {
	if s.TargetCluster != nil {
		s.TargetCluster.applyDefaults()
	}
	if s.BrokerContainer != nil {
		s.BrokerContainer.applyDefaults()
	}
	if s.ZookeeperContainer != nil {
		s.ZookeeperContainer.applyDefaults()
	}
	if s.SchemaRegistryContainer != nil {
		s.SchemaRegistryContainer.applyDefaults()
	}
	if s.Ingress != nil {
		s.Ingress.applyDefaults()
	}
}

func (s *KubernetesKafkaSpecInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.TargetCluster != nil {
		m["target_cluster"] = s.TargetCluster.toMap()
	}
	m["namespace"] = s.Namespace
	if s.CreateNamespace {
		m["create_namespace"] = s.CreateNamespace
	}
	if len(s.KafkaTopics) > 0 {
		items := make([]any, len(s.KafkaTopics))
		for i, v := range s.KafkaTopics {
			if v != nil {
				items[i] = v.toMap()
			}
		}
		m["kafka_topics"] = items
	}
	if s.BrokerContainer != nil {
		m["broker_container"] = s.BrokerContainer.toMap()
	}
	if s.ZookeeperContainer != nil {
		m["zookeeper_container"] = s.ZookeeperContainer.toMap()
	}
	if s.SchemaRegistryContainer != nil {
		m["schema_registry_container"] = s.SchemaRegistryContainer.toMap()
	}
	if s.Ingress != nil {
		m["ingress"] = s.Ingress.toMap()
	}
	if s.IsDeployKafkaUi {
		m["is_deploy_kafka_ui"] = s.IsDeployKafkaUi
	}
	return m
}

// **KafkaTopic** represents a Kafka topic to be created in the Kafka cluster.
//
//	It includes configurations such as the topic name, number of partitions, replicas, and additional configurations.
type KafkaTopicInput struct {
	// The name of the Kafka topic.
	//  Must be between 1 and 249 characters in length.
	//  The name must start and end with an alphanumeric character, can contain alphanumeric characters, '.', '_', and '-'.
	//  Must not contain '..' or non-ASCII characters.
	Name string `json:"name" jsonschema:"required,The name of the Kafka topic. Must be between 1 and 249 characters in length. The name must start and end with an alphanumeric character; can contain alphanumeric characters; '.'; '_'; and '-'. Must no..."`
	// The number of partitions for the topic.
	//  Recommended default is 1.
	Partitions int32 `json:"partitions,omitempty" jsonschema:"The number of partitions for the topic. Recommended default is 1."`
	// The number of replicas for the topic.
	//  Recommended default is 1.
	Replicas int32 `json:"replicas,omitempty" jsonschema:"The number of replicas for the topic. Recommended default is 1."`
	// Additional configuration for the Kafka topic.
	//  If not provided, default values will be set.
	//  For example, the default `delete.policy` is `delete`, but it can be set to `compact`.
	Config map[string]string `json:"config,omitempty" jsonschema:"Additional configuration for the Kafka topic. If not provided; default values will be set. For example; the default 'delete.policy' is 'delete'; but it can be set to 'compact'."`
}

func (s *KafkaTopicInput) validate() error {
	if s.Name == "" {
		return fmt.Errorf("name is required")
	}
	return nil
}

func (s *KafkaTopicInput) applyDefaults() {
	if s.Partitions == 0 {
		s.Partitions = 1
	}
	if s.Replicas == 0 {
		s.Replicas = 1
	}
}

func (s *KafkaTopicInput) toMap() map[string]any {
	m := make(map[string]any)
	m["name"] = s.Name
	if s.Partitions != 0 {
		m["partitions"] = s.Partitions
	}
	if s.Replicas != 0 {
		m["replicas"] = s.Replicas
	}
	if len(s.Config) > 0 {
		m["config"] = s.Config
	}
	return m
}

// **KubernetesKafkaBrokerContainer** specifies the configuration for the Kafka broker containers.
//
//	It includes settings such as the number of replicas, resource allocations, and disk size.
//	Proper configuration ensures optimal performance and data reliability for your Kafka brokers.
type KubernetesKafkaBrokerContainerInput struct {
	// The number of Kafka brokers to deploy.
	//  Defaults to 1 if the client sets the value to 0.
	//  Recommended default value is 1.
	Replicas int32 `json:"replicas,omitempty" jsonschema:"The number of Kafka brokers to deploy. Defaults to 1 if the client sets the value to 0. Recommended default value is 1."`
	// The CPU and memory resources allocated to the Kafka broker containers.
	Resources *ContainerResourcesInput `json:"resources,omitempty" jsonschema:"The CPU and memory resources allocated to the Kafka broker containers."`
	// The size of the disk to be attached to each broker instance (e.g., "30Gi").
	//  A default value is set if not provided by the client.
	DiskSize string `json:"disk_size,omitempty" jsonschema:"The size of the disk to be attached to each broker instance (e.g.; '30Gi'). A default value is set if not provided by the client."`
}

func (s *KubernetesKafkaBrokerContainerInput) validate() error {
	if s.Resources != nil {
		if err := s.Resources.validate(); err != nil {
			return fmt.Errorf("resources: %w", err)
		}
	}
	return nil
}

func (s *KubernetesKafkaBrokerContainerInput) applyDefaults() {
	if s.Resources != nil {
		s.Resources.applyDefaults()
	}
}

func (s *KubernetesKafkaBrokerContainerInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.Replicas != 0 {
		m["replicas"] = s.Replicas
	}
	if s.Resources != nil {
		m["resources"] = s.Resources.toMap()
	}
	if s.DiskSize != "" {
		m["disk_size"] = s.DiskSize
	}
	return m
}

// *
//
//	KubernetesKafkaIngress defines ingress configuration for Kafka.
type KubernetesKafkaIngressInput struct {
	// Flag to enable or disable ingress.
	Enabled bool `json:"enabled,omitempty" jsonschema:"Flag to enable or disable ingress."`
	// The full hostname for external access (e.g., "kafka.example.com").
	//  Required when enabled is true.
	Hostname string `json:"hostname,omitempty" jsonschema:"The full hostname for external access (e.g.; 'kafka.example.com'). Required when enabled is true."`
}

func (s *KubernetesKafkaIngressInput) validate() error {
	return nil
}

func (s *KubernetesKafkaIngressInput) applyDefaults() {
}

func (s *KubernetesKafkaIngressInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.Enabled {
		m["enabled"] = s.Enabled
	}
	if s.Hostname != "" {
		m["hostname"] = s.Hostname
	}
	return m
}

// **KubernetesKafkaSchemaRegistryContainer** specifies the configuration for the Schema Registry containers.
//
//	The Schema Registry provides a serving layer for your metadata, allowing data producers and consumers to evolve independently.
type KubernetesKafkaSchemaRegistryContainerInput struct {
	// A flag to control whether the Schema Registry is created for the Kafka deployment.
	//  Defaults to `false`.
	IsEnabled bool `json:"is_enabled,omitempty" jsonschema:"A flag to control whether the Schema Registry is created for the Kafka deployment. Defaults to 'false'."`
	// The number of Schema Registry replicas.
	//  Recommended default value is "1".
	//  This value has no effect if `is_enabled` is set to `false`.
	Replicas int32 `json:"replicas,omitempty" jsonschema:"The number of Schema Registry replicas. Recommended default value is '1'. This value has no effect if 'is_enabled' is set to 'false'."`
	// The CPU and memory resources allocated to the Schema Registry containers.
	Resources *ContainerResourcesInput `json:"resources,omitempty" jsonschema:"The CPU and memory resources allocated to the Schema Registry containers."`
}

func (s *KubernetesKafkaSchemaRegistryContainerInput) validate() error {
	if s.Resources != nil {
		if err := s.Resources.validate(); err != nil {
			return fmt.Errorf("resources: %w", err)
		}
	}
	return nil
}

func (s *KubernetesKafkaSchemaRegistryContainerInput) applyDefaults() {
	if s.Resources != nil {
		s.Resources.applyDefaults()
	}
}

func (s *KubernetesKafkaSchemaRegistryContainerInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.IsEnabled {
		m["is_enabled"] = s.IsEnabled
	}
	if s.Replicas != 0 {
		m["replicas"] = s.Replicas
	}
	if s.Resources != nil {
		m["resources"] = s.Resources.toMap()
	}
	return m
}

// **KubernetesKafkaZookeeperContainer** specifies the configuration for the Zookeeper containers.
//
//	Zookeeper is required for Kafka cluster management and coordination.
//	Proper configuration ensures high availability and reliability of your Kafka cluster.
type KubernetesKafkaZookeeperContainerInput struct {
	// The number of Zookeeper container replicas.
	//  Zookeeper requires at least 3 replicas for high availability (HA) mode.
	//  Zookeeper uses the Raft consensus algorithm; refer to https://raft.github.io/ for more information on how replica
	//  count affects availability.
	Replicas int32 `json:"replicas,omitempty" jsonschema:"The number of Zookeeper container replicas. Zookeeper requires at least 3 replicas for high availability (HA) mode. Zookeeper uses the Raft consensus algorithm; refer to https://raft.github.io/ for mo..."`
	// The CPU and memory resources allocated to the Zookeeper containers.
	Resources *ContainerResourcesInput `json:"resources,omitempty" jsonschema:"The CPU and memory resources allocated to the Zookeeper containers."`
	// The size of the disk to be attached to each Zookeeper instance (e.g., "30Gi").
	//  A default value is set if not provided by the client.
	DiskSize string `json:"disk_size,omitempty" jsonschema:"The size of the disk to be attached to each Zookeeper instance (e.g.; '30Gi'). A default value is set if not provided by the client."`
}

func (s *KubernetesKafkaZookeeperContainerInput) validate() error {
	if s.Resources != nil {
		if err := s.Resources.validate(); err != nil {
			return fmt.Errorf("resources: %w", err)
		}
	}
	return nil
}

func (s *KubernetesKafkaZookeeperContainerInput) applyDefaults() {
	if s.Resources != nil {
		s.Resources.applyDefaults()
	}
}

func (s *KubernetesKafkaZookeeperContainerInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.Replicas != 0 {
		m["replicas"] = s.Replicas
	}
	if s.Resources != nil {
		m["resources"] = s.Resources.toMap()
	}
	if s.DiskSize != "" {
		m["disk_size"] = s.DiskSize
	}
	return m
}

// ParseKubernetesKafka validates and normalizes a KubernetesKafka cloud_object.
func ParseKubernetesKafka(cloudObject map[string]any) (*structpb.Struct, error) {
	if err := parse.ValidateHeader(cloudObject, "kubernetes.openmcf.org/v1", "KubernetesKafka"); err != nil {
		return nil, err
	}

	specMap, err := parse.ExtractSpecMap(cloudObject)
	if err != nil {
		return nil, err
	}

	specBytes, err := json.Marshal(specMap)
	if err != nil {
		return nil, fmt.Errorf("marshal spec: %w", err)
	}

	var spec KubernetesKafkaSpecInput
	if err := json.Unmarshal(specBytes, &spec); err != nil {
		return nil, fmt.Errorf("invalid spec: %w", err)
	}

	if err := spec.validate(); err != nil {
		return nil, err
	}

	spec.applyDefaults()

	return parse.RebuildCloudObject(cloudObject, spec.toMap())
}
