// Code generated by schema2go. DO NOT EDIT.
// Generated: 2026-02-26T22:01:03+05:30

package kubernetes

import (
	"encoding/json"
	"fmt"

	"github.com/plantoncloud/mcp-server-planton/internal/parse"
	"google.golang.org/protobuf/types/known/structpb"
)

var (
	_ = json.Marshal
	_ = fmt.Errorf
	_ = parse.ValidateHeader
	_ = (*structpb.Struct)(nil)
)

// KubernetesRookCephCluster is the top-level resource representing a Rook Ceph storage cluster on Kubernetes.
//
//	The Rook Ceph Operator must be installed before deploying this resource.
//	This component creates a CephCluster with optional block pools, filesystems, and object stores
//	for providing distributed storage services including block, file, and object storage.
type KubernetesRookCephClusterSpecInput struct {
	// Target Kubernetes Cluster where the Ceph cluster will be deployed.
	TargetCluster *KubernetesClusterSelectorInput `json:"target_cluster,omitempty" jsonschema:"Target Kubernetes Cluster where the Ceph cluster will be deployed."`
	// Kubernetes Namespace where the Ceph cluster will be installed.
	//  This namespace should match or be different from the operator namespace depending on your multi-tenancy requirements.
	//  Default: rook-ceph
	Namespace string `json:"namespace" jsonschema:"required,Kubernetes Namespace where the Ceph cluster will be installed. This namespace should match or be different from the operator namespace depending on your multi-tenancy requirements. Default: rook-ceph"`
	// Flag to indicate if the namespace should be created if it does not exist.
	//  Default: true
	CreateNamespace bool `json:"create_namespace,omitempty" jsonschema:"Flag to indicate if the namespace should be created if it does not exist. Default: true"`
	// Namespace where the Rook Ceph Operator is installed.
	//  Default: rook-ceph
	OperatorNamespace string `json:"operator_namespace,omitempty" jsonschema:"Namespace where the Rook Ceph Operator is installed. Default: rook-ceph"`
	// The version of the Rook Ceph Cluster Helm chart to deploy.
	//  https://github.com/rook/rook/releases
	//  Default: v1.16.6
	HelmChartVersion string `json:"helm_chart_version,omitempty" jsonschema:"The version of the Rook Ceph Cluster Helm chart to deploy. https://github.com/rook/rook/releases Default: v1.16.6"`
	// Ceph container image configuration.
	CephImage *CephImageSpecInput `json:"ceph_image,omitempty" jsonschema:"Ceph container image configuration."`
	// Core Ceph cluster configuration.
	Cluster *CephClusterConfigInput `json:"cluster,omitempty" jsonschema:"Core Ceph cluster configuration."`
	// Block storage pool configuration.
	//  Block pools provide RBD (RADOS Block Device) storage for persistent volumes.
	BlockPools []*CephBlockPoolSpecInput `json:"block_pools,omitempty" jsonschema:"Block storage pool configuration. Block pools provide RBD (RADOS Block Device) storage for persistent volumes."`
	// Filesystem configuration for CephFS.
	//  CephFS provides POSIX-compliant shared filesystem storage.
	Filesystems []*CephFilesystemSpecInput `json:"filesystems,omitempty" jsonschema:"Filesystem configuration for CephFS. CephFS provides POSIX-compliant shared filesystem storage."`
	// Object store configuration for S3-compatible storage.
	//  Object stores provide Ceph RADOS Gateway (RGW) for S3/Swift API access.
	ObjectStores []*CephObjectStoreSpecInput `json:"object_stores,omitempty" jsonschema:"Object store configuration for S3-compatible storage. Object stores provide Ceph RADOS Gateway (RGW) for S3/Swift API access."`
	// Enable the Ceph toolbox deployment for debugging.
	//  Default: false
	EnableToolbox bool `json:"enable_toolbox,omitempty" jsonschema:"Enable the Ceph toolbox deployment for debugging. Default: false"`
	// Enable Prometheus monitoring integration.
	//  Default: false
	EnableMonitoring bool `json:"enable_monitoring,omitempty" jsonschema:"Enable Prometheus monitoring integration. Default: false"`
	// Enable Ceph dashboard for web-based cluster management.
	//  Default: true
	EnableDashboard bool `json:"enable_dashboard,omitempty" jsonschema:"Enable Ceph dashboard for web-based cluster management. Default: true"`
}

func (s *KubernetesRookCephClusterSpecInput) validate() error {
	if s.TargetCluster != nil {
		if err := s.TargetCluster.validate(); err != nil {
			return fmt.Errorf("target_cluster: %w", err)
		}
	}
	if s.Namespace == "" {
		return fmt.Errorf("namespace is required")
	}
	if s.CephImage != nil {
		if err := s.CephImage.validate(); err != nil {
			return fmt.Errorf("ceph_image: %w", err)
		}
	}
	if s.Cluster != nil {
		if err := s.Cluster.validate(); err != nil {
			return fmt.Errorf("cluster: %w", err)
		}
	}
	for i, v := range s.BlockPools {
		if v != nil {
			if err := v.validate(); err != nil {
				return fmt.Errorf("block_pools[%d]: %w", i, err)
			}
		}
	}
	for i, v := range s.Filesystems {
		if v != nil {
			if err := v.validate(); err != nil {
				return fmt.Errorf("filesystems[%d]: %w", i, err)
			}
		}
	}
	for i, v := range s.ObjectStores {
		if v != nil {
			if err := v.validate(); err != nil {
				return fmt.Errorf("object_stores[%d]: %w", i, err)
			}
		}
	}
	return nil
}

func (s *KubernetesRookCephClusterSpecInput) applyDefaults() {
	if s.TargetCluster != nil {
		s.TargetCluster.applyDefaults()
	}
	// default: CreateNamespace = true (applied at zero-value)
	if s.OperatorNamespace == "" {
		s.OperatorNamespace = "rook-ceph"
	}
	if s.HelmChartVersion == "" {
		s.HelmChartVersion = "v1.16.6"
	}
	if s.CephImage != nil {
		s.CephImage.applyDefaults()
	}
	if s.Cluster != nil {
		s.Cluster.applyDefaults()
	}
	// default: EnableDashboard = true (applied at zero-value)
}

func (s *KubernetesRookCephClusterSpecInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.TargetCluster != nil {
		m["target_cluster"] = s.TargetCluster.toMap()
	}
	m["namespace"] = s.Namespace
	if s.CreateNamespace {
		m["create_namespace"] = s.CreateNamespace
	}
	if s.OperatorNamespace != "" {
		m["operator_namespace"] = s.OperatorNamespace
	}
	if s.HelmChartVersion != "" {
		m["helm_chart_version"] = s.HelmChartVersion
	}
	if s.CephImage != nil {
		m["ceph_image"] = s.CephImage.toMap()
	}
	if s.Cluster != nil {
		m["cluster"] = s.Cluster.toMap()
	}
	if len(s.BlockPools) > 0 {
		items := make([]any, len(s.BlockPools))
		for i, v := range s.BlockPools {
			if v != nil {
				items[i] = v.toMap()
			}
		}
		m["block_pools"] = items
	}
	if len(s.Filesystems) > 0 {
		items := make([]any, len(s.Filesystems))
		for i, v := range s.Filesystems {
			if v != nil {
				items[i] = v.toMap()
			}
		}
		m["filesystems"] = items
	}
	if len(s.ObjectStores) > 0 {
		items := make([]any, len(s.ObjectStores))
		for i, v := range s.ObjectStores {
			if v != nil {
				items[i] = v.toMap()
			}
		}
		m["object_stores"] = items
	}
	if s.EnableToolbox {
		m["enable_toolbox"] = s.EnableToolbox
	}
	if s.EnableMonitoring {
		m["enable_monitoring"] = s.EnableMonitoring
	}
	if s.EnableDashboard {
		m["enable_dashboard"] = s.EnableDashboard
	}
	return m
}

// CephBlockPoolSpec defines a Ceph block storage pool configuration.
type CephBlockPoolSpecInput struct {
	// Name of the block pool.
	Name string `json:"name,omitempty" jsonschema:"Name of the block pool."`
	// Failure domain for data placement (host, rack, zone, etc).
	//  Default: host
	FailureDomain string `json:"failure_domain,omitempty" jsonschema:"Failure domain for data placement (host; rack; zone; etc). Default: host"`
	// Number of data replicas.
	//  Default: 3
	ReplicatedSize int32 `json:"replicated_size,omitempty" jsonschema:"Number of data replicas. Default: 3"`
	// StorageClass configuration for this pool.
	StorageClass *CephStorageClassSpecInput `json:"storage_class,omitempty" jsonschema:"StorageClass configuration for this pool."`
}

func (s *CephBlockPoolSpecInput) validate() error {
	if s.StorageClass != nil {
		if err := s.StorageClass.validate(); err != nil {
			return fmt.Errorf("storage_class: %w", err)
		}
	}
	return nil
}

func (s *CephBlockPoolSpecInput) applyDefaults() {
	if s.FailureDomain == "" {
		s.FailureDomain = "host"
	}
	if s.ReplicatedSize == 0 {
		s.ReplicatedSize = 3
	}
	if s.StorageClass != nil {
		s.StorageClass.applyDefaults()
	}
}

func (s *CephBlockPoolSpecInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.Name != "" {
		m["name"] = s.Name
	}
	if s.FailureDomain != "" {
		m["failure_domain"] = s.FailureDomain
	}
	if s.ReplicatedSize != 0 {
		m["replicated_size"] = s.ReplicatedSize
	}
	if s.StorageClass != nil {
		m["storage_class"] = s.StorageClass.toMap()
	}
	return m
}

// CephClusterConfig defines the core Ceph cluster settings.
type CephClusterConfigInput struct {
	// The path on the host where Ceph configuration files will be persisted.
	//  Must be unique if running multiple Ceph clusters.
	//  Default: /var/lib/rook
	DataDirHostPath string `json:"data_dir_host_path,omitempty" jsonschema:"The path on the host where Ceph configuration files will be persisted. Must be unique if running multiple Ceph clusters. Default: /var/lib/rook"`
	// Monitor (MON) daemon configuration.
	Mon *CephMonSpecInput `json:"mon,omitempty" jsonschema:"Monitor (MON) daemon configuration."`
	// Manager (MGR) daemon configuration.
	Mgr *CephMgrSpecInput `json:"mgr,omitempty" jsonschema:"Manager (MGR) daemon configuration."`
	// Storage configuration for OSDs (Object Storage Daemons).
	Storage *CephStorageSpecInput `json:"storage,omitempty" jsonschema:"Storage configuration for OSDs (Object Storage Daemons)."`
	// Resource requests and limits for Ceph daemons.
	Resources *CephResourcesSpecInput `json:"resources,omitempty" jsonschema:"Resource requests and limits for Ceph daemons."`
	// Network configuration for the Ceph cluster.
	Network *CephNetworkSpecInput `json:"network,omitempty" jsonschema:"Network configuration for the Ceph cluster."`
}

func (s *CephClusterConfigInput) validate() error {
	if s.Mon != nil {
		if err := s.Mon.validate(); err != nil {
			return fmt.Errorf("mon: %w", err)
		}
	}
	if s.Mgr != nil {
		if err := s.Mgr.validate(); err != nil {
			return fmt.Errorf("mgr: %w", err)
		}
	}
	if s.Storage != nil {
		if err := s.Storage.validate(); err != nil {
			return fmt.Errorf("storage: %w", err)
		}
	}
	if s.Resources != nil {
		if err := s.Resources.validate(); err != nil {
			return fmt.Errorf("resources: %w", err)
		}
	}
	if s.Network != nil {
		if err := s.Network.validate(); err != nil {
			return fmt.Errorf("network: %w", err)
		}
	}
	return nil
}

func (s *CephClusterConfigInput) applyDefaults() {
	if s.DataDirHostPath == "" {
		s.DataDirHostPath = "/var/lib/rook"
	}
	if s.Mon != nil {
		s.Mon.applyDefaults()
	}
	if s.Mgr != nil {
		s.Mgr.applyDefaults()
	}
	if s.Storage != nil {
		s.Storage.applyDefaults()
	}
	if s.Resources != nil {
		s.Resources.applyDefaults()
	}
	if s.Network != nil {
		s.Network.applyDefaults()
	}
}

func (s *CephClusterConfigInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.DataDirHostPath != "" {
		m["data_dir_host_path"] = s.DataDirHostPath
	}
	if s.Mon != nil {
		m["mon"] = s.Mon.toMap()
	}
	if s.Mgr != nil {
		m["mgr"] = s.Mgr.toMap()
	}
	if s.Storage != nil {
		m["storage"] = s.Storage.toMap()
	}
	if s.Resources != nil {
		m["resources"] = s.Resources.toMap()
	}
	if s.Network != nil {
		m["network"] = s.Network.toMap()
	}
	return m
}

// CephFilesystemSpec defines a CephFS filesystem configuration.
type CephFilesystemSpecInput struct {
	// Name of the filesystem.
	Name string `json:"name,omitempty" jsonschema:"Name of the filesystem."`
	// Metadata pool replication size.
	//  Default: 3
	MetadataPoolReplicatedSize int32 `json:"metadata_pool_replicated_size,omitempty" jsonschema:"Metadata pool replication size. Default: 3"`
	// Data pool replication size.
	//  Default: 3
	DataPoolReplicatedSize int32 `json:"data_pool_replicated_size,omitempty" jsonschema:"Data pool replication size. Default: 3"`
	// Failure domain for data placement.
	//  Default: host
	FailureDomain string `json:"failure_domain,omitempty" jsonschema:"Failure domain for data placement. Default: host"`
	// Number of active MDS (Metadata Server) daemons.
	//  Default: 1
	ActiveMdsCount int32 `json:"active_mds_count,omitempty" jsonschema:"Number of active MDS (Metadata Server) daemons. Default: 1"`
	// Enable active-standby MDS for high availability.
	//  Default: true
	ActiveStandby bool `json:"active_standby,omitempty" jsonschema:"Enable active-standby MDS for high availability. Default: true"`
	// Resource allocation for MDS daemons.
	MdsResources *ContainerResourcesInput `json:"mds_resources,omitempty" jsonschema:"Resource allocation for MDS daemons."`
	// StorageClass configuration for this filesystem.
	StorageClass *CephStorageClassSpecInput `json:"storage_class,omitempty" jsonschema:"StorageClass configuration for this filesystem."`
}

func (s *CephFilesystemSpecInput) validate() error {
	if s.MdsResources != nil {
		if err := s.MdsResources.validate(); err != nil {
			return fmt.Errorf("mds_resources: %w", err)
		}
	}
	if s.StorageClass != nil {
		if err := s.StorageClass.validate(); err != nil {
			return fmt.Errorf("storage_class: %w", err)
		}
	}
	return nil
}

func (s *CephFilesystemSpecInput) applyDefaults() {
	if s.MetadataPoolReplicatedSize == 0 {
		s.MetadataPoolReplicatedSize = 3
	}
	if s.DataPoolReplicatedSize == 0 {
		s.DataPoolReplicatedSize = 3
	}
	if s.FailureDomain == "" {
		s.FailureDomain = "host"
	}
	if s.ActiveMdsCount == 0 {
		s.ActiveMdsCount = 1
	}
	// default: ActiveStandby = true (applied at zero-value)
	if s.MdsResources != nil {
		s.MdsResources.applyDefaults()
	}
	if s.StorageClass != nil {
		s.StorageClass.applyDefaults()
	}
}

func (s *CephFilesystemSpecInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.Name != "" {
		m["name"] = s.Name
	}
	if s.MetadataPoolReplicatedSize != 0 {
		m["metadata_pool_replicated_size"] = s.MetadataPoolReplicatedSize
	}
	if s.DataPoolReplicatedSize != 0 {
		m["data_pool_replicated_size"] = s.DataPoolReplicatedSize
	}
	if s.FailureDomain != "" {
		m["failure_domain"] = s.FailureDomain
	}
	if s.ActiveMdsCount != 0 {
		m["active_mds_count"] = s.ActiveMdsCount
	}
	if s.ActiveStandby {
		m["active_standby"] = s.ActiveStandby
	}
	if s.MdsResources != nil {
		m["mds_resources"] = s.MdsResources.toMap()
	}
	if s.StorageClass != nil {
		m["storage_class"] = s.StorageClass.toMap()
	}
	return m
}

// CephImageSpec defines the Ceph container image configuration.
type CephImageSpecInput struct {
	// Container image repository.
	//  Default: quay.io/ceph/ceph
	Repository string `json:"repository,omitempty" jsonschema:"Container image repository. Default: quay.io/ceph/ceph"`
	// Container image tag.
	//  Default: v19.2.3
	Tag string `json:"tag,omitempty" jsonschema:"Container image tag. Default: v19.2.3"`
	// Allow unsupported Ceph versions. Not recommended for production.
	//  Default: false
	AllowUnsupported bool `json:"allow_unsupported,omitempty" jsonschema:"Allow unsupported Ceph versions. Not recommended for production. Default: false"`
}

func (s *CephImageSpecInput) validate() error {
	return nil
}

func (s *CephImageSpecInput) applyDefaults() {
	if s.Repository == "" {
		s.Repository = "quay.io/ceph/ceph"
	}
	if s.Tag == "" {
		s.Tag = "v19.2.3"
	}
}

func (s *CephImageSpecInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.Repository != "" {
		m["repository"] = s.Repository
	}
	if s.Tag != "" {
		m["tag"] = s.Tag
	}
	if s.AllowUnsupported {
		m["allow_unsupported"] = s.AllowUnsupported
	}
	return m
}

// CephMgrSpec defines the Ceph Manager daemon configuration.
type CephMgrSpecInput struct {
	// Number of manager daemons to run. Use 2 for high availability.
	//  Default: 2
	Count int32 `json:"count,omitempty" jsonschema:"Number of manager daemons to run. Use 2 for high availability. Default: 2"`
	// Allow multiple managers on the same node.
	//  Default: false
	AllowMultiplePerNode bool `json:"allow_multiple_per_node,omitempty" jsonschema:"Allow multiple managers on the same node. Default: false"`
}

func (s *CephMgrSpecInput) validate() error {
	return nil
}

func (s *CephMgrSpecInput) applyDefaults() {
	if s.Count == 0 {
		s.Count = 2
	}
}

func (s *CephMgrSpecInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.Count != 0 {
		m["count"] = s.Count
	}
	if s.AllowMultiplePerNode {
		m["allow_multiple_per_node"] = s.AllowMultiplePerNode
	}
	return m
}

// CephMonSpec defines the Ceph Monitor daemon configuration.
type CephMonSpecInput struct {
	// Number of monitor daemons to run. Recommended: 3 for high availability.
	//  Must be an odd number for proper quorum (1, 3, 5).
	//  Default: 3
	Count int32 `json:"count,omitempty" jsonschema:"Number of monitor daemons to run. Recommended: 3 for high availability. Must be an odd number for proper quorum (1; 3; 5). Default: 3"`
	// Allow multiple monitors on the same node. Not recommended for production.
	//  Default: false
	AllowMultiplePerNode bool `json:"allow_multiple_per_node,omitempty" jsonschema:"Allow multiple monitors on the same node. Not recommended for production. Default: false"`
}

func (s *CephMonSpecInput) validate() error {
	return nil
}

func (s *CephMonSpecInput) applyDefaults() {
	if s.Count == 0 {
		s.Count = 3
	}
}

func (s *CephMonSpecInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.Count != 0 {
		m["count"] = s.Count
	}
	if s.AllowMultiplePerNode {
		m["allow_multiple_per_node"] = s.AllowMultiplePerNode
	}
	return m
}

// CephNetworkSpec defines network configuration for the Ceph cluster.
type CephNetworkSpecInput struct {
	// Enable encryption for data in transit between Ceph daemons.
	//  Requires kernel 5.11+ for full functionality.
	//  Default: false
	EnableEncryption bool `json:"enable_encryption,omitempty" jsonschema:"Enable encryption for data in transit between Ceph daemons. Requires kernel 5.11+ for full functionality. Default: false"`
	// Enable compression for data in transit.
	//  Default: false
	EnableCompression bool `json:"enable_compression,omitempty" jsonschema:"Enable compression for data in transit. Default: false"`
	// Require msgr2 protocol (disable legacy msgr v1).
	//  Default: false
	RequireMsgr2 bool `json:"require_msgr2,omitempty" jsonschema:"Require msgr2 protocol (disable legacy msgr v1). Default: false"`
}

func (s *CephNetworkSpecInput) validate() error {
	return nil
}

func (s *CephNetworkSpecInput) applyDefaults() {
}

func (s *CephNetworkSpecInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.EnableEncryption {
		m["enable_encryption"] = s.EnableEncryption
	}
	if s.EnableCompression {
		m["enable_compression"] = s.EnableCompression
	}
	if s.RequireMsgr2 {
		m["require_msgr2"] = s.RequireMsgr2
	}
	return m
}

// CephObjectStoreSpec defines a Ceph object store (RGW) configuration.
type CephObjectStoreSpecInput struct {
	// Name of the object store.
	Name string `json:"name,omitempty" jsonschema:"Name of the object store."`
	// Metadata pool replication size.
	//  Default: 3
	MetadataPoolReplicatedSize int32 `json:"metadata_pool_replicated_size,omitempty" jsonschema:"Metadata pool replication size. Default: 3"`
	// Number of erasure coding data chunks for the data pool.
	//  Default: 2
	DataPoolErasureDataChunks int32 `json:"data_pool_erasure_data_chunks,omitempty" jsonschema:"Number of erasure coding data chunks for the data pool. Default: 2"`
	// Number of erasure coding parity chunks for the data pool.
	//  Default: 1
	DataPoolErasureCodingChunks int32 `json:"data_pool_erasure_coding_chunks,omitempty" jsonschema:"Number of erasure coding parity chunks for the data pool. Default: 1"`
	// Failure domain for data placement.
	//  Default: host
	FailureDomain string `json:"failure_domain,omitempty" jsonschema:"Failure domain for data placement. Default: host"`
	// Preserve pools when the object store is deleted.
	//  Default: true
	PreservePoolsOnDelete bool `json:"preserve_pools_on_delete,omitempty" jsonschema:"Preserve pools when the object store is deleted. Default: true"`
	// Gateway (RGW) port.
	//  Default: 80
	GatewayPort int32 `json:"gateway_port,omitempty" jsonschema:"Gateway (RGW) port. Default: 80"`
	// Number of gateway instances.
	//  Default: 1
	GatewayInstances int32 `json:"gateway_instances,omitempty" jsonschema:"Number of gateway instances. Default: 1"`
	// Resource allocation for gateway pods.
	GatewayResources *ContainerResourcesInput `json:"gateway_resources,omitempty" jsonschema:"Resource allocation for gateway pods."`
	// StorageClass configuration for object bucket claims.
	StorageClass *CephStorageClassSpecInput `json:"storage_class,omitempty" jsonschema:"StorageClass configuration for object bucket claims."`
}

func (s *CephObjectStoreSpecInput) validate() error {
	if s.GatewayResources != nil {
		if err := s.GatewayResources.validate(); err != nil {
			return fmt.Errorf("gateway_resources: %w", err)
		}
	}
	if s.StorageClass != nil {
		if err := s.StorageClass.validate(); err != nil {
			return fmt.Errorf("storage_class: %w", err)
		}
	}
	return nil
}

func (s *CephObjectStoreSpecInput) applyDefaults() {
	if s.MetadataPoolReplicatedSize == 0 {
		s.MetadataPoolReplicatedSize = 3
	}
	if s.DataPoolErasureDataChunks == 0 {
		s.DataPoolErasureDataChunks = 2
	}
	if s.DataPoolErasureCodingChunks == 0 {
		s.DataPoolErasureCodingChunks = 1
	}
	if s.FailureDomain == "" {
		s.FailureDomain = "host"
	}
	// default: PreservePoolsOnDelete = true (applied at zero-value)
	if s.GatewayPort == 0 {
		s.GatewayPort = 80
	}
	if s.GatewayInstances == 0 {
		s.GatewayInstances = 1
	}
	if s.GatewayResources != nil {
		s.GatewayResources.applyDefaults()
	}
	if s.StorageClass != nil {
		s.StorageClass.applyDefaults()
	}
}

func (s *CephObjectStoreSpecInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.Name != "" {
		m["name"] = s.Name
	}
	if s.MetadataPoolReplicatedSize != 0 {
		m["metadata_pool_replicated_size"] = s.MetadataPoolReplicatedSize
	}
	if s.DataPoolErasureDataChunks != 0 {
		m["data_pool_erasure_data_chunks"] = s.DataPoolErasureDataChunks
	}
	if s.DataPoolErasureCodingChunks != 0 {
		m["data_pool_erasure_coding_chunks"] = s.DataPoolErasureCodingChunks
	}
	if s.FailureDomain != "" {
		m["failure_domain"] = s.FailureDomain
	}
	if s.PreservePoolsOnDelete {
		m["preserve_pools_on_delete"] = s.PreservePoolsOnDelete
	}
	if s.GatewayPort != 0 {
		m["gateway_port"] = s.GatewayPort
	}
	if s.GatewayInstances != 0 {
		m["gateway_instances"] = s.GatewayInstances
	}
	if s.GatewayResources != nil {
		m["gateway_resources"] = s.GatewayResources.toMap()
	}
	if s.StorageClass != nil {
		m["storage_class"] = s.StorageClass.toMap()
	}
	return m
}

// CephResourcesSpec defines resource allocations for Ceph daemon pods.
type CephResourcesSpecInput struct {
	// Resource allocation for Monitor daemons.
	Mon *ContainerResourcesInput `json:"mon,omitempty" jsonschema:"Resource allocation for Monitor daemons."`
	// Resource allocation for Manager daemons.
	Mgr *ContainerResourcesInput `json:"mgr,omitempty" jsonschema:"Resource allocation for Manager daemons."`
	// Resource allocation for OSD daemons.
	Osd *ContainerResourcesInput `json:"osd,omitempty" jsonschema:"Resource allocation for OSD daemons."`
}

func (s *CephResourcesSpecInput) validate() error {
	if s.Mon != nil {
		if err := s.Mon.validate(); err != nil {
			return fmt.Errorf("mon: %w", err)
		}
	}
	if s.Mgr != nil {
		if err := s.Mgr.validate(); err != nil {
			return fmt.Errorf("mgr: %w", err)
		}
	}
	if s.Osd != nil {
		if err := s.Osd.validate(); err != nil {
			return fmt.Errorf("osd: %w", err)
		}
	}
	return nil
}

func (s *CephResourcesSpecInput) applyDefaults() {
	if s.Mon != nil {
		s.Mon.applyDefaults()
	}
	if s.Mgr != nil {
		s.Mgr.applyDefaults()
	}
	if s.Osd != nil {
		s.Osd.applyDefaults()
	}
}

func (s *CephResourcesSpecInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.Mon != nil {
		m["mon"] = s.Mon.toMap()
	}
	if s.Mgr != nil {
		m["mgr"] = s.Mgr.toMap()
	}
	if s.Osd != nil {
		m["osd"] = s.Osd.toMap()
	}
	return m
}

// CephStorageClassSpec defines Kubernetes StorageClass configuration.
type CephStorageClassSpecInput struct {
	// Enable creation of a StorageClass for this pool.
	//  Default: true
	Enabled bool `json:"enabled,omitempty" jsonschema:"Enable creation of a StorageClass for this pool. Default: true"`
	// Name of the StorageClass.
	Name string `json:"name,omitempty" jsonschema:"Name of the StorageClass."`
	// Set as the default StorageClass in the cluster.
	//  Default: false
	IsDefault bool `json:"is_default,omitempty" jsonschema:"Set as the default StorageClass in the cluster. Default: false"`
	// Reclaim policy for volumes (Delete or Retain).
	//  Default: Delete
	ReclaimPolicy string `json:"reclaim_policy,omitempty" jsonschema:"Reclaim policy for volumes (Delete or Retain). Default: Delete"`
	// Allow volume expansion after creation.
	//  Default: true
	AllowVolumeExpansion bool `json:"allow_volume_expansion,omitempty" jsonschema:"Allow volume expansion after creation. Default: true"`
	// Volume binding mode (Immediate or WaitForFirstConsumer).
	//  Default: Immediate
	VolumeBindingMode string `json:"volume_binding_mode,omitempty" jsonschema:"Volume binding mode (Immediate or WaitForFirstConsumer). Default: Immediate"`
}

func (s *CephStorageClassSpecInput) validate() error {
	return nil
}

func (s *CephStorageClassSpecInput) applyDefaults() {
	// default: Enabled = true (applied at zero-value)
	if s.ReclaimPolicy == "" {
		s.ReclaimPolicy = "Delete"
	}
	// default: AllowVolumeExpansion = true (applied at zero-value)
	if s.VolumeBindingMode == "" {
		s.VolumeBindingMode = "Immediate"
	}
}

func (s *CephStorageClassSpecInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.Enabled {
		m["enabled"] = s.Enabled
	}
	if s.Name != "" {
		m["name"] = s.Name
	}
	if s.IsDefault {
		m["is_default"] = s.IsDefault
	}
	if s.ReclaimPolicy != "" {
		m["reclaim_policy"] = s.ReclaimPolicy
	}
	if s.AllowVolumeExpansion {
		m["allow_volume_expansion"] = s.AllowVolumeExpansion
	}
	if s.VolumeBindingMode != "" {
		m["volume_binding_mode"] = s.VolumeBindingMode
	}
	return m
}

// CephStorageNodeSpec defines storage configuration for a specific node.
type CephStorageNodeSpecInput struct {
	// Node name matching kubernetes.io/hostname label.
	Name string `json:"name,omitempty" jsonschema:"Node name matching kubernetes.io/hostname label."`
	// Specific devices to use on this node.
	Devices []string `json:"devices,omitempty" jsonschema:"Specific devices to use on this node."`
	// Device filter pattern for this node.
	DeviceFilter string `json:"device_filter,omitempty" jsonschema:"Device filter pattern for this node."`
}

func (s *CephStorageNodeSpecInput) validate() error {
	return nil
}

func (s *CephStorageNodeSpecInput) applyDefaults() {
}

func (s *CephStorageNodeSpecInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.Name != "" {
		m["name"] = s.Name
	}
	if len(s.Devices) > 0 {
		m["devices"] = s.Devices
	}
	if s.DeviceFilter != "" {
		m["device_filter"] = s.DeviceFilter
	}
	return m
}

// CephStorageSpec defines how Ceph discovers and uses storage devices.
type CephStorageSpecInput struct {
	// Use all nodes in the cluster for storage.
	//  Default: true
	UseAllNodes bool `json:"use_all_nodes,omitempty" jsonschema:"Use all nodes in the cluster for storage. Default: true"`
	// Use all devices on each node for storage.
	//  Default: true
	UseAllDevices bool `json:"use_all_devices,omitempty" jsonschema:"Use all devices on each node for storage. Default: true"`
	// Filter devices by name pattern (regex).
	//  Example: "^sd[a-z]$" to match sda, sdb, etc.
	DeviceFilter string `json:"device_filter,omitempty" jsonschema:"Filter devices by name pattern (regex). Example: '^sd[a-z]$' to match sda; sdb; etc."`
	// Specific nodes and their storage configuration.
	//  Only used when use_all_nodes is false.
	Nodes []*CephStorageNodeSpecInput `json:"nodes,omitempty" jsonschema:"Specific nodes and their storage configuration. Only used when use_all_nodes is false."`
}

func (s *CephStorageSpecInput) validate() error {
	for i, v := range s.Nodes {
		if v != nil {
			if err := v.validate(); err != nil {
				return fmt.Errorf("nodes[%d]: %w", i, err)
			}
		}
	}
	return nil
}

func (s *CephStorageSpecInput) applyDefaults() {
	// default: UseAllNodes = true (applied at zero-value)
	// default: UseAllDevices = true (applied at zero-value)
}

func (s *CephStorageSpecInput) toMap() map[string]any {
	m := make(map[string]any)
	if s.UseAllNodes {
		m["use_all_nodes"] = s.UseAllNodes
	}
	if s.UseAllDevices {
		m["use_all_devices"] = s.UseAllDevices
	}
	if s.DeviceFilter != "" {
		m["device_filter"] = s.DeviceFilter
	}
	if len(s.Nodes) > 0 {
		items := make([]any, len(s.Nodes))
		for i, v := range s.Nodes {
			if v != nil {
				items[i] = v.toMap()
			}
		}
		m["nodes"] = items
	}
	return m
}

// ParseKubernetesRookCephCluster validates and normalizes a KubernetesRookCephCluster cloud_object.
func ParseKubernetesRookCephCluster(cloudObject map[string]any) (*structpb.Struct, error) {
	if err := parse.ValidateHeader(cloudObject, "kubernetes.openmcf.org/v1", "KubernetesRookCephCluster"); err != nil {
		return nil, err
	}

	specMap, err := parse.ExtractSpecMap(cloudObject)
	if err != nil {
		return nil, err
	}

	specBytes, err := json.Marshal(specMap)
	if err != nil {
		return nil, fmt.Errorf("marshal spec: %w", err)
	}

	var spec KubernetesRookCephClusterSpecInput
	if err := json.Unmarshal(specBytes, &spec); err != nil {
		return nil, fmt.Errorf("invalid spec: %w", err)
	}

	if err := spec.validate(); err != nil {
		return nil, err
	}

	spec.applyDefaults()

	return parse.RebuildCloudObject(cloudObject, spec.toMap())
}
